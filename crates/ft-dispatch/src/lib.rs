#![forbid(unsafe_code)]

use std::{collections::BTreeMap, fmt};

use ft_core::{DType, Device, ExecutionMode, ScalarTensor, TensorCompatError, TensorMeta, TensorStorage};
use ft_kernel_cpu::{
    KernelError,
    // --- f64 scalar ops ---
    abs_scalar, acos_scalar, add_scalar, asin_scalar, atan_scalar, atan2_scalar, ceil_scalar,
    clamp_scalar, cos_scalar, cosh_scalar, div_scalar, elu_scalar, eq_scalar, erf_scalar,
    erfc_scalar, exp_scalar, expm1_scalar, floor_scalar, fmod_scalar, frac_scalar, ge_scalar,
    gelu_scalar, gt_scalar, hardsigmoid_scalar, hardswish_scalar, hardtanh_scalar,
    isfinite_scalar, isinf_scalar, isnan_scalar, le_scalar, leaky_relu_scalar, log_scalar,
    log1p_scalar, log2_scalar, log10_scalar, lt_scalar, max_scalar, min_scalar, mish_scalar,
    mul_scalar, ne_scalar, neg_scalar, pow_scalar, reciprocal_scalar, relu_scalar,
    remainder_scalar, round_scalar, rsqrt_scalar, sigmoid_scalar, sign_scalar, silu_scalar,
    sin_scalar, sinh_scalar, softplus_scalar, sqrt_scalar, square_scalar, sub_scalar, tan_scalar,
    tanh_scalar, trunc_scalar,
    // --- f64 tensor ops ---
    abs_tensor_contiguous_f64, acos_tensor_contiguous_f64, add_tensor_contiguous_f64,
    addmm_tensor_contiguous_f64, addmv_tensor_contiguous_f64, asin_tensor_contiguous_f64,
    atan_tensor_contiguous_f64, atan2_tensor_contiguous_f64, bmm_tensor_contiguous_f64,
    cat_tensor_contiguous_f64, ceil_tensor_contiguous_f64, clamp_tensor_contiguous_f64,
    cos_tensor_contiguous_f64, cosh_tensor_contiguous_f64, cumprod_tensor_contiguous_f64,
    cumsum_tensor_contiguous_f64, div_tensor_contiguous_f64, dot_tensor_contiguous_f64,
    elu_tensor_contiguous_f64, eq_tensor_contiguous_f64, erf_tensor_contiguous_f64,
    erfc_tensor_contiguous_f64, exp_tensor_contiguous_f64, expm1_tensor_contiguous_f64,
    floor_tensor_contiguous_f64, fmod_tensor_contiguous_f64, frac_tensor_contiguous_f64,
    ge_tensor_contiguous_f64, gelu_tensor_contiguous_f64, gt_tensor_contiguous_f64,
    hardsigmoid_tensor_contiguous_f64, hardswish_tensor_contiguous_f64,
    hardtanh_tensor_contiguous_f64, isfinite_tensor_contiguous_f64, isinf_tensor_contiguous_f64,
    isnan_tensor_contiguous_f64, le_tensor_contiguous_f64, leaky_relu_tensor_contiguous_f64,
    lerp_tensor_contiguous_f64, log_softmax_dim_tensor_contiguous_f64,
    log_tensor_contiguous_f64, log1p_tensor_contiguous_f64, log2_tensor_contiguous_f64,
    log10_tensor_contiguous_f64, lt_tensor_contiguous_f64, matmul_tensor_contiguous_f64,
    max_tensor_contiguous_f64, mean_dim_tensor_contiguous_f64, mean_tensor_contiguous_f64,
    min_tensor_contiguous_f64, mish_tensor_contiguous_f64, mul_tensor_contiguous_f64,
    ne_tensor_contiguous_f64, neg_tensor_contiguous_f64, norm_dim_tensor_contiguous_f64,
    norm_tensor_contiguous_f64, outer_tensor_contiguous_f64, pow_tensor_contiguous_f64,
    prod_dim_tensor_contiguous_f64, reciprocal_tensor_contiguous_f64, relu_tensor_contiguous_f64,
    remainder_tensor_contiguous_f64, round_tensor_contiguous_f64, rsqrt_tensor_contiguous_f64,
    sigmoid_tensor_contiguous_f64, sign_tensor_contiguous_f64, silu_tensor_contiguous_f64,
    sin_tensor_contiguous_f64, sinh_tensor_contiguous_f64, softmax_dim_tensor_contiguous_f64,
    softplus_tensor_contiguous_f64, sort_tensor_contiguous_f64, sqrt_tensor_contiguous_f64,
    square_tensor_contiguous_f64, stack_tensor_contiguous_f64, std_dim_tensor_contiguous_f64,
    sub_tensor_contiguous_f64, sum_dim_tensor_contiguous_f64, sum_tensor_contiguous_f64,
    tan_tensor_contiguous_f64, tanh_tensor_contiguous_f64, topk_tensor_contiguous_f64,
    trace_tensor_contiguous_f64, trunc_tensor_contiguous_f64, var_dim_tensor_contiguous_f64,
    // --- f32 unary tensor ops ---
    abs_tensor_contiguous_f32, acos_tensor_contiguous_f32, asin_tensor_contiguous_f32,
    atan_tensor_contiguous_f32, ceil_tensor_contiguous_f32, cos_tensor_contiguous_f32,
    cosh_tensor_contiguous_f32, elu_tensor_contiguous_f32, erf_tensor_contiguous_f32,
    erfc_tensor_contiguous_f32, exp_tensor_contiguous_f32, expm1_tensor_contiguous_f32,
    floor_tensor_contiguous_f32, frac_tensor_contiguous_f32, gelu_tensor_contiguous_f32,
    hardsigmoid_tensor_contiguous_f32, hardswish_tensor_contiguous_f32,
    hardtanh_tensor_contiguous_f32, isfinite_tensor_contiguous_f32, isinf_tensor_contiguous_f32,
    isnan_tensor_contiguous_f32, leaky_relu_tensor_contiguous_f32, log_tensor_contiguous_f32,
    log1p_tensor_contiguous_f32, log2_tensor_contiguous_f32, log10_tensor_contiguous_f32,
    mish_tensor_contiguous_f32, neg_tensor_contiguous_f32, reciprocal_tensor_contiguous_f32,
    relu_tensor_contiguous_f32, round_tensor_contiguous_f32, rsqrt_tensor_contiguous_f32,
    sigmoid_tensor_contiguous_f32, sign_tensor_contiguous_f32, silu_tensor_contiguous_f32,
    sin_tensor_contiguous_f32, sinh_tensor_contiguous_f32, softplus_tensor_contiguous_f32,
    sqrt_tensor_contiguous_f32, square_tensor_contiguous_f32, tan_tensor_contiguous_f32,
    tanh_tensor_contiguous_f32, trunc_tensor_contiguous_f32,
    // --- f32 binary tensor ops ---
    add_tensor_contiguous_f32, atan2_tensor_contiguous_f32, bmm_tensor_contiguous_f32,
    div_tensor_contiguous_f32, dot_tensor_contiguous_f32, fmod_tensor_contiguous_f32,
    matmul_tensor_contiguous_f32, max_tensor_contiguous_f32, min_tensor_contiguous_f32,
    mul_tensor_contiguous_f32, outer_tensor_contiguous_f32, remainder_tensor_contiguous_f32,
    sub_tensor_contiguous_f32,
};

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum BinaryOp {
    Add,
    Sub,
    Div,
    Mul,
    MatMul,
    Min,
    Max,
    Dot,
    Outer,
    Bmm,
    Atan2,
    Fmod,
    Remainder,
}

impl BinaryOp {
    #[must_use]
    pub fn from_schema_base(base: &str) -> Option<Self> {
        match base {
            "add" => Some(Self::Add),
            "sub" => Some(Self::Sub),
            "div" => Some(Self::Div),
            "mul" => Some(Self::Mul),
            "matmul" => Some(Self::MatMul),
            "min" => Some(Self::Min),
            "max" => Some(Self::Max),
            "dot" => Some(Self::Dot),
            "outer" => Some(Self::Outer),
            "bmm" => Some(Self::Bmm),
            "atan2" => Some(Self::Atan2),
            "fmod" => Some(Self::Fmod),
            "remainder" => Some(Self::Remainder),
            _ => None,
        }
    }
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum UnaryOp {
    Neg,
    Abs,
    Exp,
    Log,
    Relu,
    Sigmoid,
    Tanh,
    Sqrt,
    Reciprocal,
    Sin,
    Cos,
    Tan,
    Floor,
    Ceil,
    Round,
    Log2,
    Log10,
    Log1p,
    Expm1,
    Sign,
    Trunc,
    Frac,
    Asin,
    Acos,
    Atan,
    Sinh,
    Cosh,
    Gelu,
    Silu,
    LeakyRelu,
    Elu,
    Rsqrt,
    Erf,
    Erfc,
    Hardswish,
    Hardsigmoid,
    Hardtanh,
    Softplus,
    Mish,
    Square,
    IsNan,
    IsInf,
    IsFinite,
}

impl UnaryOp {
    #[must_use]
    pub fn from_schema_base(base: &str) -> Option<Self> {
        match base {
            "neg" => Some(Self::Neg),
            "abs" => Some(Self::Abs),
            "exp" => Some(Self::Exp),
            "log" => Some(Self::Log),
            "relu" => Some(Self::Relu),
            "sigmoid" => Some(Self::Sigmoid),
            "tanh" => Some(Self::Tanh),
            "sqrt" => Some(Self::Sqrt),
            "reciprocal" => Some(Self::Reciprocal),
            "sin" => Some(Self::Sin),
            "cos" => Some(Self::Cos),
            "tan" => Some(Self::Tan),
            "floor" => Some(Self::Floor),
            "ceil" => Some(Self::Ceil),
            "round" => Some(Self::Round),
            "log2" => Some(Self::Log2),
            "log10" => Some(Self::Log10),
            "log1p" => Some(Self::Log1p),
            "expm1" => Some(Self::Expm1),
            "sign" => Some(Self::Sign),
            "trunc" => Some(Self::Trunc),
            "frac" => Some(Self::Frac),
            "asin" => Some(Self::Asin),
            "acos" => Some(Self::Acos),
            "atan" => Some(Self::Atan),
            "sinh" => Some(Self::Sinh),
            "cosh" => Some(Self::Cosh),
            "gelu" => Some(Self::Gelu),
            "silu" => Some(Self::Silu),
            "leaky_relu" => Some(Self::LeakyRelu),
            "elu" => Some(Self::Elu),
            "rsqrt" => Some(Self::Rsqrt),
            "erf" => Some(Self::Erf),
            "erfc" => Some(Self::Erfc),
            "hardswish" => Some(Self::Hardswish),
            "hardsigmoid" => Some(Self::Hardsigmoid),
            "hardtanh" => Some(Self::Hardtanh),
            "softplus" => Some(Self::Softplus),
            "mish" => Some(Self::Mish),
            "square" => Some(Self::Square),
            "isnan" => Some(Self::IsNan),
            "isinf" => Some(Self::IsInf),
            "isfinite" => Some(Self::IsFinite),
            _ => None,
        }
    }
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ReductionOp {
    Sum,
    Mean,
    Prod,
    Var,
    Std,
    Trace,
}

impl ReductionOp {
    #[must_use]
    pub fn from_schema_base(base: &str) -> Option<Self> {
        match base {
            "sum" => Some(Self::Sum),
            "mean" => Some(Self::Mean),
            "prod" => Some(Self::Prod),
            "var" => Some(Self::Var),
            "std" => Some(Self::Std),
            "trace" => Some(Self::Trace),
            _ => None,
        }
    }
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ScanOp {
    CumSum,
    CumProd,
}

impl ScanOp {
    #[must_use]
    pub fn from_schema_base(base: &str) -> Option<Self> {
        match base {
            "cumsum" => Some(Self::CumSum),
            "cumprod" => Some(Self::CumProd),
            _ => None,
        }
    }
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum NormalizeOp {
    Softmax,
    LogSoftmax,
}

impl NormalizeOp {
    #[must_use]
    pub fn from_schema_base(base: &str) -> Option<Self> {
        match base {
            "softmax" => Some(Self::Softmax),
            "log_softmax" => Some(Self::LogSoftmax),
            _ => None,
        }
    }
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum JoinOp {
    Cat,
    Stack,
}

impl JoinOp {
    #[must_use]
    pub fn from_schema_base(base: &str) -> Option<Self> {
        match base {
            "cat" => Some(Self::Cat),
            "stack" => Some(Self::Stack),
            _ => None,
        }
    }
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ComparisonOp {
    Eq,
    Ne,
    Lt,
    Gt,
    Le,
    Ge,
}

impl ComparisonOp {
    #[must_use]
    pub fn from_schema_base(base: &str) -> Option<Self> {
        match base {
            "eq" => Some(Self::Eq),
            "ne" => Some(Self::Ne),
            "lt" => Some(Self::Lt),
            "gt" => Some(Self::Gt),
            "le" => Some(Self::Le),
            "ge" => Some(Self::Ge),
            _ => None,
        }
    }
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
#[repr(u8)]
pub enum DispatchKey {
    Undefined = 0,
    BackendSelect = 1,
    CompositeImplicitAutograd = 2,
    CompositeExplicitAutograd = 3,
    CPU = 4,
    AutogradCPU = 5,
}

impl DispatchKey {
    #[must_use]
    pub const fn all() -> &'static [DispatchKey] {
        &[
            DispatchKey::BackendSelect,
            DispatchKey::CompositeImplicitAutograd,
            DispatchKey::CompositeExplicitAutograd,
            DispatchKey::CPU,
            DispatchKey::AutogradCPU,
        ]
    }

    #[must_use]
    pub const fn bit(self) -> u64 {
        1u64 << (self as u8)
    }
}

const TYPE_PRIORITY: [DispatchKey; 5] = [
    DispatchKey::AutogradCPU,
    DispatchKey::CompositeExplicitAutograd,
    DispatchKey::CompositeImplicitAutograd,
    DispatchKey::CPU,
    DispatchKey::BackendSelect,
];

const BACKEND_PRIORITY: [DispatchKey; 1] = [DispatchKey::CPU];

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Default)]
pub struct DispatchKeySet {
    bits: u64,
}

impl DispatchKeySet {
    #[must_use]
    pub const fn empty() -> Self {
        Self { bits: 0 }
    }

    #[must_use]
    pub fn from_keys(keys: &[DispatchKey]) -> Self {
        let mut out = Self::empty();
        for key in keys {
            out.add(*key);
        }
        out
    }

    pub fn from_bits_checked(bits: u64) -> Result<Self, DispatchKeyError> {
        let known_mask = DispatchKey::all()
            .iter()
            .fold(0u64, |mask, key| mask | key.bit());
        let unknown = bits & !known_mask;
        if unknown != 0 {
            return Err(DispatchKeyError::UnknownBits {
                unknown_mask: unknown,
            });
        }
        Ok(Self { bits })
    }

    #[must_use]
    pub const fn bits(self) -> u64 {
        self.bits
    }

    #[must_use]
    pub const fn is_empty(self) -> bool {
        self.bits == 0
    }

    pub fn add(&mut self, key: DispatchKey) {
        self.bits |= key.bit();
    }

    pub fn remove(&mut self, key: DispatchKey) {
        self.bits &= !key.bit();
    }

    #[must_use]
    pub const fn has(self, key: DispatchKey) -> bool {
        (self.bits & key.bit()) != 0
    }

    #[must_use]
    pub const fn union(self, other: Self) -> Self {
        Self {
            bits: self.bits | other.bits,
        }
    }

    #[must_use]
    pub const fn intersection(self, other: Self) -> Self {
        Self {
            bits: self.bits & other.bits,
        }
    }

    pub fn highest_priority_type_id(self) -> Result<DispatchKey, DispatchKeyError> {
        if self.is_empty() {
            return Err(DispatchKeyError::EmptySet);
        }
        TYPE_PRIORITY
            .iter()
            .find(|&&key| self.has(key))
            .copied()
            .ok_or(DispatchKeyError::NoTypeKey)
    }

    pub fn highest_priority_backend_type_id(self) -> Result<DispatchKey, DispatchKeyError> {
        if self.is_empty() {
            return Err(DispatchKeyError::EmptySet);
        }
        BACKEND_PRIORITY
            .iter()
            .find(|&&key| self.has(key))
            .copied()
            .ok_or(DispatchKeyError::NoBackendKey)
    }

    pub fn validate_for_scalar_binary(self) -> Result<(), DispatchKeyError> {
        if self.is_empty() {
            return Err(DispatchKeyError::EmptySet);
        }
        if self.has(DispatchKey::AutogradCPU) && !self.has(DispatchKey::CPU) {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "AutogradCPU requires CPU backend availability",
            });
        }
        self.highest_priority_type_id()?;
        self.highest_priority_backend_type_id()?;
        Ok(())
    }
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum DispatchKeyError {
    EmptySet,
    NoTypeKey,
    NoBackendKey,
    UnknownBits { unknown_mask: u64 },
    IncompatibleSet { reason: &'static str },
}

impl fmt::Display for DispatchKeyError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Self::EmptySet => write!(f, "dispatch keyset is empty"),
            Self::NoTypeKey => write!(f, "dispatch keyset has no resolvable type key"),
            Self::NoBackendKey => write!(f, "dispatch keyset has no backend key"),
            Self::UnknownBits { unknown_mask } => {
                write!(
                    f,
                    "dispatch keyset has unknown bitmask 0x{unknown_mask:016x}"
                )
            }
            Self::IncompatibleSet { reason } => {
                write!(f, "incompatible dispatch keyset: {reason}")
            }
        }
    }
}

impl std::error::Error for DispatchKeyError {}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub struct DispatchDecision {
    pub op: BinaryOp,
    pub mode: ExecutionMode,
    pub kernel: &'static str,
    pub selected_key: DispatchKey,
    pub backend_key: DispatchKey,
    pub keyset_bits: u64,
    pub fallback_used: bool,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub struct UnaryDispatchDecision {
    pub op: UnaryOp,
    pub mode: ExecutionMode,
    pub kernel: &'static str,
    pub selected_key: DispatchKey,
    pub backend_key: DispatchKey,
    pub keyset_bits: u64,
    pub fallback_used: bool,
}

#[derive(Debug, Clone, PartialEq)]
pub struct DispatchOutcome {
    pub tensor: ScalarTensor,
    pub decision: DispatchDecision,
}

#[derive(Debug, Clone, PartialEq)]
pub struct UnaryDispatchOutcome {
    pub tensor: ScalarTensor,
    pub decision: UnaryDispatchDecision,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorDispatchOutcome {
    pub values: Vec<f64>,
    pub decision: DispatchDecision,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorUnaryDispatchOutcome {
    pub values: Vec<f64>,
    pub decision: UnaryDispatchDecision,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorDispatchOutcomeF32 {
    pub values: Vec<f32>,
    pub decision: DispatchDecision,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorUnaryDispatchOutcomeF32 {
    pub values: Vec<f32>,
    pub decision: UnaryDispatchDecision,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TypedUnaryOutcome {
    pub storage: TensorStorage,
    pub decision: UnaryDispatchDecision,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TypedBinaryOutcome {
    pub storage: TensorStorage,
    pub decision: DispatchDecision,
}

#[derive(Debug, Clone, Copy, PartialEq)]
pub struct ReductionDispatchDecision {
    pub op: ReductionOp,
    pub mode: ExecutionMode,
    pub kernel: &'static str,
    pub selected_key: DispatchKey,
    pub backend_key: DispatchKey,
    pub keyset_bits: u64,
    pub fallback_used: bool,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorReductionDispatchOutcome {
    pub value: f64,
    pub decision: ReductionDispatchDecision,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub struct ComparisonDispatchDecision {
    pub op: ComparisonOp,
    pub mode: ExecutionMode,
    pub kernel: &'static str,
    pub selected_key: DispatchKey,
    pub backend_key: DispatchKey,
    pub keyset_bits: u64,
    pub fallback_used: bool,
}

#[derive(Debug, Clone, PartialEq)]
pub struct ComparisonDispatchOutcome {
    pub tensor: ScalarTensor,
    pub decision: ComparisonDispatchDecision,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorComparisonDispatchOutcome {
    pub values: Vec<f64>,
    pub decision: ComparisonDispatchDecision,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum DispatchError {
    Kernel(KernelError),
    Key(DispatchKeyError),
}

impl fmt::Display for DispatchError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Self::Kernel(error) => write!(f, "kernel dispatch failure: {error}"),
            Self::Key(error) => write!(f, "dispatch key failure: {error}"),
        }
    }
}

impl std::error::Error for DispatchError {}

impl From<KernelError> for DispatchError {
    fn from(value: KernelError) -> Self {
        Self::Kernel(value)
    }
}

impl From<DispatchKeyError> for DispatchError {
    fn from(value: DispatchKeyError) -> Self {
        Self::Key(value)
    }
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum OpSchemaError {
    EmptyInput,
    InvalidOperatorName { reason: &'static str },
    InvalidOverloadName { overload: String },
    MalformedSchema { reason: &'static str },
    UnknownDispatchKey { key: String },
    IncompatibleDispatchKeyset(DispatchKeyError),
}

impl fmt::Display for OpSchemaError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Self::EmptyInput => write!(f, "schema input is empty"),
            Self::InvalidOperatorName { reason } => write!(f, "invalid operator name: {reason}"),
            Self::InvalidOverloadName { overload } => {
                write!(f, "invalid overload name: {overload}")
            }
            Self::MalformedSchema { reason } => write!(f, "malformed schema: {reason}"),
            Self::UnknownDispatchKey { key } => {
                write!(f, "unknown schema dispatch key '{key}'")
            }
            Self::IncompatibleDispatchKeyset(error) => {
                write!(f, "incompatible schema dispatch keyset: {error}")
            }
        }
    }
}

impl std::error::Error for OpSchemaError {}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct OpSchemaName {
    pub base: String,
    pub overload: Option<String>,
    pub is_inplace: bool,
}

impl OpSchemaName {
    #[must_use]
    pub fn unambiguous_name(&self) -> String {
        match &self.overload {
            Some(overload) => format!("{}_{}", self.base, overload),
            None => self.base.clone(),
        }
    }
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct ParsedOpSchema {
    pub op: OpSchemaName,
    pub arguments: String,
    pub returns: String,
    pub is_out_variant: bool,
    pub schema_digest: u64,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum ParsedSchemaInput {
    Name(OpSchemaName),
    Schema(ParsedOpSchema),
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct SchemaDispatchEntry {
    pub normalized_name: String,
    pub op: BinaryOp,
    pub keyset: DispatchKeySet,
    pub is_out_variant: bool,
    pub schema_digest: u64,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum SchemaRegistryError {
    DuplicateSchema { name: String },
    MissingSchema { name: String },
    UnsupportedOperator { base: String },
    IncompatibleDispatchKeyset(DispatchKeyError),
}

impl fmt::Display for SchemaRegistryError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Self::DuplicateSchema { name } => {
                write!(f, "schema registry already has an entry for '{name}'")
            }
            Self::MissingSchema { name } => {
                write!(f, "schema registry has no entry for '{name}'")
            }
            Self::UnsupportedOperator { base } => {
                write!(f, "schema base operator '{base}' is not yet dispatchable")
            }
            Self::IncompatibleDispatchKeyset(error) => {
                write!(f, "schema dispatch keyset is incompatible: {error}")
            }
        }
    }
}

impl std::error::Error for SchemaRegistryError {}

#[derive(Debug, Clone, PartialEq, Eq, Default)]
pub struct SchemaRegistry {
    entries: BTreeMap<String, SchemaDispatchEntry>,
}

impl SchemaRegistry {
    #[must_use]
    pub fn new() -> Self {
        Self::default()
    }

    #[must_use]
    pub fn len(&self) -> usize {
        self.entries.len()
    }

    #[must_use]
    pub fn is_empty(&self) -> bool {
        self.entries.is_empty()
    }

    pub fn register(
        &mut self,
        parsed: &ParsedSchemaInput,
        keyset: DispatchKeySet,
    ) -> Result<String, SchemaRegistryError> {
        keyset
            .validate_for_scalar_binary()
            .map_err(SchemaRegistryError::IncompatibleDispatchKeyset)?;

        let (name, is_out_variant, schema_digest) = match parsed {
            ParsedSchemaInput::Name(name) => {
                (name, false, digest64(name.unambiguous_name().as_str()))
            }
            ParsedSchemaInput::Schema(schema) => {
                (&schema.op, schema.is_out_variant, schema.schema_digest)
            }
        };
        let normalized_name = name.unambiguous_name();
        if self.entries.contains_key(normalized_name.as_str()) {
            return Err(SchemaRegistryError::DuplicateSchema {
                name: normalized_name,
            });
        }

        let op = BinaryOp::from_schema_base(name.base.as_str()).ok_or_else(|| {
            SchemaRegistryError::UnsupportedOperator {
                base: name.base.clone(),
            }
        })?;

        self.entries.insert(
            normalized_name.clone(),
            SchemaDispatchEntry {
                normalized_name: normalized_name.clone(),
                op,
                keyset,
                is_out_variant,
                schema_digest,
            },
        );
        Ok(normalized_name)
    }

    pub fn lookup(
        &self,
        normalized_name: &str,
    ) -> Result<&SchemaDispatchEntry, SchemaRegistryError> {
        self.entries
            .get(normalized_name)
            .ok_or_else(|| SchemaRegistryError::MissingSchema {
                name: normalized_name.to_string(),
            })
    }

    pub fn iter(&self) -> impl Iterator<Item = &SchemaDispatchEntry> {
        self.entries.values()
    }
}

fn is_valid_ident(value: &str) -> bool {
    let mut chars = value.chars();
    match chars.next() {
        Some(first) if first.is_ascii_alphabetic() || first == '_' => {}
        _ => return false,
    }
    chars.all(|ch| ch.is_ascii_alphanumeric() || ch == '_')
}

fn is_reserved_overload_name(value: &str) -> bool {
    matches!(value, "default") || value.starts_with("__")
}

fn digest64(input: &str) -> u64 {
    let mut hash = 0xcbf2_9ce4_8422_2325u64;
    for byte in input.as_bytes() {
        hash ^= u64::from(*byte);
        hash = hash.wrapping_mul(0x0000_0100_0000_01b3);
    }
    hash
}

pub fn parse_schema_name(input: &str) -> Result<OpSchemaName, OpSchemaError> {
    let trimmed = input.trim();
    if trimmed.is_empty() {
        return Err(OpSchemaError::EmptyInput);
    }

    let op_without_namespace = match trimmed.rsplit_once("::") {
        Some((_, op)) => op,
        None => trimmed,
    };
    if op_without_namespace.is_empty() {
        return Err(OpSchemaError::InvalidOperatorName {
            reason: "missing operator token after namespace",
        });
    }

    let (raw_base, overload) = match op_without_namespace.split_once('.') {
        Some((name, overload_name)) => {
            if overload_name.is_empty() {
                return Err(OpSchemaError::InvalidOverloadName {
                    overload: overload_name.to_string(),
                });
            }
            if is_reserved_overload_name(overload_name) {
                return Err(OpSchemaError::InvalidOverloadName {
                    overload: overload_name.to_string(),
                });
            }
            if !is_valid_ident(overload_name) {
                return Err(OpSchemaError::InvalidOverloadName {
                    overload: overload_name.to_string(),
                });
            }
            (name, Some(overload_name.to_string()))
        }
        None => (op_without_namespace, None),
    };

    if raw_base.is_empty() {
        return Err(OpSchemaError::InvalidOperatorName {
            reason: "empty base operator name",
        });
    }

    let is_inplace = raw_base.ends_with('_');
    let base = if is_inplace {
        raw_base.trim_end_matches('_')
    } else {
        raw_base
    };
    if base.is_empty() || !is_valid_ident(base) {
        return Err(OpSchemaError::InvalidOperatorName {
            reason: "operator base contains invalid characters",
        });
    }

    Ok(OpSchemaName {
        base: base.to_string(),
        overload,
        is_inplace,
    })
}

pub fn parse_schema_or_name(input: &str) -> Result<ParsedSchemaInput, OpSchemaError> {
    let trimmed = input.trim();
    if trimmed.is_empty() {
        return Err(OpSchemaError::EmptyInput);
    }

    if let Some(open_idx) = trimmed.find('(') {
        let arrow_idx = trimmed
            .rfind(") -> ")
            .ok_or(OpSchemaError::MalformedSchema {
                reason: "expected ') ->' separator",
            })?;
        if arrow_idx < open_idx {
            return Err(OpSchemaError::MalformedSchema {
                reason: "found return separator before argument list",
            });
        }
        let op_name = parse_schema_name(&trimmed[..open_idx])?;
        let args = trimmed[open_idx + 1..arrow_idx].trim();
        let returns = trimmed[arrow_idx + 5..].trim();
        if returns.is_empty() {
            return Err(OpSchemaError::MalformedSchema {
                reason: "missing return declaration",
            });
        }
        let is_out_variant = op_name.overload.as_deref() == Some("out")
            || args.contains("Tensor(a!) out")
            || args.contains("Tensor(a!) out)");

        return Ok(ParsedSchemaInput::Schema(ParsedOpSchema {
            op: op_name,
            arguments: args.to_string(),
            returns: returns.to_string(),
            is_out_variant,
            schema_digest: digest64(trimmed),
        }));
    }

    Ok(ParsedSchemaInput::Name(parse_schema_name(trimmed)?))
}

pub fn schema_dispatch_key_from_tag(tag: &str) -> Result<DispatchKey, OpSchemaError> {
    match tag {
        "BackendSelect" => Ok(DispatchKey::BackendSelect),
        "CompositeImplicitAutograd" => Ok(DispatchKey::CompositeImplicitAutograd),
        "CompositeExplicitAutograd" => Ok(DispatchKey::CompositeExplicitAutograd),
        "CPU" => Ok(DispatchKey::CPU),
        "AutogradCPU" => Ok(DispatchKey::AutogradCPU),
        _ => Err(OpSchemaError::UnknownDispatchKey {
            key: tag.to_string(),
        }),
    }
}

pub fn schema_dispatch_keyset_from_tags(tags: &[&str]) -> Result<DispatchKeySet, OpSchemaError> {
    let mut keyset = DispatchKeySet::empty();
    for tag in tags {
        keyset.add(schema_dispatch_key_from_tag(tag)?);
    }
    keyset
        .validate_for_scalar_binary()
        .map_err(OpSchemaError::IncompatibleDispatchKeyset)?;
    Ok(keyset)
}

#[derive(Debug, Clone, PartialEq)]
pub enum SchemaDispatchError {
    Registry(SchemaRegistryError),
    Dispatch(DispatchError),
}

impl fmt::Display for SchemaDispatchError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Self::Registry(error) => write!(f, "schema registry failure: {error}"),
            Self::Dispatch(error) => write!(f, "schema dispatch failure: {error}"),
        }
    }
}

impl std::error::Error for SchemaDispatchError {}

impl From<SchemaRegistryError> for SchemaDispatchError {
    fn from(value: SchemaRegistryError) -> Self {
        Self::Registry(value)
    }
}

impl From<DispatchError> for SchemaDispatchError {
    fn from(value: DispatchError) -> Self {
        Self::Dispatch(value)
    }
}

#[must_use]
fn dispatch_keyset_for_device(device: Device, requires_grad: bool) -> DispatchKeySet {
    let mut keyset = DispatchKeySet::empty();
    keyset.add(DispatchKey::BackendSelect);
    if device == Device::Cpu {
        keyset.add(DispatchKey::CPU);
    }
    if requires_grad {
        keyset.add(DispatchKey::AutogradCPU);
    }
    keyset
}

#[must_use]
pub fn dispatch_keyset_for_tensors(
    lhs: &ScalarTensor,
    _rhs: &ScalarTensor,
    requires_grad: bool,
) -> DispatchKeySet {
    dispatch_keyset_for_device(lhs.meta().device(), requires_grad)
}

#[must_use]
pub fn dispatch_keyset_for_tensor_meta(
    lhs: &TensorMeta,
    _rhs: &TensorMeta,
    requires_grad: bool,
) -> DispatchKeySet {
    dispatch_keyset_for_device(lhs.device(), requires_grad)
}

fn ensure_tensor_meta_compatible(lhs: &TensorMeta, rhs: &TensorMeta) -> Result<(), DispatchError> {
    if lhs.dtype() != rhs.dtype() {
        return Err(DispatchError::Kernel(KernelError::Incompatible(
            TensorCompatError::DTypeMismatch {
                lhs: lhs.dtype(),
                rhs: rhs.dtype(),
            },
        )));
    }
    if lhs.device() != rhs.device() {
        return Err(DispatchError::Kernel(KernelError::Incompatible(
            TensorCompatError::DeviceMismatch {
                lhs: lhs.device(),
                rhs: rhs.device(),
            },
        )));
    }
    Ok(())
}

fn resolve_dispatch_keys(
    mode: ExecutionMode,
    keyset: DispatchKeySet,
) -> Result<(DispatchKey, DispatchKey, DispatchKey, bool), DispatchError> {
    keyset.validate_for_scalar_binary()?;
    let selected_key = keyset.highest_priority_type_id()?;
    let backend_key = keyset.highest_priority_backend_type_id()?;

    let (effective_key, fallback_used) = match selected_key {
        DispatchKey::AutogradCPU | DispatchKey::CPU => (selected_key, false),
        DispatchKey::CompositeExplicitAutograd
        | DispatchKey::CompositeImplicitAutograd
        | DispatchKey::BackendSelect => match mode {
            ExecutionMode::Strict => {
                return Err(DispatchKeyError::IncompatibleSet {
                    reason: "strict mode forbids composite/backend fallback routing",
                }
                .into());
            }
            ExecutionMode::Hardened => (backend_key, true),
        },
        DispatchKey::Undefined => return Err(DispatchKeyError::NoTypeKey.into()),
    };

    if effective_key != backend_key && effective_key != DispatchKey::AutogradCPU {
        return Err(DispatchKeyError::IncompatibleSet {
            reason: "resolved key/backend key drifted to incompatible pair",
        }
        .into());
    }

    Ok((selected_key, backend_key, effective_key, fallback_used))
}

pub fn dispatch_scalar_binary(
    op: BinaryOp,
    mode: ExecutionMode,
    lhs: &ScalarTensor,
    rhs: &ScalarTensor,
    requires_grad: bool,
) -> Result<DispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_tensors(lhs, rhs, requires_grad);
    dispatch_scalar_binary_with_keyset(op, mode, lhs, rhs, keyset)
}

pub fn dispatch_scalar_binary_with_keyset(
    op: BinaryOp,
    mode: ExecutionMode,
    lhs: &ScalarTensor,
    rhs: &ScalarTensor,
    keyset: DispatchKeySet,
) -> Result<DispatchOutcome, DispatchError> {
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (tensor, kernel) = match (effective_key, op) {
        (DispatchKey::AutogradCPU, BinaryOp::Add) => {
            (add_scalar(lhs, rhs)?, "autograd_cpu::add_scalar")
        }
        (DispatchKey::AutogradCPU, BinaryOp::Sub) => {
            (sub_scalar(lhs, rhs)?, "autograd_cpu::sub_scalar")
        }
        (DispatchKey::AutogradCPU, BinaryOp::Div) => {
            (div_scalar(lhs, rhs)?, "autograd_cpu::div_scalar")
        }
        (DispatchKey::AutogradCPU, BinaryOp::Mul) => {
            (mul_scalar(lhs, rhs)?, "autograd_cpu::mul_scalar")
        }
        (DispatchKey::AutogradCPU, BinaryOp::MatMul)
        | (DispatchKey::AutogradCPU, BinaryOp::Dot)
        | (DispatchKey::AutogradCPU, BinaryOp::Outer)
        | (DispatchKey::AutogradCPU, BinaryOp::Bmm) => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "matmul/dot/outer/bmm is unsupported for scalar tensors",
            }
            .into());
        }
        (DispatchKey::AutogradCPU, BinaryOp::Min) => {
            (min_scalar(lhs, rhs)?, "autograd_cpu::min_scalar")
        }
        (DispatchKey::AutogradCPU, BinaryOp::Max) => {
            (max_scalar(lhs, rhs)?, "autograd_cpu::max_scalar")
        }
        (DispatchKey::AutogradCPU, BinaryOp::Atan2) => {
            (atan2_scalar(lhs, rhs)?, "autograd_cpu::atan2_scalar")
        }
        (DispatchKey::AutogradCPU, BinaryOp::Fmod) => {
            (fmod_scalar(lhs, rhs)?, "autograd_cpu::fmod_scalar")
        }
        (DispatchKey::AutogradCPU, BinaryOp::Remainder) => (
            remainder_scalar(lhs, rhs)?,
            "autograd_cpu::remainder_scalar",
        ),
        (DispatchKey::CPU, BinaryOp::Add) => (add_scalar(lhs, rhs)?, "cpu::add_scalar"),
        (DispatchKey::CPU, BinaryOp::Sub) => (sub_scalar(lhs, rhs)?, "cpu::sub_scalar"),
        (DispatchKey::CPU, BinaryOp::Div) => (div_scalar(lhs, rhs)?, "cpu::div_scalar"),
        (DispatchKey::CPU, BinaryOp::Mul) => (mul_scalar(lhs, rhs)?, "cpu::mul_scalar"),
        (DispatchKey::CPU, BinaryOp::MatMul)
        | (DispatchKey::CPU, BinaryOp::Dot)
        | (DispatchKey::CPU, BinaryOp::Outer)
        | (DispatchKey::CPU, BinaryOp::Bmm) => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "matmul/dot/outer/bmm is unsupported for scalar tensors",
            }
            .into());
        }
        (DispatchKey::CPU, BinaryOp::Min) => (min_scalar(lhs, rhs)?, "cpu::min_scalar"),
        (DispatchKey::CPU, BinaryOp::Max) => (max_scalar(lhs, rhs)?, "cpu::max_scalar"),
        (DispatchKey::CPU, BinaryOp::Atan2) => (atan2_scalar(lhs, rhs)?, "cpu::atan2_scalar"),
        (DispatchKey::CPU, BinaryOp::Fmod) => (fmod_scalar(lhs, rhs)?, "cpu::fmod_scalar"),
        (DispatchKey::CPU, BinaryOp::Remainder) => {
            (remainder_scalar(lhs, rhs)?, "cpu::remainder_scalar")
        }
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for scalar binary ops",
            }
            .into());
        }
    };

    Ok(DispatchOutcome {
        tensor,
        decision: DispatchDecision {
            op,
            mode,
            kernel,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

pub fn dispatch_scalar_binary_registered(
    registry: &SchemaRegistry,
    normalized_name: &str,
    mode: ExecutionMode,
    lhs: &ScalarTensor,
    rhs: &ScalarTensor,
) -> Result<DispatchOutcome, SchemaDispatchError> {
    let entry = registry.lookup(normalized_name)?;
    dispatch_scalar_binary_with_keyset(entry.op, mode, lhs, rhs, entry.keyset).map_err(Into::into)
}

pub fn dispatch_tensor_binary_contiguous_f64(
    op: BinaryOp,
    mode: ExecutionMode,
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
    requires_grad: bool,
) -> Result<TensorDispatchOutcome, DispatchError> {
    ensure_tensor_meta_compatible(lhs_meta, rhs_meta)?;
    let keyset = dispatch_keyset_for_tensor_meta(lhs_meta, rhs_meta, requires_grad);
    dispatch_tensor_binary_contiguous_f64_with_keyset(
        op, mode, lhs, rhs, lhs_meta, rhs_meta, keyset,
    )
}

pub fn dispatch_tensor_binary_contiguous_f64_with_keyset(
    op: BinaryOp,
    mode: ExecutionMode,
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
    keyset: DispatchKeySet,
) -> Result<TensorDispatchOutcome, DispatchError> {
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (values, kernel) = match (effective_key, op) {
        (DispatchKey::AutogradCPU, BinaryOp::Add) => (
            add_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::add_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Sub) => (
            sub_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::sub_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Div) => (
            div_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::div_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Mul) => (
            mul_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::mul_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::MatMul) => (
            matmul_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::matmul_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Min) => (
            min_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::min_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Max) => (
            max_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::max_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Dot) => (
            vec![dot_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?],
            "autograd_cpu::dot_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Outer) => (
            outer_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::outer_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Bmm) => (
            bmm_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::bmm_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Atan2) => (
            atan2_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::atan2_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Fmod) => (
            fmod_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::fmod_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Remainder) => (
            remainder_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::remainder_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, BinaryOp::Add) => (
            add_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::add_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, BinaryOp::Sub) => (
            sub_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::sub_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, BinaryOp::Div) => (
            div_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::div_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, BinaryOp::Mul) => (
            mul_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::mul_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, BinaryOp::MatMul) => (
            matmul_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::matmul_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, BinaryOp::Min) => (
            min_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::min_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, BinaryOp::Max) => (
            max_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::max_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, BinaryOp::Dot) => (
            vec![dot_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?],
            "cpu::dot_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, BinaryOp::Outer) => (
            outer_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::outer_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, BinaryOp::Bmm) => (
            bmm_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::bmm_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, BinaryOp::Atan2) => (
            atan2_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::atan2_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, BinaryOp::Fmod) => (
            fmod_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::fmod_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, BinaryOp::Remainder) => (
            remainder_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::remainder_tensor_contiguous_f64",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for contiguous tensor binary ops",
            }
            .into());
        }
    };

    Ok(TensorDispatchOutcome {
        values,
        decision: DispatchDecision {
            op,
            mode,
            kernel,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

pub fn dispatch_tensor_binary_contiguous_f32(
    op: BinaryOp,
    mode: ExecutionMode,
    lhs: &[f32],
    rhs: &[f32],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
    requires_grad: bool,
) -> Result<TensorDispatchOutcomeF32, DispatchError> {
    ensure_tensor_meta_compatible(lhs_meta, rhs_meta)?;
    let keyset = dispatch_keyset_for_tensor_meta(lhs_meta, rhs_meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (values, kernel) = match (effective_key, op) {
        (DispatchKey::AutogradCPU, BinaryOp::Add) => (
            add_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::add_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Sub) => (
            sub_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::sub_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Div) => (
            div_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::div_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Mul) => (
            mul_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::mul_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::MatMul) => (
            matmul_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::matmul_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Min) => (
            min_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::min_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Max) => (
            max_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::max_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Dot) => (
            vec![dot_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?],
            "autograd_cpu::dot_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Outer) => (
            outer_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::outer_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Bmm) => (
            bmm_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::bmm_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Atan2) => (
            atan2_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::atan2_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Fmod) => (
            fmod_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::fmod_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, BinaryOp::Remainder) => (
            remainder_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "autograd_cpu::remainder_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, BinaryOp::Add) => (
            add_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::add_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, BinaryOp::Sub) => (
            sub_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::sub_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, BinaryOp::Div) => (
            div_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::div_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, BinaryOp::Mul) => (
            mul_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::mul_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, BinaryOp::MatMul) => (
            matmul_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::matmul_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, BinaryOp::Min) => (
            min_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::min_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, BinaryOp::Max) => (
            max_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::max_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, BinaryOp::Dot) => (
            vec![dot_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?],
            "cpu::dot_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, BinaryOp::Outer) => (
            outer_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::outer_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, BinaryOp::Bmm) => (
            bmm_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::bmm_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, BinaryOp::Atan2) => (
            atan2_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::atan2_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, BinaryOp::Fmod) => (
            fmod_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::fmod_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, BinaryOp::Remainder) => (
            remainder_tensor_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta)?,
            "cpu::remainder_tensor_contiguous_f32",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for contiguous tensor binary f32 ops",
            }
            .into());
        }
    };

    Ok(TensorDispatchOutcomeF32 {
        values,
        decision: DispatchDecision {
            op,
            mode,
            kernel,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

#[must_use]
pub fn dispatch_keyset_for_single_tensor(
    input: &ScalarTensor,
    requires_grad: bool,
) -> DispatchKeySet {
    dispatch_keyset_for_device(input.meta().device(), requires_grad)
}

#[must_use]
pub fn dispatch_keyset_for_single_tensor_meta(
    meta: &TensorMeta,
    requires_grad: bool,
) -> DispatchKeySet {
    dispatch_keyset_for_device(meta.device(), requires_grad)
}

pub fn dispatch_scalar_unary(
    op: UnaryOp,
    mode: ExecutionMode,
    input: &ScalarTensor,
    requires_grad: bool,
) -> Result<UnaryDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor(input, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (tensor, kernel) = match (effective_key, op) {
        (DispatchKey::AutogradCPU, UnaryOp::Neg) => (neg_scalar(input), "autograd_cpu::neg_scalar"),
        (DispatchKey::AutogradCPU, UnaryOp::Abs) => (abs_scalar(input), "autograd_cpu::abs_scalar"),
        (DispatchKey::AutogradCPU, UnaryOp::Exp) => (exp_scalar(input), "autograd_cpu::exp_scalar"),
        (DispatchKey::AutogradCPU, UnaryOp::Log) => (log_scalar(input), "autograd_cpu::log_scalar"),
        (DispatchKey::AutogradCPU, UnaryOp::Relu) => {
            (relu_scalar(input), "autograd_cpu::relu_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Sigmoid) => {
            (sigmoid_scalar(input), "autograd_cpu::sigmoid_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Tanh) => {
            (tanh_scalar(input), "autograd_cpu::tanh_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Sqrt) => {
            (sqrt_scalar(input), "autograd_cpu::sqrt_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Reciprocal) => {
            (reciprocal_scalar(input), "autograd_cpu::reciprocal_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Sin) => (sin_scalar(input), "autograd_cpu::sin_scalar"),
        (DispatchKey::AutogradCPU, UnaryOp::Cos) => (cos_scalar(input), "autograd_cpu::cos_scalar"),
        (DispatchKey::AutogradCPU, UnaryOp::Tan) => (tan_scalar(input), "autograd_cpu::tan_scalar"),
        (DispatchKey::AutogradCPU, UnaryOp::Floor) => {
            (floor_scalar(input), "autograd_cpu::floor_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Ceil) => {
            (ceil_scalar(input), "autograd_cpu::ceil_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Round) => {
            (round_scalar(input), "autograd_cpu::round_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Log2) => {
            (log2_scalar(input), "autograd_cpu::log2_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Log10) => {
            (log10_scalar(input), "autograd_cpu::log10_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Log1p) => {
            (log1p_scalar(input), "autograd_cpu::log1p_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Expm1) => {
            (expm1_scalar(input), "autograd_cpu::expm1_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Sign) => {
            (sign_scalar(input), "autograd_cpu::sign_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Trunc) => {
            (trunc_scalar(input), "autograd_cpu::trunc_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Frac) => {
            (frac_scalar(input), "autograd_cpu::frac_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Asin) => {
            (asin_scalar(input), "autograd_cpu::asin_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Acos) => {
            (acos_scalar(input), "autograd_cpu::acos_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Atan) => {
            (atan_scalar(input), "autograd_cpu::atan_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Sinh) => {
            (sinh_scalar(input), "autograd_cpu::sinh_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Cosh) => {
            (cosh_scalar(input), "autograd_cpu::cosh_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Gelu) => {
            (gelu_scalar(input), "autograd_cpu::gelu_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Silu) => {
            (silu_scalar(input), "autograd_cpu::silu_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::LeakyRelu) => {
            (leaky_relu_scalar(input), "autograd_cpu::leaky_relu_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Elu) => (elu_scalar(input), "autograd_cpu::elu_scalar"),
        (DispatchKey::AutogradCPU, UnaryOp::Rsqrt) => {
            (rsqrt_scalar(input), "autograd_cpu::rsqrt_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Erf) => (erf_scalar(input), "autograd_cpu::erf_scalar"),
        (DispatchKey::AutogradCPU, UnaryOp::Erfc) => {
            (erfc_scalar(input), "autograd_cpu::erfc_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Hardswish) => {
            (hardswish_scalar(input), "autograd_cpu::hardswish_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Hardsigmoid) => (
            hardsigmoid_scalar(input),
            "autograd_cpu::hardsigmoid_scalar",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Hardtanh) => {
            (hardtanh_scalar(input), "autograd_cpu::hardtanh_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Softplus) => {
            (softplus_scalar(input), "autograd_cpu::softplus_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Mish) => {
            (mish_scalar(input), "autograd_cpu::mish_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::Square) => {
            (square_scalar(input), "autograd_cpu::square_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::IsNan) => {
            (isnan_scalar(input), "autograd_cpu::isnan_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::IsInf) => {
            (isinf_scalar(input), "autograd_cpu::isinf_scalar")
        }
        (DispatchKey::AutogradCPU, UnaryOp::IsFinite) => {
            (isfinite_scalar(input), "autograd_cpu::isfinite_scalar")
        }
        (DispatchKey::CPU, UnaryOp::Neg) => (neg_scalar(input), "cpu::neg_scalar"),
        (DispatchKey::CPU, UnaryOp::Abs) => (abs_scalar(input), "cpu::abs_scalar"),
        (DispatchKey::CPU, UnaryOp::Exp) => (exp_scalar(input), "cpu::exp_scalar"),
        (DispatchKey::CPU, UnaryOp::Log) => (log_scalar(input), "cpu::log_scalar"),
        (DispatchKey::CPU, UnaryOp::Relu) => (relu_scalar(input), "cpu::relu_scalar"),
        (DispatchKey::CPU, UnaryOp::Sigmoid) => (sigmoid_scalar(input), "cpu::sigmoid_scalar"),
        (DispatchKey::CPU, UnaryOp::Tanh) => (tanh_scalar(input), "cpu::tanh_scalar"),
        (DispatchKey::CPU, UnaryOp::Sqrt) => (sqrt_scalar(input), "cpu::sqrt_scalar"),
        (DispatchKey::CPU, UnaryOp::Reciprocal) => {
            (reciprocal_scalar(input), "cpu::reciprocal_scalar")
        }
        (DispatchKey::CPU, UnaryOp::Sin) => (sin_scalar(input), "cpu::sin_scalar"),
        (DispatchKey::CPU, UnaryOp::Cos) => (cos_scalar(input), "cpu::cos_scalar"),
        (DispatchKey::CPU, UnaryOp::Tan) => (tan_scalar(input), "cpu::tan_scalar"),
        (DispatchKey::CPU, UnaryOp::Floor) => (floor_scalar(input), "cpu::floor_scalar"),
        (DispatchKey::CPU, UnaryOp::Ceil) => (ceil_scalar(input), "cpu::ceil_scalar"),
        (DispatchKey::CPU, UnaryOp::Round) => (round_scalar(input), "cpu::round_scalar"),
        (DispatchKey::CPU, UnaryOp::Log2) => (log2_scalar(input), "cpu::log2_scalar"),
        (DispatchKey::CPU, UnaryOp::Log10) => (log10_scalar(input), "cpu::log10_scalar"),
        (DispatchKey::CPU, UnaryOp::Log1p) => (log1p_scalar(input), "cpu::log1p_scalar"),
        (DispatchKey::CPU, UnaryOp::Expm1) => (expm1_scalar(input), "cpu::expm1_scalar"),
        (DispatchKey::CPU, UnaryOp::Sign) => (sign_scalar(input), "cpu::sign_scalar"),
        (DispatchKey::CPU, UnaryOp::Trunc) => (trunc_scalar(input), "cpu::trunc_scalar"),
        (DispatchKey::CPU, UnaryOp::Frac) => (frac_scalar(input), "cpu::frac_scalar"),
        (DispatchKey::CPU, UnaryOp::Asin) => (asin_scalar(input), "cpu::asin_scalar"),
        (DispatchKey::CPU, UnaryOp::Acos) => (acos_scalar(input), "cpu::acos_scalar"),
        (DispatchKey::CPU, UnaryOp::Atan) => (atan_scalar(input), "cpu::atan_scalar"),
        (DispatchKey::CPU, UnaryOp::Sinh) => (sinh_scalar(input), "cpu::sinh_scalar"),
        (DispatchKey::CPU, UnaryOp::Cosh) => (cosh_scalar(input), "cpu::cosh_scalar"),
        (DispatchKey::CPU, UnaryOp::Gelu) => (gelu_scalar(input), "cpu::gelu_scalar"),
        (DispatchKey::CPU, UnaryOp::Silu) => (silu_scalar(input), "cpu::silu_scalar"),
        (DispatchKey::CPU, UnaryOp::LeakyRelu) => {
            (leaky_relu_scalar(input), "cpu::leaky_relu_scalar")
        }
        (DispatchKey::CPU, UnaryOp::Elu) => (elu_scalar(input), "cpu::elu_scalar"),
        (DispatchKey::CPU, UnaryOp::Rsqrt) => (rsqrt_scalar(input), "cpu::rsqrt_scalar"),
        (DispatchKey::CPU, UnaryOp::Erf) => (erf_scalar(input), "cpu::erf_scalar"),
        (DispatchKey::CPU, UnaryOp::Erfc) => (erfc_scalar(input), "cpu::erfc_scalar"),
        (DispatchKey::CPU, UnaryOp::Hardswish) => {
            (hardswish_scalar(input), "cpu::hardswish_scalar")
        }
        (DispatchKey::CPU, UnaryOp::Hardsigmoid) => {
            (hardsigmoid_scalar(input), "cpu::hardsigmoid_scalar")
        }
        (DispatchKey::CPU, UnaryOp::Hardtanh) => (hardtanh_scalar(input), "cpu::hardtanh_scalar"),
        (DispatchKey::CPU, UnaryOp::Softplus) => (softplus_scalar(input), "cpu::softplus_scalar"),
        (DispatchKey::CPU, UnaryOp::Mish) => (mish_scalar(input), "cpu::mish_scalar"),
        (DispatchKey::CPU, UnaryOp::Square) => (square_scalar(input), "cpu::square_scalar"),
        (DispatchKey::CPU, UnaryOp::IsNan) => (isnan_scalar(input), "cpu::isnan_scalar"),
        (DispatchKey::CPU, UnaryOp::IsInf) => (isinf_scalar(input), "cpu::isinf_scalar"),
        (DispatchKey::CPU, UnaryOp::IsFinite) => (isfinite_scalar(input), "cpu::isfinite_scalar"),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for scalar unary ops",
            }
            .into());
        }
    };

    Ok(UnaryDispatchOutcome {
        tensor,
        decision: UnaryDispatchDecision {
            op,
            mode,
            kernel,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

pub fn dispatch_tensor_unary_contiguous_f64(
    op: UnaryOp,
    mode: ExecutionMode,
    input: &[f64],
    meta: &TensorMeta,
    requires_grad: bool,
) -> Result<TensorUnaryDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor_meta(meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (values, kernel) = match (effective_key, op) {
        (DispatchKey::AutogradCPU, UnaryOp::Neg) => (
            neg_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::neg_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Abs) => (
            abs_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::abs_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Exp) => (
            exp_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::exp_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Log) => (
            log_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::log_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Relu) => (
            relu_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::relu_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Sigmoid) => (
            sigmoid_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::sigmoid_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Tanh) => (
            tanh_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::tanh_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Sqrt) => (
            sqrt_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::sqrt_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Reciprocal) => (
            reciprocal_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::reciprocal_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Sin) => (
            sin_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::sin_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Cos) => (
            cos_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::cos_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Tan) => (
            tan_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::tan_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Floor) => (
            floor_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::floor_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Ceil) => (
            ceil_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::ceil_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Round) => (
            round_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::round_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Log2) => (
            log2_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::log2_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Log10) => (
            log10_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::log10_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Log1p) => (
            log1p_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::log1p_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Expm1) => (
            expm1_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::expm1_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Sign) => (
            sign_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::sign_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Trunc) => (
            trunc_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::trunc_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Frac) => (
            frac_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::frac_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Asin) => (
            asin_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::asin_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Acos) => (
            acos_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::acos_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Atan) => (
            atan_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::atan_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Sinh) => (
            sinh_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::sinh_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Cosh) => (
            cosh_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::cosh_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Gelu) => (
            gelu_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::gelu_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Silu) => (
            silu_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::silu_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::LeakyRelu) => (
            leaky_relu_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::leaky_relu_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Elu) => (
            elu_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::elu_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Rsqrt) => (
            rsqrt_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::rsqrt_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Erf) => (
            erf_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::erf_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Erfc) => (
            erfc_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::erfc_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Hardswish) => (
            hardswish_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::hardswish_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Hardsigmoid) => (
            hardsigmoid_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::hardsigmoid_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Hardtanh) => (
            hardtanh_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::hardtanh_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Softplus) => (
            softplus_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::softplus_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Mish) => (
            mish_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::mish_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Square) => (
            square_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::square_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::IsNan) => (
            isnan_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::isnan_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::IsInf) => (
            isinf_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::isinf_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::IsFinite) => (
            isfinite_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::isfinite_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Neg) => (
            neg_tensor_contiguous_f64(input, meta)?,
            "cpu::neg_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Abs) => (
            abs_tensor_contiguous_f64(input, meta)?,
            "cpu::abs_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Exp) => (
            exp_tensor_contiguous_f64(input, meta)?,
            "cpu::exp_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Log) => (
            log_tensor_contiguous_f64(input, meta)?,
            "cpu::log_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Relu) => (
            relu_tensor_contiguous_f64(input, meta)?,
            "cpu::relu_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Sigmoid) => (
            sigmoid_tensor_contiguous_f64(input, meta)?,
            "cpu::sigmoid_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Tanh) => (
            tanh_tensor_contiguous_f64(input, meta)?,
            "cpu::tanh_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Sqrt) => (
            sqrt_tensor_contiguous_f64(input, meta)?,
            "cpu::sqrt_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Reciprocal) => (
            reciprocal_tensor_contiguous_f64(input, meta)?,
            "cpu::reciprocal_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Sin) => (
            sin_tensor_contiguous_f64(input, meta)?,
            "cpu::sin_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Cos) => (
            cos_tensor_contiguous_f64(input, meta)?,
            "cpu::cos_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Tan) => (
            tan_tensor_contiguous_f64(input, meta)?,
            "cpu::tan_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Floor) => (
            floor_tensor_contiguous_f64(input, meta)?,
            "cpu::floor_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Ceil) => (
            ceil_tensor_contiguous_f64(input, meta)?,
            "cpu::ceil_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Round) => (
            round_tensor_contiguous_f64(input, meta)?,
            "cpu::round_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Log2) => (
            log2_tensor_contiguous_f64(input, meta)?,
            "cpu::log2_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Log10) => (
            log10_tensor_contiguous_f64(input, meta)?,
            "cpu::log10_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Log1p) => (
            log1p_tensor_contiguous_f64(input, meta)?,
            "cpu::log1p_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Expm1) => (
            expm1_tensor_contiguous_f64(input, meta)?,
            "cpu::expm1_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Sign) => (
            sign_tensor_contiguous_f64(input, meta)?,
            "cpu::sign_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Trunc) => (
            trunc_tensor_contiguous_f64(input, meta)?,
            "cpu::trunc_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Frac) => (
            frac_tensor_contiguous_f64(input, meta)?,
            "cpu::frac_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Asin) => (
            asin_tensor_contiguous_f64(input, meta)?,
            "cpu::asin_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Acos) => (
            acos_tensor_contiguous_f64(input, meta)?,
            "cpu::acos_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Atan) => (
            atan_tensor_contiguous_f64(input, meta)?,
            "cpu::atan_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Sinh) => (
            sinh_tensor_contiguous_f64(input, meta)?,
            "cpu::sinh_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Cosh) => (
            cosh_tensor_contiguous_f64(input, meta)?,
            "cpu::cosh_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Gelu) => (
            gelu_tensor_contiguous_f64(input, meta)?,
            "cpu::gelu_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Silu) => (
            silu_tensor_contiguous_f64(input, meta)?,
            "cpu::silu_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::LeakyRelu) => (
            leaky_relu_tensor_contiguous_f64(input, meta)?,
            "cpu::leaky_relu_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Elu) => (
            elu_tensor_contiguous_f64(input, meta)?,
            "cpu::elu_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Rsqrt) => (
            rsqrt_tensor_contiguous_f64(input, meta)?,
            "cpu::rsqrt_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Erf) => (
            erf_tensor_contiguous_f64(input, meta)?,
            "cpu::erf_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Erfc) => (
            erfc_tensor_contiguous_f64(input, meta)?,
            "cpu::erfc_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Hardswish) => (
            hardswish_tensor_contiguous_f64(input, meta)?,
            "cpu::hardswish_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Hardsigmoid) => (
            hardsigmoid_tensor_contiguous_f64(input, meta)?,
            "cpu::hardsigmoid_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Hardtanh) => (
            hardtanh_tensor_contiguous_f64(input, meta)?,
            "cpu::hardtanh_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Softplus) => (
            softplus_tensor_contiguous_f64(input, meta)?,
            "cpu::softplus_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Mish) => (
            mish_tensor_contiguous_f64(input, meta)?,
            "cpu::mish_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::Square) => (
            square_tensor_contiguous_f64(input, meta)?,
            "cpu::square_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::IsNan) => (
            isnan_tensor_contiguous_f64(input, meta)?,
            "cpu::isnan_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::IsInf) => (
            isinf_tensor_contiguous_f64(input, meta)?,
            "cpu::isinf_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, UnaryOp::IsFinite) => (
            isfinite_tensor_contiguous_f64(input, meta)?,
            "cpu::isfinite_tensor_contiguous_f64",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for contiguous tensor unary ops",
            }
            .into());
        }
    };

    Ok(TensorUnaryDispatchOutcome {
        values,
        decision: UnaryDispatchDecision {
            op,
            mode,
            kernel,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

pub fn dispatch_tensor_unary_contiguous_f32(
    op: UnaryOp,
    mode: ExecutionMode,
    input: &[f32],
    meta: &TensorMeta,
    requires_grad: bool,
) -> Result<TensorUnaryDispatchOutcomeF32, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor_meta(meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (values, kernel) = match (effective_key, op) {
        (DispatchKey::AutogradCPU, UnaryOp::Neg) => (
            neg_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::neg_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Abs) => (
            abs_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::abs_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Exp) => (
            exp_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::exp_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Log) => (
            log_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::log_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Relu) => (
            relu_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::relu_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Sigmoid) => (
            sigmoid_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::sigmoid_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Tanh) => (
            tanh_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::tanh_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Sqrt) => (
            sqrt_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::sqrt_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Reciprocal) => (
            reciprocal_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::reciprocal_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Sin) => (
            sin_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::sin_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Cos) => (
            cos_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::cos_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Tan) => (
            tan_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::tan_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Floor) => (
            floor_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::floor_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Ceil) => (
            ceil_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::ceil_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Round) => (
            round_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::round_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Log2) => (
            log2_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::log2_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Log10) => (
            log10_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::log10_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Log1p) => (
            log1p_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::log1p_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Expm1) => (
            expm1_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::expm1_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Sign) => (
            sign_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::sign_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Trunc) => (
            trunc_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::trunc_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Frac) => (
            frac_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::frac_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Asin) => (
            asin_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::asin_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Acos) => (
            acos_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::acos_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Atan) => (
            atan_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::atan_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Sinh) => (
            sinh_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::sinh_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Cosh) => (
            cosh_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::cosh_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Gelu) => (
            gelu_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::gelu_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Silu) => (
            silu_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::silu_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::LeakyRelu) => (
            leaky_relu_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::leaky_relu_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Elu) => (
            elu_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::elu_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Rsqrt) => (
            rsqrt_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::rsqrt_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Erf) => (
            erf_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::erf_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Erfc) => (
            erfc_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::erfc_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Hardswish) => (
            hardswish_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::hardswish_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Hardsigmoid) => (
            hardsigmoid_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::hardsigmoid_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Hardtanh) => (
            hardtanh_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::hardtanh_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Softplus) => (
            softplus_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::softplus_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Mish) => (
            mish_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::mish_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::Square) => (
            square_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::square_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::IsNan) => (
            isnan_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::isnan_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::IsInf) => (
            isinf_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::isinf_tensor_contiguous_f32",
        ),
        (DispatchKey::AutogradCPU, UnaryOp::IsFinite) => (
            isfinite_tensor_contiguous_f32(input, meta)?,
            "autograd_cpu::isfinite_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Neg) => (
            neg_tensor_contiguous_f32(input, meta)?,
            "cpu::neg_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Abs) => (
            abs_tensor_contiguous_f32(input, meta)?,
            "cpu::abs_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Exp) => (
            exp_tensor_contiguous_f32(input, meta)?,
            "cpu::exp_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Log) => (
            log_tensor_contiguous_f32(input, meta)?,
            "cpu::log_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Relu) => (
            relu_tensor_contiguous_f32(input, meta)?,
            "cpu::relu_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Sigmoid) => (
            sigmoid_tensor_contiguous_f32(input, meta)?,
            "cpu::sigmoid_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Tanh) => (
            tanh_tensor_contiguous_f32(input, meta)?,
            "cpu::tanh_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Sqrt) => (
            sqrt_tensor_contiguous_f32(input, meta)?,
            "cpu::sqrt_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Reciprocal) => (
            reciprocal_tensor_contiguous_f32(input, meta)?,
            "cpu::reciprocal_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Sin) => (
            sin_tensor_contiguous_f32(input, meta)?,
            "cpu::sin_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Cos) => (
            cos_tensor_contiguous_f32(input, meta)?,
            "cpu::cos_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Tan) => (
            tan_tensor_contiguous_f32(input, meta)?,
            "cpu::tan_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Floor) => (
            floor_tensor_contiguous_f32(input, meta)?,
            "cpu::floor_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Ceil) => (
            ceil_tensor_contiguous_f32(input, meta)?,
            "cpu::ceil_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Round) => (
            round_tensor_contiguous_f32(input, meta)?,
            "cpu::round_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Log2) => (
            log2_tensor_contiguous_f32(input, meta)?,
            "cpu::log2_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Log10) => (
            log10_tensor_contiguous_f32(input, meta)?,
            "cpu::log10_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Log1p) => (
            log1p_tensor_contiguous_f32(input, meta)?,
            "cpu::log1p_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Expm1) => (
            expm1_tensor_contiguous_f32(input, meta)?,
            "cpu::expm1_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Sign) => (
            sign_tensor_contiguous_f32(input, meta)?,
            "cpu::sign_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Trunc) => (
            trunc_tensor_contiguous_f32(input, meta)?,
            "cpu::trunc_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Frac) => (
            frac_tensor_contiguous_f32(input, meta)?,
            "cpu::frac_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Asin) => (
            asin_tensor_contiguous_f32(input, meta)?,
            "cpu::asin_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Acos) => (
            acos_tensor_contiguous_f32(input, meta)?,
            "cpu::acos_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Atan) => (
            atan_tensor_contiguous_f32(input, meta)?,
            "cpu::atan_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Sinh) => (
            sinh_tensor_contiguous_f32(input, meta)?,
            "cpu::sinh_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Cosh) => (
            cosh_tensor_contiguous_f32(input, meta)?,
            "cpu::cosh_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Gelu) => (
            gelu_tensor_contiguous_f32(input, meta)?,
            "cpu::gelu_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Silu) => (
            silu_tensor_contiguous_f32(input, meta)?,
            "cpu::silu_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::LeakyRelu) => (
            leaky_relu_tensor_contiguous_f32(input, meta)?,
            "cpu::leaky_relu_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Elu) => (
            elu_tensor_contiguous_f32(input, meta)?,
            "cpu::elu_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Rsqrt) => (
            rsqrt_tensor_contiguous_f32(input, meta)?,
            "cpu::rsqrt_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Erf) => (
            erf_tensor_contiguous_f32(input, meta)?,
            "cpu::erf_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Erfc) => (
            erfc_tensor_contiguous_f32(input, meta)?,
            "cpu::erfc_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Hardswish) => (
            hardswish_tensor_contiguous_f32(input, meta)?,
            "cpu::hardswish_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Hardsigmoid) => (
            hardsigmoid_tensor_contiguous_f32(input, meta)?,
            "cpu::hardsigmoid_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Hardtanh) => (
            hardtanh_tensor_contiguous_f32(input, meta)?,
            "cpu::hardtanh_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Softplus) => (
            softplus_tensor_contiguous_f32(input, meta)?,
            "cpu::softplus_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Mish) => (
            mish_tensor_contiguous_f32(input, meta)?,
            "cpu::mish_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::Square) => (
            square_tensor_contiguous_f32(input, meta)?,
            "cpu::square_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::IsNan) => (
            isnan_tensor_contiguous_f32(input, meta)?,
            "cpu::isnan_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::IsInf) => (
            isinf_tensor_contiguous_f32(input, meta)?,
            "cpu::isinf_tensor_contiguous_f32",
        ),
        (DispatchKey::CPU, UnaryOp::IsFinite) => (
            isfinite_tensor_contiguous_f32(input, meta)?,
            "cpu::isfinite_tensor_contiguous_f32",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for contiguous tensor unary f32 ops",
            }
            .into());
        }
    };

    Ok(TensorUnaryDispatchOutcomeF32 {
        values,
        decision: UnaryDispatchDecision {
            op,
            mode,
            kernel,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

pub fn dispatch_tensor_reduction_contiguous_f64(
    op: ReductionOp,
    mode: ExecutionMode,
    input: &[f64],
    meta: &TensorMeta,
    requires_grad: bool,
) -> Result<TensorReductionDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor_meta(meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (value, kernel) = match (effective_key, op) {
        (DispatchKey::AutogradCPU, ReductionOp::Sum) => (
            sum_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::sum_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, ReductionOp::Mean) => (
            mean_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::mean_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, ReductionOp::Sum) => (
            sum_tensor_contiguous_f64(input, meta)?,
            "cpu::sum_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, ReductionOp::Mean) => (
            mean_tensor_contiguous_f64(input, meta)?,
            "cpu::mean_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, ReductionOp::Trace) => (
            trace_tensor_contiguous_f64(input, meta)?,
            "autograd_cpu::trace_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, ReductionOp::Trace) => (
            trace_tensor_contiguous_f64(input, meta)?,
            "cpu::trace_tensor_contiguous_f64",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for contiguous tensor reduction ops",
            }
            .into());
        }
    };

    Ok(TensorReductionDispatchOutcome {
        value,
        decision: ReductionDispatchDecision {
            op,
            mode,
            kernel,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

#[derive(Debug, Clone, PartialEq)]
pub struct ReductionDimDispatchDecision {
    pub op: ReductionOp,
    pub dim: usize,
    pub mode: ExecutionMode,
    pub kernel: &'static str,
    pub selected_key: DispatchKey,
    pub backend_key: DispatchKey,
    pub keyset_bits: u64,
    pub fallback_used: bool,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorReductionDimDispatchOutcome {
    pub values: Vec<f64>,
    pub decision: ReductionDimDispatchDecision,
}

pub fn dispatch_tensor_reduction_dim_contiguous_f64(
    op: ReductionOp,
    mode: ExecutionMode,
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
    requires_grad: bool,
) -> Result<TensorReductionDimDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor_meta(meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (values, kernel) = match (effective_key, op) {
        (DispatchKey::AutogradCPU, ReductionOp::Sum) => (
            sum_dim_tensor_contiguous_f64(input, meta, dim)?,
            "autograd_cpu::sum_dim_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, ReductionOp::Mean) => (
            mean_dim_tensor_contiguous_f64(input, meta, dim)?,
            "autograd_cpu::mean_dim_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, ReductionOp::Sum) => (
            sum_dim_tensor_contiguous_f64(input, meta, dim)?,
            "cpu::sum_dim_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, ReductionOp::Mean) => (
            mean_dim_tensor_contiguous_f64(input, meta, dim)?,
            "cpu::mean_dim_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, ReductionOp::Prod) => (
            prod_dim_tensor_contiguous_f64(input, meta, dim)?,
            "autograd_cpu::prod_dim_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, ReductionOp::Prod) => (
            prod_dim_tensor_contiguous_f64(input, meta, dim)?,
            "cpu::prod_dim_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, ReductionOp::Var) => (
            var_dim_tensor_contiguous_f64(input, meta, dim)?,
            "autograd_cpu::var_dim_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, ReductionOp::Var) => (
            var_dim_tensor_contiguous_f64(input, meta, dim)?,
            "cpu::var_dim_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, ReductionOp::Std) => (
            std_dim_tensor_contiguous_f64(input, meta, dim)?,
            "autograd_cpu::std_dim_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, ReductionOp::Std) => (
            std_dim_tensor_contiguous_f64(input, meta, dim)?,
            "cpu::std_dim_tensor_contiguous_f64",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason:
                    "resolved dispatch key is unsupported for contiguous tensor dim reduction ops",
            }
            .into());
        }
    };

    Ok(TensorReductionDimDispatchOutcome {
        values,
        decision: ReductionDimDispatchDecision {
            op,
            dim,
            mode,
            kernel,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

// --- Scan (cumsum/cumprod) dispatch ---

#[derive(Debug, Clone, PartialEq)]
pub struct ScanDimDispatchDecision {
    pub op: ScanOp,
    pub dim: usize,
    pub mode: ExecutionMode,
    pub kernel: &'static str,
    pub selected_key: DispatchKey,
    pub backend_key: DispatchKey,
    pub keyset_bits: u64,
    pub fallback_used: bool,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorScanDimDispatchOutcome {
    pub values: Vec<f64>,
    pub decision: ScanDimDispatchDecision,
}

pub fn dispatch_tensor_scan_dim_contiguous_f64(
    op: ScanOp,
    mode: ExecutionMode,
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
    requires_grad: bool,
) -> Result<TensorScanDimDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor_meta(meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (values, kernel) = match (effective_key, op) {
        (DispatchKey::AutogradCPU, ScanOp::CumSum) => (
            cumsum_tensor_contiguous_f64(input, meta, dim)?,
            "autograd_cpu::cumsum_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, ScanOp::CumSum) => (
            cumsum_tensor_contiguous_f64(input, meta, dim)?,
            "cpu::cumsum_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, ScanOp::CumProd) => (
            cumprod_tensor_contiguous_f64(input, meta, dim)?,
            "autograd_cpu::cumprod_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, ScanOp::CumProd) => (
            cumprod_tensor_contiguous_f64(input, meta, dim)?,
            "cpu::cumprod_tensor_contiguous_f64",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for contiguous tensor scan ops",
            }
            .into());
        }
    };

    Ok(TensorScanDimDispatchOutcome {
        values,
        decision: ScanDimDispatchDecision {
            op,
            dim,
            mode,
            kernel,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

#[derive(Debug, Clone, PartialEq)]
pub struct NormalizeDimDispatchDecision {
    pub op: NormalizeOp,
    pub dim: usize,
    pub mode: ExecutionMode,
    pub kernel: &'static str,
    pub selected_key: DispatchKey,
    pub backend_key: DispatchKey,
    pub keyset_bits: u64,
    pub fallback_used: bool,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorNormalizeDimDispatchOutcome {
    pub values: Vec<f64>,
    pub decision: NormalizeDimDispatchDecision,
}

pub fn dispatch_tensor_normalize_dim_contiguous_f64(
    op: NormalizeOp,
    mode: ExecutionMode,
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
    requires_grad: bool,
) -> Result<TensorNormalizeDimDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor_meta(meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (values, kernel) = match (effective_key, op) {
        (DispatchKey::AutogradCPU, NormalizeOp::Softmax) => (
            softmax_dim_tensor_contiguous_f64(input, meta, dim)?,
            "autograd_cpu::softmax_dim_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, NormalizeOp::LogSoftmax) => (
            log_softmax_dim_tensor_contiguous_f64(input, meta, dim)?,
            "autograd_cpu::log_softmax_dim_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, NormalizeOp::Softmax) => (
            softmax_dim_tensor_contiguous_f64(input, meta, dim)?,
            "cpu::softmax_dim_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, NormalizeOp::LogSoftmax) => (
            log_softmax_dim_tensor_contiguous_f64(input, meta, dim)?,
            "cpu::log_softmax_dim_tensor_contiguous_f64",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason:
                    "resolved dispatch key is unsupported for contiguous tensor normalize dim ops",
            }
            .into());
        }
    };

    Ok(TensorNormalizeDimDispatchOutcome {
        values,
        decision: NormalizeDimDispatchDecision {
            op,
            dim,
            mode,
            kernel,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

#[derive(Debug, Clone, PartialEq)]
pub struct JoinDispatchDecision {
    pub op: JoinOp,
    pub dim: usize,
    pub num_inputs: usize,
    pub mode: ExecutionMode,
    pub kernel: &'static str,
    pub selected_key: DispatchKey,
    pub backend_key: DispatchKey,
    pub keyset_bits: u64,
    pub fallback_used: bool,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorJoinDispatchOutcome {
    pub values: Vec<f64>,
    pub decision: JoinDispatchDecision,
}

pub fn dispatch_tensor_join_contiguous_f64(
    op: JoinOp,
    mode: ExecutionMode,
    inputs: &[(&[f64], &TensorMeta)],
    dim: usize,
    requires_grad: bool,
) -> Result<TensorJoinDispatchOutcome, DispatchError> {
    if inputs.is_empty() {
        return Err(DispatchKeyError::IncompatibleSet {
            reason: "join op requires at least one input",
        }
        .into());
    }
    let first_meta = inputs[0].1;
    for &(_, meta) in &inputs[1..] {
        ensure_tensor_meta_compatible(first_meta, meta)?;
    }
    let keyset = dispatch_keyset_for_single_tensor_meta(first_meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (values, kernel) = match (effective_key, op) {
        (DispatchKey::AutogradCPU, JoinOp::Cat) => (
            cat_tensor_contiguous_f64(inputs, dim)?,
            "autograd_cpu::cat_tensor_contiguous_f64",
        ),
        (DispatchKey::AutogradCPU, JoinOp::Stack) => (
            stack_tensor_contiguous_f64(inputs, dim)?,
            "autograd_cpu::stack_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, JoinOp::Cat) => (
            cat_tensor_contiguous_f64(inputs, dim)?,
            "cpu::cat_tensor_contiguous_f64",
        ),
        (DispatchKey::CPU, JoinOp::Stack) => (
            stack_tensor_contiguous_f64(inputs, dim)?,
            "cpu::stack_tensor_contiguous_f64",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for contiguous tensor join ops",
            }
            .into());
        }
    };

    Ok(TensorJoinDispatchOutcome {
        values,
        decision: JoinDispatchDecision {
            op,
            dim,
            num_inputs: inputs.len(),
            mode,
            kernel,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

pub fn dispatch_scalar_comparison(
    op: ComparisonOp,
    mode: ExecutionMode,
    lhs: &ScalarTensor,
    rhs: &ScalarTensor,
    requires_grad: bool,
) -> Result<ComparisonDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_tensors(lhs, rhs, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (tensor, kernel) = match effective_key {
        DispatchKey::AutogradCPU => match op {
            ComparisonOp::Eq => (eq_scalar(lhs, rhs)?, "autograd_cpu::eq_scalar"),
            ComparisonOp::Ne => (ne_scalar(lhs, rhs)?, "autograd_cpu::ne_scalar"),
            ComparisonOp::Lt => (lt_scalar(lhs, rhs)?, "autograd_cpu::lt_scalar"),
            ComparisonOp::Gt => (gt_scalar(lhs, rhs)?, "autograd_cpu::gt_scalar"),
            ComparisonOp::Le => (le_scalar(lhs, rhs)?, "autograd_cpu::le_scalar"),
            ComparisonOp::Ge => (ge_scalar(lhs, rhs)?, "autograd_cpu::ge_scalar"),
        },
        DispatchKey::CPU => match op {
            ComparisonOp::Eq => (eq_scalar(lhs, rhs)?, "cpu::eq_scalar"),
            ComparisonOp::Ne => (ne_scalar(lhs, rhs)?, "cpu::ne_scalar"),
            ComparisonOp::Lt => (lt_scalar(lhs, rhs)?, "cpu::lt_scalar"),
            ComparisonOp::Gt => (gt_scalar(lhs, rhs)?, "cpu::gt_scalar"),
            ComparisonOp::Le => (le_scalar(lhs, rhs)?, "cpu::le_scalar"),
            ComparisonOp::Ge => (ge_scalar(lhs, rhs)?, "cpu::ge_scalar"),
        },
        _ => Err(DispatchKeyError::IncompatibleSet {
            reason: "resolved dispatch key is unsupported for scalar comparison ops",
        })?,
    };

    Ok(ComparisonDispatchOutcome {
        tensor,
        decision: ComparisonDispatchDecision {
            op,
            mode,
            kernel,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

pub fn dispatch_tensor_comparison_contiguous_f64(
    op: ComparisonOp,
    mode: ExecutionMode,
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
    requires_grad: bool,
) -> Result<TensorComparisonDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_tensor_meta(lhs_meta, rhs_meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (values, kernel) = match effective_key {
        DispatchKey::AutogradCPU => match op {
            ComparisonOp::Eq => (
                eq_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
                "autograd_cpu::eq_tensor_contiguous_f64",
            ),
            ComparisonOp::Ne => (
                ne_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
                "autograd_cpu::ne_tensor_contiguous_f64",
            ),
            ComparisonOp::Lt => (
                lt_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
                "autograd_cpu::lt_tensor_contiguous_f64",
            ),
            ComparisonOp::Gt => (
                gt_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
                "autograd_cpu::gt_tensor_contiguous_f64",
            ),
            ComparisonOp::Le => (
                le_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
                "autograd_cpu::le_tensor_contiguous_f64",
            ),
            ComparisonOp::Ge => (
                ge_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
                "autograd_cpu::ge_tensor_contiguous_f64",
            ),
        },
        DispatchKey::CPU => match op {
            ComparisonOp::Eq => (
                eq_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
                "cpu::eq_tensor_contiguous_f64",
            ),
            ComparisonOp::Ne => (
                ne_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
                "cpu::ne_tensor_contiguous_f64",
            ),
            ComparisonOp::Lt => (
                lt_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
                "cpu::lt_tensor_contiguous_f64",
            ),
            ComparisonOp::Gt => (
                gt_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
                "cpu::gt_tensor_contiguous_f64",
            ),
            ComparisonOp::Le => (
                le_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
                "cpu::le_tensor_contiguous_f64",
            ),
            ComparisonOp::Ge => (
                ge_tensor_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta)?,
                "cpu::ge_tensor_contiguous_f64",
            ),
        },
        _ => Err(DispatchKeyError::IncompatibleSet {
            reason: "resolved dispatch key is unsupported for contiguous tensor comparison ops",
        })?,
    };

    Ok(TensorComparisonDispatchOutcome {
        values,
        decision: ComparisonDispatchDecision {
            op,
            mode,
            kernel,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

#[derive(Debug, Clone, PartialEq)]
pub struct PowDispatchDecision {
    pub mode: ExecutionMode,
    pub kernel: &'static str,
    pub exponent: f64,
    pub selected_key: DispatchKey,
    pub backend_key: DispatchKey,
    pub keyset_bits: u64,
    pub fallback_used: bool,
}

#[derive(Debug, Clone, PartialEq)]
pub struct PowDispatchOutcome {
    pub tensor: ScalarTensor,
    pub decision: PowDispatchDecision,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorPowDispatchOutcome {
    pub values: Vec<f64>,
    pub decision: PowDispatchDecision,
}

pub fn dispatch_scalar_pow(
    mode: ExecutionMode,
    input: &ScalarTensor,
    exponent: f64,
    requires_grad: bool,
) -> Result<PowDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor(input, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (tensor, kernel) = match effective_key {
        DispatchKey::AutogradCPU => (pow_scalar(input, exponent), "autograd_cpu::pow_scalar"),
        DispatchKey::CPU => (pow_scalar(input, exponent), "cpu::pow_scalar"),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for scalar pow op",
            }
            .into());
        }
    };

    Ok(PowDispatchOutcome {
        tensor,
        decision: PowDispatchDecision {
            mode,
            kernel,
            exponent,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

pub fn dispatch_tensor_pow_contiguous_f64(
    mode: ExecutionMode,
    input: &[f64],
    meta: &TensorMeta,
    exponent: f64,
    requires_grad: bool,
) -> Result<TensorPowDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor_meta(meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (values, kernel) = match effective_key {
        DispatchKey::AutogradCPU => (
            pow_tensor_contiguous_f64(input, meta, exponent)?,
            "autograd_cpu::pow_tensor_contiguous_f64",
        ),
        DispatchKey::CPU => (
            pow_tensor_contiguous_f64(input, meta, exponent)?,
            "cpu::pow_tensor_contiguous_f64",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for contiguous tensor pow op",
            }
            .into());
        }
    };

    Ok(TensorPowDispatchOutcome {
        values,
        decision: PowDispatchDecision {
            mode,
            kernel,
            exponent,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

#[derive(Debug, Clone, PartialEq)]
pub struct NormDispatchDecision {
    pub mode: ExecutionMode,
    pub kernel: &'static str,
    pub p: f64,
    pub selected_key: DispatchKey,
    pub backend_key: DispatchKey,
    pub keyset_bits: u64,
    pub fallback_used: bool,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorNormDispatchOutcome {
    pub value: f64,
    pub decision: NormDispatchDecision,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorNormDimDispatchOutcome {
    pub values: Vec<f64>,
    pub decision: NormDispatchDecision,
}

pub fn dispatch_tensor_norm_contiguous_f64(
    mode: ExecutionMode,
    input: &[f64],
    meta: &TensorMeta,
    p: f64,
    requires_grad: bool,
) -> Result<TensorNormDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor_meta(meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (value, kernel) = match effective_key {
        DispatchKey::AutogradCPU => (
            norm_tensor_contiguous_f64(input, meta, p)?,
            "autograd_cpu::norm_tensor_contiguous_f64",
        ),
        DispatchKey::CPU => (
            norm_tensor_contiguous_f64(input, meta, p)?,
            "cpu::norm_tensor_contiguous_f64",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for contiguous tensor norm op",
            }
            .into());
        }
    };

    Ok(TensorNormDispatchOutcome {
        value,
        decision: NormDispatchDecision {
            mode,
            kernel,
            p,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

pub fn dispatch_tensor_norm_dim_contiguous_f64(
    mode: ExecutionMode,
    input: &[f64],
    meta: &TensorMeta,
    p: f64,
    dim: usize,
    requires_grad: bool,
) -> Result<TensorNormDimDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor_meta(meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (values, kernel) = match effective_key {
        DispatchKey::AutogradCPU => (
            norm_dim_tensor_contiguous_f64(input, meta, p, dim)?,
            "autograd_cpu::norm_dim_tensor_contiguous_f64",
        ),
        DispatchKey::CPU => (
            norm_dim_tensor_contiguous_f64(input, meta, p, dim)?,
            "cpu::norm_dim_tensor_contiguous_f64",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for contiguous tensor norm dim op",
            }
            .into());
        }
    };

    Ok(TensorNormDimDispatchOutcome {
        values,
        decision: NormDispatchDecision {
            mode,
            kernel,
            p,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

#[derive(Debug, Clone, PartialEq)]
pub struct LerpDispatchDecision {
    pub mode: ExecutionMode,
    pub kernel: &'static str,
    pub weight: f64,
    pub selected_key: DispatchKey,
    pub backend_key: DispatchKey,
    pub keyset_bits: u64,
    pub fallback_used: bool,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorLerpDispatchOutcome {
    pub values: Vec<f64>,
    pub decision: LerpDispatchDecision,
}

pub fn dispatch_tensor_lerp_contiguous_f64(
    mode: ExecutionMode,
    start: &[f64],
    end: &[f64],
    weight: f64,
    meta: &TensorMeta,
    requires_grad: bool,
) -> Result<TensorLerpDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor_meta(meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (values, kernel) = match effective_key {
        DispatchKey::AutogradCPU => (
            lerp_tensor_contiguous_f64(start, end, weight, meta)?,
            "autograd_cpu::lerp_tensor_contiguous_f64",
        ),
        DispatchKey::CPU => (
            lerp_tensor_contiguous_f64(start, end, weight, meta)?,
            "cpu::lerp_tensor_contiguous_f64",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for contiguous tensor lerp op",
            }
            .into());
        }
    };

    Ok(TensorLerpDispatchOutcome {
        values,
        decision: LerpDispatchDecision {
            mode,
            kernel,
            weight,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

#[derive(Debug, Clone, PartialEq)]
pub struct AddmmDispatchDecision {
    pub mode: ExecutionMode,
    pub kernel: &'static str,
    pub beta: f64,
    pub alpha: f64,
    pub selected_key: DispatchKey,
    pub backend_key: DispatchKey,
    pub keyset_bits: u64,
    pub fallback_used: bool,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorAddmmDispatchOutcome {
    pub values: Vec<f64>,
    pub decision: AddmmDispatchDecision,
}

#[allow(clippy::too_many_arguments)]
pub fn dispatch_tensor_addmm_contiguous_f64(
    mode: ExecutionMode,
    input: &[f64],
    mat1: &[f64],
    mat2: &[f64],
    input_meta: &TensorMeta,
    mat1_meta: &TensorMeta,
    mat2_meta: &TensorMeta,
    beta: f64,
    alpha: f64,
    requires_grad: bool,
) -> Result<TensorAddmmDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor_meta(mat1_meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (values, kernel) = match effective_key {
        DispatchKey::AutogradCPU => (
            addmm_tensor_contiguous_f64(
                input, mat1, mat2, input_meta, mat1_meta, mat2_meta, beta, alpha,
            )?,
            "autograd_cpu::addmm_tensor_contiguous_f64",
        ),
        DispatchKey::CPU => (
            addmm_tensor_contiguous_f64(
                input, mat1, mat2, input_meta, mat1_meta, mat2_meta, beta, alpha,
            )?,
            "cpu::addmm_tensor_contiguous_f64",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for contiguous tensor addmm op",
            }
            .into());
        }
    };

    Ok(TensorAddmmDispatchOutcome {
        values,
        decision: AddmmDispatchDecision {
            mode,
            kernel,
            beta,
            alpha,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorAddmvDispatchOutcome {
    pub values: Vec<f64>,
    pub decision: AddmmDispatchDecision,
}

#[allow(clippy::too_many_arguments)]
pub fn dispatch_tensor_addmv_contiguous_f64(
    mode: ExecutionMode,
    input: &[f64],
    mat: &[f64],
    vec_data: &[f64],
    input_meta: &TensorMeta,
    mat_meta: &TensorMeta,
    vec_meta: &TensorMeta,
    beta: f64,
    alpha: f64,
    requires_grad: bool,
) -> Result<TensorAddmvDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor_meta(mat_meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (values, kernel) = match effective_key {
        DispatchKey::AutogradCPU => (
            addmv_tensor_contiguous_f64(
                input, mat, vec_data, input_meta, mat_meta, vec_meta, beta, alpha,
            )?,
            "autograd_cpu::addmv_tensor_contiguous_f64",
        ),
        DispatchKey::CPU => (
            addmv_tensor_contiguous_f64(
                input, mat, vec_data, input_meta, mat_meta, vec_meta, beta, alpha,
            )?,
            "cpu::addmv_tensor_contiguous_f64",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for contiguous tensor addmv op",
            }
            .into());
        }
    };

    Ok(TensorAddmvDispatchOutcome {
        values,
        decision: AddmmDispatchDecision {
            mode,
            kernel,
            beta,
            alpha,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

#[derive(Debug, Clone, PartialEq)]
pub struct ClampDispatchDecision {
    pub mode: ExecutionMode,
    pub kernel: &'static str,
    pub min_val: f64,
    pub max_val: f64,
    pub selected_key: DispatchKey,
    pub backend_key: DispatchKey,
    pub keyset_bits: u64,
    pub fallback_used: bool,
}

#[derive(Debug, Clone, PartialEq)]
pub struct ClampDispatchOutcome {
    pub tensor: ScalarTensor,
    pub decision: ClampDispatchDecision,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorClampDispatchOutcome {
    pub values: Vec<f64>,
    pub decision: ClampDispatchDecision,
}

pub fn dispatch_scalar_clamp(
    mode: ExecutionMode,
    input: &ScalarTensor,
    min_val: f64,
    max_val: f64,
    requires_grad: bool,
) -> Result<ClampDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor(input, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (tensor, kernel) = match effective_key {
        DispatchKey::AutogradCPU => (
            clamp_scalar(input, min_val, max_val),
            "autograd_cpu::clamp_scalar",
        ),
        DispatchKey::CPU => (clamp_scalar(input, min_val, max_val), "cpu::clamp_scalar"),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for scalar clamp op",
            }
            .into());
        }
    };

    Ok(ClampDispatchOutcome {
        tensor,
        decision: ClampDispatchDecision {
            mode,
            kernel,
            min_val,
            max_val,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

pub fn dispatch_tensor_clamp_contiguous_f64(
    mode: ExecutionMode,
    input: &[f64],
    meta: &TensorMeta,
    min_val: f64,
    max_val: f64,
    requires_grad: bool,
) -> Result<TensorClampDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor_meta(meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let (values, kernel) = match effective_key {
        DispatchKey::AutogradCPU => (
            clamp_tensor_contiguous_f64(input, meta, min_val, max_val)?,
            "autograd_cpu::clamp_tensor_contiguous_f64",
        ),
        DispatchKey::CPU => (
            clamp_tensor_contiguous_f64(input, meta, min_val, max_val)?,
            "cpu::clamp_tensor_contiguous_f64",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for contiguous tensor clamp op",
            }
            .into());
        }
    };

    Ok(TensorClampDispatchOutcome {
        values,
        decision: ClampDispatchDecision {
            mode,
            kernel,
            min_val,
            max_val,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

// --- Sort dispatch ---

#[derive(Debug, Clone, PartialEq)]
pub struct SortDispatchDecision {
    pub dim: usize,
    pub descending: bool,
    pub mode: ExecutionMode,
    pub kernel: &'static str,
    pub selected_key: DispatchKey,
    pub backend_key: DispatchKey,
    pub keyset_bits: u64,
    pub fallback_used: bool,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorSortDispatchOutcome {
    pub values: Vec<f64>,
    pub indices: Vec<usize>,
    pub decision: SortDispatchDecision,
}

pub fn dispatch_tensor_sort_contiguous_f64(
    mode: ExecutionMode,
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
    descending: bool,
    requires_grad: bool,
) -> Result<TensorSortDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor_meta(meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let ((values, indices), kernel) = match effective_key {
        DispatchKey::AutogradCPU => (
            sort_tensor_contiguous_f64(input, meta, dim, descending)?,
            "autograd_cpu::sort_tensor_contiguous_f64",
        ),
        DispatchKey::CPU => (
            sort_tensor_contiguous_f64(input, meta, dim, descending)?,
            "cpu::sort_tensor_contiguous_f64",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for contiguous tensor sort op",
            }
            .into());
        }
    };

    Ok(TensorSortDispatchOutcome {
        values,
        indices,
        decision: SortDispatchDecision {
            dim,
            descending,
            mode,
            kernel,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

// --- TopK dispatch ---

#[derive(Debug, Clone, PartialEq)]
pub struct TopKDispatchDecision {
    pub k: usize,
    pub dim: usize,
    pub largest: bool,
    pub sorted: bool,
    pub mode: ExecutionMode,
    pub kernel: &'static str,
    pub selected_key: DispatchKey,
    pub backend_key: DispatchKey,
    pub keyset_bits: u64,
    pub fallback_used: bool,
}

#[derive(Debug, Clone, PartialEq)]
pub struct TensorTopKDispatchOutcome {
    pub values: Vec<f64>,
    pub indices: Vec<usize>,
    pub decision: TopKDispatchDecision,
}

#[allow(clippy::too_many_arguments)]
pub fn dispatch_tensor_topk_contiguous_f64(
    mode: ExecutionMode,
    input: &[f64],
    meta: &TensorMeta,
    k: usize,
    dim: usize,
    largest: bool,
    sorted: bool,
    requires_grad: bool,
) -> Result<TensorTopKDispatchOutcome, DispatchError> {
    let keyset = dispatch_keyset_for_single_tensor_meta(meta, requires_grad);
    let (selected_key, backend_key, effective_key, fallback_used) =
        resolve_dispatch_keys(mode, keyset)?;

    let ((values, indices), kernel) = match effective_key {
        DispatchKey::AutogradCPU => (
            topk_tensor_contiguous_f64(input, meta, k, dim, largest, sorted)?,
            "autograd_cpu::topk_tensor_contiguous_f64",
        ),
        DispatchKey::CPU => (
            topk_tensor_contiguous_f64(input, meta, k, dim, largest, sorted)?,
            "cpu::topk_tensor_contiguous_f64",
        ),
        _ => {
            return Err(DispatchKeyError::IncompatibleSet {
                reason: "resolved dispatch key is unsupported for contiguous tensor topk op",
            }
            .into());
        }
    };

    Ok(TensorTopKDispatchOutcome {
        values,
        indices,
        decision: TopKDispatchDecision {
            k,
            dim,
            largest,
            sorted,
            mode,
            kernel,
            selected_key,
            backend_key,
            keyset_bits: keyset.bits(),
            fallback_used,
        },
    })
}

// --- Typed dispatch wrappers (f32/f64 routing) ---

pub fn dispatch_tensor_unary_contiguous_typed(
    op: UnaryOp,
    mode: ExecutionMode,
    storage: &TensorStorage,
    meta: &TensorMeta,
    requires_grad: bool,
) -> Result<TypedUnaryOutcome, DispatchError> {
    match storage {
        TensorStorage::F64(data) => {
            let outcome =
                dispatch_tensor_unary_contiguous_f64(op, mode, data, meta, requires_grad)?;
            Ok(TypedUnaryOutcome {
                storage: TensorStorage::F64(outcome.values),
                decision: outcome.decision,
            })
        }
        TensorStorage::F32(data) => {
            let outcome =
                dispatch_tensor_unary_contiguous_f32(op, mode, data, meta, requires_grad)?;
            Ok(TypedUnaryOutcome {
                storage: TensorStorage::F32(outcome.values),
                decision: outcome.decision,
            })
        }
    }
}

pub fn dispatch_tensor_binary_contiguous_typed(
    op: BinaryOp,
    mode: ExecutionMode,
    lhs_storage: &TensorStorage,
    rhs_storage: &TensorStorage,
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
    requires_grad: bool,
) -> Result<TypedBinaryOutcome, DispatchError> {
    match (lhs_storage, rhs_storage) {
        (TensorStorage::F64(lhs), TensorStorage::F64(rhs)) => {
            let outcome = dispatch_tensor_binary_contiguous_f64(
                op,
                mode,
                lhs,
                rhs,
                lhs_meta,
                rhs_meta,
                requires_grad,
            )?;
            Ok(TypedBinaryOutcome {
                storage: TensorStorage::F64(outcome.values),
                decision: outcome.decision,
            })
        }
        (TensorStorage::F32(lhs), TensorStorage::F32(rhs)) => {
            let outcome = dispatch_tensor_binary_contiguous_f32(
                op,
                mode,
                lhs,
                rhs,
                lhs_meta,
                rhs_meta,
                requires_grad,
            )?;
            Ok(TypedBinaryOutcome {
                storage: TensorStorage::F32(outcome.values),
                decision: outcome.decision,
            })
        }
        // Mixed dtypes: promote f32 to f64
        (TensorStorage::F64(lhs), TensorStorage::F32(rhs_f32)) => {
            let rhs: Vec<f64> = rhs_f32.iter().map(|&v| f64::from(v)).collect();
            let promoted_rhs_meta = rhs_meta.clone().with_dtype(DType::F64);
            let outcome = dispatch_tensor_binary_contiguous_f64(
                op,
                mode,
                lhs,
                &rhs,
                lhs_meta,
                &promoted_rhs_meta,
                requires_grad,
            )?;
            Ok(TypedBinaryOutcome {
                storage: TensorStorage::F64(outcome.values),
                decision: outcome.decision,
            })
        }
        (TensorStorage::F32(lhs_f32), TensorStorage::F64(rhs)) => {
            let lhs: Vec<f64> = lhs_f32.iter().map(|&v| f64::from(v)).collect();
            let promoted_lhs_meta = lhs_meta.clone().with_dtype(DType::F64);
            let outcome = dispatch_tensor_binary_contiguous_f64(
                op,
                mode,
                &lhs,
                rhs,
                &promoted_lhs_meta,
                rhs_meta,
                requires_grad,
            )?;
            Ok(TypedBinaryOutcome {
                storage: TensorStorage::F64(outcome.values),
                decision: outcome.decision,
            })
        }
    }
}

#[cfg(test)]
mod tests {
    use std::collections::BTreeMap;

    use ft_core::{DType, Device, ExecutionMode, ScalarTensor, TensorCompatError, TensorMeta};
    use ft_kernel_cpu::KernelError;
    use proptest::prelude::*;

    use super::{
        BinaryOp, ComparisonOp, DispatchError, DispatchKey, DispatchKeyError, DispatchKeySet,
        OpSchemaError, ParsedSchemaInput, SchemaDispatchError, SchemaRegistry, SchemaRegistryError,
        TYPE_PRIORITY, UnaryOp, dispatch_keyset_for_tensor_meta, dispatch_keyset_for_tensors,
        dispatch_scalar_binary, dispatch_scalar_binary_registered,
        dispatch_scalar_binary_with_keyset, dispatch_scalar_comparison, dispatch_scalar_unary,
        dispatch_tensor_binary_contiguous_f64, dispatch_tensor_binary_contiguous_f64_with_keyset,
        dispatch_tensor_comparison_contiguous_f64, dispatch_tensor_unary_contiguous_f64,
        parse_schema_name, parse_schema_or_name, schema_dispatch_keyset_from_tags,
    };

    fn det_seed(parts: &[u64]) -> u64 {
        let mut hash = 0xcbf2_9ce4_8422_2325u64;
        for value in parts {
            for byte in value.to_le_bytes() {
                hash ^= u64::from(byte);
                hash = hash.wrapping_mul(0x0000_0100_0000_01b3);
            }
        }
        hash
    }

    fn build_property_log(
        test_id: &str,
        mode: &str,
        seed: u64,
        input_digest: u64,
        output_digest: u64,
        reason_code: &str,
    ) -> BTreeMap<String, String> {
        let mut log = BTreeMap::new();
        let scenario_id = format!("dispatch_key/{mode}:{test_id}");
        log.insert("ts_utc".to_string(), "1970-01-01T00:00:00Z".to_string());
        log.insert("suite_id".to_string(), "ft_dispatch_property".to_string());
        log.insert("test_id".to_string(), test_id.to_string());
        log.insert("packet_id".to_string(), "FT-P2C-002".to_string());
        log.insert(
            "fixture_id".to_string(),
            "ft_dispatch_property_generated".to_string(),
        );
        log.insert("scenario_id".to_string(), scenario_id);
        log.insert("mode".to_string(), mode.to_string());
        log.insert("seed".to_string(), seed.to_string());
        log.insert(
            "input_digest".to_string(),
            format!("det64:{input_digest:016x}"),
        );
        log.insert(
            "output_digest".to_string(),
            format!("det64:{output_digest:016x}"),
        );
        log.insert(
            "env_fingerprint".to_string(),
            "det64:ft-dispatch-test".to_string(),
        );
        log.insert(
            "artifact_refs".to_string(),
            "artifacts/phase2c/FT-P2C-002/fixture_manifest.json".to_string(),
        );
        log.insert(
            "replay_command".to_string(),
            format!("cargo test -p ft-dispatch {test_id} -- --nocapture"),
        );
        log.insert("duration_ms".to_string(), "0".to_string());
        log.insert("outcome".to_string(), "pass".to_string());
        log.insert("contract_id".to_string(), reason_code.to_string());
        log.insert("shrink_trace".to_string(), "none".to_string());
        log.insert("reason_code".to_string(), reason_code.to_string());
        log
    }

    fn build_packet_007_property_log(
        test_id: &str,
        mode: &str,
        seed: u64,
        input_digest: u64,
        output_digest: u64,
        reason_code: &str,
        scenario_id: &str,
    ) -> BTreeMap<String, String> {
        let mut log = build_property_log(
            test_id,
            mode,
            seed,
            input_digest,
            output_digest,
            reason_code,
        );
        log.insert("packet_id".to_string(), "FT-P2C-007".to_string());
        log.insert("scenario_id".to_string(), scenario_id.to_string());
        log.insert(
            "artifact_refs".to_string(),
            "artifacts/phase2c/FT-P2C-007/contract_table.md".to_string(),
        );
        log
    }

    fn assert_log_contract(log: &BTreeMap<String, String>) {
        for key in [
            "ts_utc",
            "suite_id",
            "test_id",
            "packet_id",
            "fixture_id",
            "scenario_id",
            "mode",
            "seed",
            "input_digest",
            "output_digest",
            "env_fingerprint",
            "artifact_refs",
            "replay_command",
            "duration_ms",
            "outcome",
            "contract_id",
            "shrink_trace",
            "reason_code",
        ] {
            assert!(
                log.contains_key(key),
                "property log missing required key '{key}'"
            );
        }
    }

    struct SchemaLogParams<'a> {
        test_id: &'a str,
        mode: &'a str,
        seed: u64,
        op_name: &'a str,
        overload_name: &'a str,
        schema_digest: u64,
        dispatch_keyset_bits: u64,
        reason_code: &'a str,
    }

    fn build_schema_property_log(params: SchemaLogParams<'_>) -> BTreeMap<String, String> {
        let mut log = BTreeMap::new();
        let scenario_id = format!("op_schema/{}:{}", params.mode, params.test_id);
        log.insert("ts_utc".to_string(), "1970-01-01T00:00:00Z".to_string());
        log.insert(
            "suite_id".to_string(),
            "ft_dispatch_schema_property".to_string(),
        );
        log.insert("test_id".to_string(), params.test_id.to_string());
        log.insert("packet_id".to_string(), "FT-P2C-003".to_string());
        log.insert(
            "fixture_id".to_string(),
            "ft_dispatch_schema_property_generated".to_string(),
        );
        log.insert("scenario_id".to_string(), scenario_id);
        log.insert("mode".to_string(), params.mode.to_string());
        log.insert("seed".to_string(), params.seed.to_string());
        log.insert("op_name".to_string(), params.op_name.to_string());
        log.insert(
            "overload_name".to_string(),
            params.overload_name.to_string(),
        );
        log.insert(
            "schema_digest".to_string(),
            format!("det64:{:016x}", params.schema_digest),
        );
        log.insert(
            "dispatch_keyset_bits".to_string(),
            format!("0x{:016x}", params.dispatch_keyset_bits),
        );
        log.insert(
            "env_fingerprint".to_string(),
            "det64:ft-dispatch-schema-test".to_string(),
        );
        log.insert(
            "artifact_refs".to_string(),
            "artifacts/phase2c/FT-P2C-003/contract_table.md".to_string(),
        );
        log.insert(
            "replay_command".to_string(),
            format!(
                "cargo test -p ft-dispatch {} -- --nocapture",
                params.test_id
            ),
        );
        log.insert("duration_ms".to_string(), "0".to_string());
        log.insert("outcome".to_string(), "pass".to_string());
        log.insert("contract_id".to_string(), params.reason_code.to_string());
        log.insert("shrink_trace".to_string(), "none".to_string());
        log.insert("reason_code".to_string(), params.reason_code.to_string());
        log
    }

    fn assert_schema_log_contract(log: &BTreeMap<String, String>) {
        for key in [
            "ts_utc",
            "suite_id",
            "test_id",
            "packet_id",
            "fixture_id",
            "scenario_id",
            "mode",
            "seed",
            "op_name",
            "overload_name",
            "schema_digest",
            "dispatch_keyset_bits",
            "env_fingerprint",
            "artifact_refs",
            "replay_command",
            "duration_ms",
            "outcome",
            "contract_id",
            "shrink_trace",
            "reason_code",
        ] {
            assert!(
                log.contains_key(key),
                "schema property log missing required key '{key}'"
            );
        }
    }

    #[test]
    fn dispatch_keyset_set_algebra_is_stable() {
        let mut left = DispatchKeySet::from_keys(&[DispatchKey::CPU, DispatchKey::BackendSelect]);
        let right = DispatchKeySet::from_keys(&[DispatchKey::AutogradCPU, DispatchKey::CPU]);

        let union = left.union(right);
        assert!(union.has(DispatchKey::CPU));
        assert!(union.has(DispatchKey::AutogradCPU));
        assert!(union.has(DispatchKey::BackendSelect));

        let intersection = left.intersection(right);
        assert!(intersection.has(DispatchKey::CPU));
        assert!(!intersection.has(DispatchKey::AutogradCPU));

        left.remove(DispatchKey::BackendSelect);
        assert!(!left.has(DispatchKey::BackendSelect));
    }

    #[test]
    fn priority_resolution_prefers_autograd_cpu() {
        let keys = DispatchKeySet::from_keys(&[
            DispatchKey::BackendSelect,
            DispatchKey::CPU,
            DispatchKey::AutogradCPU,
        ]);
        let selected = keys
            .highest_priority_type_id()
            .expect("priority resolution should succeed");
        assert_eq!(selected, DispatchKey::AutogradCPU);
    }

    #[test]
    fn backend_priority_returns_cpu() {
        let keys = DispatchKeySet::from_keys(&[DispatchKey::BackendSelect, DispatchKey::CPU]);
        let backend = keys
            .highest_priority_backend_type_id()
            .expect("backend priority should resolve");
        assert_eq!(backend, DispatchKey::CPU);
    }

    #[test]
    fn unknown_bits_fail_closed() {
        let err =
            DispatchKeySet::from_bits_checked(1u64 << 63).expect_err("unknown bits must fail");
        let msg = err.to_string();
        assert!(msg.contains("unknown bitmask"));
    }

    #[test]
    fn known_bits_parse_successfully() {
        let known_bits = DispatchKey::BackendSelect.bit()
            | DispatchKey::CompositeImplicitAutograd.bit()
            | DispatchKey::CompositeExplicitAutograd.bit()
            | DispatchKey::CPU.bit()
            | DispatchKey::AutogradCPU.bit();
        let parsed = DispatchKeySet::from_bits_checked(known_bits)
            .expect("known dispatch key bits must parse successfully");
        assert_eq!(parsed.bits(), known_bits);
    }

    #[test]
    fn dispatch_keyset_for_tensors_tracks_requires_grad() {
        let lhs = ScalarTensor::new(1.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);

        let no_grad = dispatch_keyset_for_tensors(&lhs, &rhs, false);
        assert!(no_grad.has(DispatchKey::CPU));
        assert!(no_grad.has(DispatchKey::BackendSelect));
        assert!(!no_grad.has(DispatchKey::AutogradCPU));

        let with_grad = dispatch_keyset_for_tensors(&lhs, &rhs, true);
        assert!(with_grad.has(DispatchKey::CPU));
        assert!(with_grad.has(DispatchKey::BackendSelect));
        assert!(with_grad.has(DispatchKey::AutogradCPU));
    }

    #[test]
    fn validate_requires_cpu_for_autograd() {
        let keyset = DispatchKeySet::from_keys(&[DispatchKey::AutogradCPU]);
        let err = keyset
            .validate_for_scalar_binary()
            .expect_err("autograd without cpu must fail");
        assert!(matches!(err, DispatchKeyError::IncompatibleSet { .. }));
    }

    #[test]
    fn strict_mode_rejects_composite_fallback() {
        let lhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(3.0, DType::F64, Device::Cpu);
        let keyset = DispatchKeySet::from_keys(&[
            DispatchKey::CompositeExplicitAutograd,
            DispatchKey::CPU,
            DispatchKey::BackendSelect,
        ]);

        let err = dispatch_scalar_binary_with_keyset(
            BinaryOp::Add,
            ExecutionMode::Strict,
            &lhs,
            &rhs,
            keyset,
        )
        .expect_err("strict mode must fail closed");
        assert!(err.to_string().contains("strict mode forbids"));
    }

    #[test]
    fn strict_mode_prefers_cpu_over_backendselect() {
        let lhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(3.0, DType::F64, Device::Cpu);
        let keyset = DispatchKeySet::from_keys(&[DispatchKey::BackendSelect, DispatchKey::CPU]);

        let out = dispatch_scalar_binary_with_keyset(
            BinaryOp::Add,
            ExecutionMode::Strict,
            &lhs,
            &rhs,
            keyset,
        )
        .expect("strict mode should resolve directly to cpu when cpu type key exists");
        assert_eq!(out.tensor.value(), 5.0);
        assert_eq!(out.decision.selected_key, DispatchKey::CPU);
        assert_eq!(out.decision.backend_key, DispatchKey::CPU);
        assert!(!out.decision.fallback_used);
        assert_eq!(out.decision.kernel, "cpu::add_scalar");
    }

    #[test]
    fn strict_mode_subtracts_with_cpu_kernel() {
        let lhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(3.0, DType::F64, Device::Cpu);
        let keyset = DispatchKeySet::from_keys(&[DispatchKey::BackendSelect, DispatchKey::CPU]);

        let out = dispatch_scalar_binary_with_keyset(
            BinaryOp::Sub,
            ExecutionMode::Strict,
            &lhs,
            &rhs,
            keyset,
        )
        .expect("strict mode should resolve subtraction directly to cpu");
        assert_eq!(out.tensor.value(), -1.0);
        assert_eq!(out.decision.selected_key, DispatchKey::CPU);
        assert_eq!(out.decision.backend_key, DispatchKey::CPU);
        assert!(!out.decision.fallback_used);
        assert_eq!(out.decision.kernel, "cpu::sub_scalar");
    }

    #[test]
    fn strict_mode_divides_with_cpu_kernel() {
        let lhs = ScalarTensor::new(7.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let keyset = DispatchKeySet::from_keys(&[DispatchKey::BackendSelect, DispatchKey::CPU]);

        let out = dispatch_scalar_binary_with_keyset(
            BinaryOp::Div,
            ExecutionMode::Strict,
            &lhs,
            &rhs,
            keyset,
        )
        .expect("strict mode should resolve division directly to cpu");
        assert_eq!(out.tensor.value(), 3.5);
        assert_eq!(out.decision.selected_key, DispatchKey::CPU);
        assert_eq!(out.decision.backend_key, DispatchKey::CPU);
        assert!(!out.decision.fallback_used);
        assert_eq!(out.decision.kernel, "cpu::div_scalar");
    }

    #[test]
    fn hardened_mode_allows_composite_fallback() {
        let lhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(3.0, DType::F64, Device::Cpu);
        let keyset = DispatchKeySet::from_keys(&[
            DispatchKey::CompositeExplicitAutograd,
            DispatchKey::CPU,
            DispatchKey::BackendSelect,
        ]);

        let out = dispatch_scalar_binary_with_keyset(
            BinaryOp::Add,
            ExecutionMode::Hardened,
            &lhs,
            &rhs,
            keyset,
        )
        .expect("hardened mode should fallback");
        assert_eq!(out.tensor.value(), 5.0);
        assert!(out.decision.fallback_used);
        assert_eq!(
            out.decision.selected_key,
            DispatchKey::CompositeExplicitAutograd
        );
        assert_eq!(out.decision.backend_key, DispatchKey::CPU);
    }

    #[test]
    fn hardened_mode_prefers_cpu_over_backendselect() {
        let lhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(4.0, DType::F64, Device::Cpu);
        let keyset = DispatchKeySet::from_keys(&[DispatchKey::BackendSelect, DispatchKey::CPU]);

        let out = dispatch_scalar_binary_with_keyset(
            BinaryOp::Mul,
            ExecutionMode::Hardened,
            &lhs,
            &rhs,
            keyset,
        )
        .expect("hardened mode should resolve directly to cpu when cpu type key exists");

        assert_eq!(out.tensor.value(), 8.0);
        assert!(!out.decision.fallback_used);
        assert_eq!(out.decision.selected_key, DispatchKey::CPU);
        assert_eq!(out.decision.backend_key, DispatchKey::CPU);
        assert_eq!(out.decision.kernel, "cpu::mul_scalar");
    }

    #[test]
    fn dispatch_returns_kernel_metadata() {
        let lhs = ScalarTensor::new(1.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let outcome =
            dispatch_scalar_binary(BinaryOp::Add, ExecutionMode::Strict, &lhs, &rhs, true)
                .expect("dispatch should succeed");

        assert_eq!(outcome.tensor.value(), 3.0);
        assert_eq!(outcome.decision.kernel, "autograd_cpu::add_scalar");
        assert_eq!(outcome.decision.mode, ExecutionMode::Strict);
        assert_eq!(outcome.decision.selected_key, DispatchKey::AutogradCPU);
        assert_eq!(outcome.decision.backend_key, DispatchKey::CPU);
        assert!(!outcome.decision.fallback_used);
    }

    #[test]
    fn dispatch_keyset_for_tensor_meta_tracks_requires_grad() {
        let lhs_meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let rhs_meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);

        let no_grad = dispatch_keyset_for_tensor_meta(&lhs_meta, &rhs_meta, false);
        assert!(no_grad.has(DispatchKey::CPU));
        assert!(no_grad.has(DispatchKey::BackendSelect));
        assert!(!no_grad.has(DispatchKey::AutogradCPU));

        let with_grad = dispatch_keyset_for_tensor_meta(&lhs_meta, &rhs_meta, true);
        assert!(with_grad.has(DispatchKey::CPU));
        assert!(with_grad.has(DispatchKey::BackendSelect));
        assert!(with_grad.has(DispatchKey::AutogradCPU));
    }

    #[test]
    fn tensor_dispatch_strict_mode_uses_cpu_add_kernel() {
        let lhs_meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let rhs_meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let lhs = vec![1.0, 2.0, 3.0, 4.0];
        let rhs = vec![0.5, 1.5, 2.5, 3.5];
        let keyset = DispatchKeySet::from_keys(&[DispatchKey::BackendSelect, DispatchKey::CPU]);

        let out = dispatch_tensor_binary_contiguous_f64_with_keyset(
            BinaryOp::Add,
            ExecutionMode::Strict,
            &lhs,
            &rhs,
            &lhs_meta,
            &rhs_meta,
            keyset,
        )
        .expect("strict tensor dispatch should resolve to cpu add");

        assert_eq!(out.values, vec![1.5, 3.5, 5.5, 7.5]);
        assert_eq!(out.decision.kernel, "cpu::add_tensor_contiguous_f64");
        assert_eq!(out.decision.selected_key, DispatchKey::CPU);
        assert_eq!(out.decision.backend_key, DispatchKey::CPU);
        assert!(!out.decision.fallback_used);
    }

    #[test]
    fn tensor_dispatch_hardened_mode_allows_composite_fallback() {
        let lhs_meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let rhs_meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let lhs = vec![2.0, 4.0];
        let rhs = vec![3.0, 5.0];
        let keyset = DispatchKeySet::from_keys(&[
            DispatchKey::CompositeExplicitAutograd,
            DispatchKey::CPU,
            DispatchKey::BackendSelect,
        ]);

        let out = dispatch_tensor_binary_contiguous_f64_with_keyset(
            BinaryOp::Mul,
            ExecutionMode::Hardened,
            &lhs,
            &rhs,
            &lhs_meta,
            &rhs_meta,
            keyset,
        )
        .expect("hardened tensor dispatch should fallback to cpu");

        assert_eq!(out.values, vec![6.0, 20.0]);
        assert_eq!(
            out.decision.selected_key,
            DispatchKey::CompositeExplicitAutograd
        );
        assert_eq!(out.decision.backend_key, DispatchKey::CPU);
        assert!(out.decision.fallback_used);
    }

    #[test]
    fn tensor_dispatch_strict_mode_routes_matmul_to_cpu_kernel() {
        let lhs_meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let rhs_meta = TensorMeta::from_shape(vec![3, 2], DType::F64, Device::Cpu);
        let lhs = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let rhs = vec![7.0, 8.0, 9.0, 10.0, 11.0, 12.0];
        let keyset = DispatchKeySet::from_keys(&[DispatchKey::BackendSelect, DispatchKey::CPU]);

        let out = dispatch_tensor_binary_contiguous_f64_with_keyset(
            BinaryOp::MatMul,
            ExecutionMode::Strict,
            &lhs,
            &rhs,
            &lhs_meta,
            &rhs_meta,
            keyset,
        )
        .expect("strict tensor dispatch should route matmul to cpu kernel");

        assert_eq!(out.values, vec![58.0, 64.0, 139.0, 154.0]);
        assert_eq!(out.decision.kernel, "cpu::matmul_tensor_contiguous_f64");
        assert_eq!(out.decision.selected_key, DispatchKey::CPU);
        assert_eq!(out.decision.backend_key, DispatchKey::CPU);
        assert!(!out.decision.fallback_used);
    }

    #[test]
    fn tensor_dispatch_strict_mode_rejects_composite_fallback() {
        let lhs_meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let rhs_meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let lhs = vec![2.0, 4.0];
        let rhs = vec![3.0, 5.0];
        let keyset = DispatchKeySet::from_keys(&[
            DispatchKey::CompositeExplicitAutograd,
            DispatchKey::CPU,
            DispatchKey::BackendSelect,
        ]);

        let err = dispatch_tensor_binary_contiguous_f64_with_keyset(
            BinaryOp::Mul,
            ExecutionMode::Strict,
            &lhs,
            &rhs,
            &lhs_meta,
            &rhs_meta,
            keyset,
        )
        .expect_err("strict tensor dispatch must fail closed");
        assert!(err.to_string().contains("strict mode forbids"));
    }

    #[test]
    fn tensor_dispatch_propagates_kernel_dtype_mismatch() {
        let lhs_meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let rhs_meta = TensorMeta::from_shape(vec![2], DType::F32, Device::Cpu);
        let lhs = vec![1.0, 2.0];
        let rhs = vec![3.0, 4.0];

        let err = dispatch_tensor_binary_contiguous_f64(
            BinaryOp::Add,
            ExecutionMode::Strict,
            &lhs,
            &rhs,
            &lhs_meta,
            &rhs_meta,
            false,
        )
        .expect_err("dtype mismatch should bubble as kernel error");

        assert!(matches!(
            err,
            DispatchError::Kernel(KernelError::Incompatible(
                TensorCompatError::DTypeMismatch { .. }
            ))
        ));
    }

    #[test]
    fn tensor_dispatch_propagates_non_contiguous_layout_rejection() {
        let lhs_meta =
            TensorMeta::from_shape_and_strides(vec![2, 2], vec![1, 2], 0, DType::F64, Device::Cpu)
                .expect("test meta should be valid");
        let rhs_meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let lhs = vec![1.0, 2.0, 3.0, 4.0];
        let rhs = vec![5.0, 6.0, 7.0, 8.0];

        let err = dispatch_tensor_binary_contiguous_f64(
            BinaryOp::Add,
            ExecutionMode::Strict,
            &lhs,
            &rhs,
            &lhs_meta,
            &rhs_meta,
            false,
        )
        .expect_err("non-contiguous layout must fail closed");

        assert!(matches!(
            err,
            DispatchError::Kernel(KernelError::UnsupportedLayout { side: "lhs" })
        ));
    }

    #[test]
    fn schema_row_parse_round_trips_add_tensor_signature() -> Result<(), String> {
        let schema_text = "add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor";
        let parsed = parse_schema_or_name(schema_text)
            .map_err(|error| format!("schema should parse: {error}"))?;
        let schema = match parsed {
            ParsedSchemaInput::Schema(schema) => schema,
            other => return Err(format!("expected full schema parse, got {other:?}")),
        };
        assert_eq!(schema.op.base, "add");
        assert_eq!(schema.op.overload.as_deref(), Some("Tensor"));
        assert!(!schema.op.is_inplace);
        assert!(!schema.is_out_variant);
        assert!(schema.arguments.contains("Tensor self"));
        assert_eq!(schema.returns, "Tensor");

        let seed = det_seed(&[schema.schema_digest, 0x0034, 0x1405]);
        let log = build_schema_property_log(SchemaLogParams {
            test_id: "schema_row_parse_round_trips_add_tensor_signature",
            mode: "strict",
            seed,
            op_name: &schema.op.base,
            overload_name: schema.op.overload.as_deref().unwrap_or(""),
            schema_digest: schema.schema_digest,
            dispatch_keyset_bits: 0,
            reason_code: "op_schema_row_roundtrip_ok",
        });
        assert_schema_log_contract(&log);
        Ok(())
    }

    #[test]
    fn operator_name_parse_preserves_overload_token() {
        let parsed = parse_schema_name("add.Tensor").expect("name should parse");
        assert_eq!(parsed.base, "add");
        assert_eq!(parsed.overload.as_deref(), Some("Tensor"));
        assert_eq!(parsed.unambiguous_name(), "add_Tensor");
        assert!(!parsed.is_inplace);
    }

    #[test]
    fn base_operator_name_parse_inplace_suffix_contract() {
        let parsed = parse_schema_name("add_").expect("inplace name should parse");
        assert_eq!(parsed.base, "add");
        assert!(parsed.is_inplace);
        assert!(parsed.overload.is_none());
    }

    #[test]
    fn schema_out_variant_requires_mutable_out_alias() -> Result<(), String> {
        let schema_text =
            "add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)";
        let parsed = parse_schema_or_name(schema_text)
            .map_err(|error| format!("schema should parse: {error}"))?;
        let schema = match parsed {
            ParsedSchemaInput::Schema(schema) => schema,
            other => return Err(format!("expected full schema parse, got {other:?}")),
        };
        assert_eq!(schema.op.overload.as_deref(), Some("out"));
        assert!(schema.is_out_variant);
        assert!(schema.arguments.contains("Tensor(a!) out"));
        assert_eq!(schema.returns, "Tensor(a!)");
        Ok(())
    }

    #[test]
    fn parse_schema_or_name_classifies_name_only_vs_full_schema() {
        let name_only = parse_schema_or_name("add.Tensor").expect("name-only form should parse");
        assert!(matches!(name_only, ParsedSchemaInput::Name(_)));

        let full_schema = parse_schema_or_name(
            "add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",
        )
        .expect("full schema should parse");
        assert!(matches!(full_schema, ParsedSchemaInput::Schema(_)));
    }

    #[test]
    fn schema_parser_rejects_malformed_tokens() {
        let malformed = "add.Tensor(Tensor self, Tensor other -> Tensor";
        let err = parse_schema_or_name(malformed).expect_err("malformed schema must fail");
        assert!(matches!(err, OpSchemaError::MalformedSchema { .. }));
    }

    #[test]
    fn schema_parser_rejects_illegal_overload_name() {
        let err = parse_schema_name("add.default").expect_err("default overload must fail");
        assert!(matches!(err, OpSchemaError::InvalidOverloadName { .. }));

        let err = parse_schema_name("add.__magic").expect_err("dunder overload must fail");
        assert!(matches!(err, OpSchemaError::InvalidOverloadName { .. }));
    }

    #[test]
    fn schema_dispatch_keyset_rejects_unknown_backend_key() {
        let err = schema_dispatch_keyset_from_tags(&["CPU", "NotARealKey"])
            .expect_err("unknown dispatch key tags must fail");
        assert!(matches!(err, OpSchemaError::UnknownDispatchKey { .. }));
    }

    #[test]
    fn schema_dispatch_keyset_requires_cpu_backend_for_scoped_ops() {
        let err = schema_dispatch_keyset_from_tags(&["AutogradCPU"])
            .expect_err("autograd key without cpu backend should fail");
        assert!(matches!(
            err,
            OpSchemaError::IncompatibleDispatchKeyset(DispatchKeyError::IncompatibleSet { .. })
        ));
    }

    #[test]
    fn schema_registry_registers_and_dispatches_add_tensor_schema() {
        let parsed = parse_schema_or_name(
            "add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",
        )
        .expect("schema should parse");
        let keyset =
            schema_dispatch_keyset_from_tags(&["CPU", "AutogradCPU"]).expect("keyset should parse");

        let mut registry = SchemaRegistry::new();
        let normalized = registry
            .register(&parsed, keyset)
            .expect("registration should succeed");
        assert_eq!(normalized, "add_Tensor");
        assert_eq!(registry.len(), 1);
        let entry = registry
            .lookup(normalized.as_str())
            .expect("entry should exist");
        assert_eq!(entry.op, BinaryOp::Add);
        assert_eq!(entry.keyset.bits(), keyset.bits());

        let lhs = ScalarTensor::new(1.5, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let out = dispatch_scalar_binary_registered(
            &registry,
            normalized.as_str(),
            ExecutionMode::Strict,
            &lhs,
            &rhs,
        )
        .expect("registered schema dispatch should succeed");
        assert_eq!(out.tensor.value(), 3.5);
        assert_eq!(out.decision.kernel, "autograd_cpu::add_scalar");
        assert_eq!(out.decision.selected_key, DispatchKey::AutogradCPU);
    }

    #[test]
    fn schema_registry_rejects_duplicate_registration() {
        let parsed = parse_schema_or_name(
            "add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",
        )
        .expect("schema should parse");
        let keyset =
            schema_dispatch_keyset_from_tags(&["CPU", "AutogradCPU"]).expect("keyset should parse");

        let mut registry = SchemaRegistry::new();
        registry
            .register(&parsed, keyset)
            .expect("first registration should succeed");
        let err = registry
            .register(&parsed, keyset)
            .expect_err("duplicate registration must fail");
        assert!(matches!(err, SchemaRegistryError::DuplicateSchema { .. }));
    }

    #[test]
    fn schema_registry_rejects_unsupported_operator_base() {
        let parsed = parse_schema_or_name("pow.Tensor(Tensor self, Tensor exponent) -> Tensor")
            .expect("schema should parse");
        let keyset =
            schema_dispatch_keyset_from_tags(&["CPU", "AutogradCPU"]).expect("keyset should parse");

        let mut registry = SchemaRegistry::new();
        let err = registry
            .register(&parsed, keyset)
            .expect_err("unsupported operators should fail closed");
        assert!(matches!(
            err,
            SchemaRegistryError::UnsupportedOperator { .. }
        ));
    }

    #[test]
    fn schema_registry_accepts_matmul_operator_base() {
        let parsed = parse_schema_or_name("matmul.Tensor(Tensor self, Tensor other) -> Tensor")
            .expect("schema should parse");
        let keyset =
            schema_dispatch_keyset_from_tags(&["CPU", "AutogradCPU"]).expect("keyset should parse");

        let mut registry = SchemaRegistry::new();
        let normalized = registry
            .register(&parsed, keyset)
            .expect("matmul registration should succeed");
        let entry = registry
            .lookup(normalized.as_str())
            .expect("matmul entry should exist");
        assert_eq!(entry.op, BinaryOp::MatMul);
        assert_eq!(entry.keyset.bits(), keyset.bits());
    }

    #[test]
    fn schema_dispatch_rejects_missing_registry_entry() {
        let registry = SchemaRegistry::new();
        let lhs = ScalarTensor::new(1.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let err = dispatch_scalar_binary_registered(
            &registry,
            "add_Tensor",
            ExecutionMode::Strict,
            &lhs,
            &rhs,
        )
        .expect_err("unknown schema must fail");
        assert!(matches!(
            err,
            SchemaDispatchError::Registry(SchemaRegistryError::MissingSchema { .. })
        ));
    }

    #[test]
    fn property_log_contract_maps_to_dispatch_scenarios() {
        let seed = det_seed(&[0x13, 0x5, 0x2]);
        let log = build_property_log(
            "prop_contract_mapping",
            "strict",
            seed,
            0xdead_beef,
            0xbead_1337,
            "dispatch_property_contract_mapping_ok",
        );
        assert_log_contract(&log);

        let scenario_id = log
            .get("scenario_id")
            .expect("scenario_id must be present in property logs");
        assert!(scenario_id.starts_with("dispatch_key/strict:"));
        assert!(
            scenario_id.contains("prop_contract_mapping"),
            "scenario id should preserve test identity"
        );

        let replay = log
            .get("replay_command")
            .expect("replay command must be present in property logs");
        assert!(replay.contains("cargo test -p ft-dispatch"));
        assert!(replay.contains("prop_contract_mapping"));
    }

    proptest! {
        #[test]
        fn prop_schema_name_roundtrip(
            base in "[a-z][a-z0-9]{0,8}",
            overload in "[A-Z][A-Za-z0-9_]{0,8}",
            has_overload in any::<bool>(),
            inplace in any::<bool>(),
        ) {
            let mut full_name = base.clone();
            if inplace {
                full_name.push('_');
            }
            if has_overload {
                full_name.push('.');
                full_name.push_str(&overload);
            }

            let parsed = parse_schema_name(&full_name)
                .expect("generated schema names should parse");
            prop_assert_eq!(parsed.base.as_str(), base.as_str());
            prop_assert_eq!(parsed.is_inplace, inplace);
            if has_overload {
                prop_assert_eq!(parsed.overload.as_deref(), Some(overload.as_str()));
                prop_assert_eq!(
                    parsed.unambiguous_name(),
                    format!("{}_{}", base, overload),
                );
            } else {
                prop_assert!(parsed.overload.is_none());
                prop_assert_eq!(parsed.unambiguous_name(), base.clone());
            }

            let schema_digest = det_seed(&[
                parsed.base.len() as u64,
                parsed.overload.as_ref().map_or(0, |value| value.len() as u64),
                inplace as u64,
            ]);
            let seed = det_seed(&[schema_digest, has_overload as u64]);
            let log = build_schema_property_log(SchemaLogParams {
                test_id: "prop_schema_name_roundtrip",
                mode: "strict",
                seed,
                op_name: &parsed.base,
                overload_name: parsed.overload.as_deref().unwrap_or(""),
                schema_digest,
                dispatch_keyset_bits: 0,
                reason_code: "op_schema_name_roundtrip_ok",
            });
            assert_schema_log_contract(&log);
        }

        #[test]
        fn prop_known_bits_roundtrip(
            backend_select in any::<bool>(),
            composite_implicit in any::<bool>(),
            composite_explicit in any::<bool>(),
            cpu in any::<bool>(),
            autograd_cpu in any::<bool>(),
        ) {
            let mut keyset = DispatchKeySet::empty();
            if backend_select {
                keyset.add(DispatchKey::BackendSelect);
            }
            if composite_implicit {
                keyset.add(DispatchKey::CompositeImplicitAutograd);
            }
            if composite_explicit {
                keyset.add(DispatchKey::CompositeExplicitAutograd);
            }
            if cpu {
                keyset.add(DispatchKey::CPU);
            }
            if autograd_cpu {
                keyset.add(DispatchKey::AutogradCPU);
            }

            let bits = keyset.bits();
            let reparsed = DispatchKeySet::from_bits_checked(bits)
                .expect("known bit combinations must parse");
            prop_assert_eq!(reparsed.bits(), bits);

            let seed = det_seed(&[bits, backend_select as u64, cpu as u64, autograd_cpu as u64]);
            let log = build_property_log(
                "prop_known_bits_roundtrip",
                "strict",
                seed,
                bits,
                reparsed.bits(),
                "dispatch_known_bits_roundtrip_ok",
            );
            assert_log_contract(&log);
        }

        #[test]
        fn prop_unknown_bits_mask_fail_closed(bit in prop_oneof![Just(0u8), 6u8..=63u8]) {
            let mask = 1u64 << u32::from(bit);
            let err = DispatchKeySet::from_bits_checked(mask)
                .expect_err("unknown bit masks must fail closed");
            match err {
                DispatchKeyError::UnknownBits { unknown_mask } => {
                    prop_assert_eq!(unknown_mask, mask);
                }
                other => prop_assert!(false, "expected UnknownBits, got {other:?}"),
            }

            let seed = det_seed(&[mask, bit as u64]);
            let log = build_property_log(
                "prop_unknown_bits_mask_fail_closed",
                "strict",
                seed,
                mask,
                mask,
                "dispatch_unknown_bits_fail_closed",
            );
            assert_log_contract(&log);
        }

        #[test]
        fn prop_priority_matches_explicit_table(
            backend_select in any::<bool>(),
            composite_implicit in any::<bool>(),
            composite_explicit in any::<bool>(),
            cpu in any::<bool>(),
            autograd_cpu in any::<bool>(),
        ) {
            let mut keyset = DispatchKeySet::empty();
            if backend_select {
                keyset.add(DispatchKey::BackendSelect);
            }
            if composite_implicit {
                keyset.add(DispatchKey::CompositeImplicitAutograd);
            }
            if composite_explicit {
                keyset.add(DispatchKey::CompositeExplicitAutograd);
            }
            if cpu {
                keyset.add(DispatchKey::CPU);
            }
            if autograd_cpu {
                keyset.add(DispatchKey::AutogradCPU);
            }

            let result = keyset.highest_priority_type_id();
            if keyset.is_empty() {
                prop_assert!(matches!(result, Err(DispatchKeyError::EmptySet)));
            } else {
                let expected = TYPE_PRIORITY
                    .iter()
                    .copied()
                    .find(|key| keyset.has(*key))
                    .expect("non-empty keyset should have a type key");
                prop_assert_eq!(result.expect("type key should resolve"), expected);
            }

            let bits = keyset.bits();
            let output = result.map_or(0u64, |key| key as u8 as u64);
            let seed = det_seed(&[bits, output]);
            let log = build_property_log(
                "prop_priority_matches_explicit_table",
                "strict",
                seed,
                bits,
                output,
                "dispatch_type_priority_contract_ok",
            );
            assert_log_contract(&log);
        }

        #[test]
        fn prop_backend_resolution_requires_cpu(
            backend_select in any::<bool>(),
            composite_implicit in any::<bool>(),
            composite_explicit in any::<bool>(),
            cpu in any::<bool>(),
            autograd_cpu in any::<bool>(),
        ) {
            let mut keyset = DispatchKeySet::empty();
            if backend_select {
                keyset.add(DispatchKey::BackendSelect);
            }
            if composite_implicit {
                keyset.add(DispatchKey::CompositeImplicitAutograd);
            }
            if composite_explicit {
                keyset.add(DispatchKey::CompositeExplicitAutograd);
            }
            if cpu {
                keyset.add(DispatchKey::CPU);
            }
            if autograd_cpu {
                keyset.add(DispatchKey::AutogradCPU);
            }

            let result = keyset.highest_priority_backend_type_id();
            if keyset.is_empty() {
                prop_assert!(matches!(result, Err(DispatchKeyError::EmptySet)));
            } else if cpu {
                prop_assert_eq!(result.expect("cpu backend should resolve"), DispatchKey::CPU);
            } else {
                prop_assert!(matches!(result, Err(DispatchKeyError::NoBackendKey)));
            }

            let bits = keyset.bits();
            let output = result.map_or(0u64, |key| key as u8 as u64);
            let seed = det_seed(&[bits, output, cpu as u64]);
            let log = build_property_log(
                "prop_backend_resolution_requires_cpu",
                "strict",
                seed,
                bits,
                output,
                "dispatch_backend_resolution_contract_ok",
            );
            assert_log_contract(&log);
        }

        #[test]
        fn prop_validate_requires_cpu_for_autograd(
            backend_select in any::<bool>(),
            composite_implicit in any::<bool>(),
            composite_explicit in any::<bool>(),
            cpu in any::<bool>(),
            autograd_cpu in any::<bool>(),
        ) {
            let mut keyset = DispatchKeySet::empty();
            if backend_select {
                keyset.add(DispatchKey::BackendSelect);
            }
            if composite_implicit {
                keyset.add(DispatchKey::CompositeImplicitAutograd);
            }
            if composite_explicit {
                keyset.add(DispatchKey::CompositeExplicitAutograd);
            }
            if cpu {
                keyset.add(DispatchKey::CPU);
            }
            if autograd_cpu {
                keyset.add(DispatchKey::AutogradCPU);
            }

            let validation = keyset.validate_for_scalar_binary();
            if keyset.is_empty() {
                prop_assert!(matches!(validation, Err(DispatchKeyError::EmptySet)));
            } else if autograd_cpu && !cpu {
                match validation {
                    Err(DispatchKeyError::IncompatibleSet { .. }) => {}
                    other => prop_assert!(false, "expected IncompatibleSet, got {other:?}"),
                }
            } else if !cpu {
                prop_assert!(matches!(validation, Err(DispatchKeyError::NoBackendKey)));
            } else {
                prop_assert!(validation.is_ok());
            }

            let bits = keyset.bits();
            let outcome = if validation.is_ok() { 1u64 } else { 0u64 };
            let seed = det_seed(&[bits, outcome, autograd_cpu as u64]);
            let log = build_property_log(
                "prop_validate_requires_cpu_for_autograd",
                "strict",
                seed,
                bits,
                outcome,
                "dispatch_validate_contract_ok",
            );
            assert_log_contract(&log);
        }

        #[test]
        fn prop_mode_split_for_composite_keysets(
            lhs_value in -1_000.0f64..1_000.0f64,
            rhs_value in -1_000.0f64..1_000.0f64,
            use_explicit in any::<bool>(),
        ) {
            let lhs = ScalarTensor::new(lhs_value, DType::F64, Device::Cpu);
            let rhs = ScalarTensor::new(rhs_value, DType::F64, Device::Cpu);
            let selected_key = if use_explicit {
                DispatchKey::CompositeExplicitAutograd
            } else {
                DispatchKey::CompositeImplicitAutograd
            };
            let keyset =
                DispatchKeySet::from_keys(&[selected_key, DispatchKey::CPU, DispatchKey::BackendSelect]);

            let strict_err = dispatch_scalar_binary_with_keyset(
                BinaryOp::Add,
                ExecutionMode::Strict,
                &lhs,
                &rhs,
                keyset,
            )
            .expect_err("strict mode must reject composite/backend fallback");
            prop_assert!(strict_err.to_string().contains("strict mode forbids"));

            let hardened_out = dispatch_scalar_binary_with_keyset(
                BinaryOp::Add,
                ExecutionMode::Hardened,
                &lhs,
                &rhs,
                keyset,
            )
            .expect("hardened mode should allow bounded fallback");

            prop_assert!(hardened_out.decision.fallback_used);
            prop_assert_eq!(hardened_out.decision.selected_key, selected_key);
            prop_assert_eq!(hardened_out.decision.backend_key, DispatchKey::CPU);
            prop_assert!((hardened_out.tensor.value() - (lhs_value + rhs_value)).abs() <= 1e-12);

            let seed = det_seed(&[
                lhs_value.to_bits(),
                rhs_value.to_bits(),
                use_explicit as u64,
            ]);
            let log = build_property_log(
                "prop_mode_split_for_composite_keysets",
                "hardened",
                seed,
                lhs.evidence_fingerprint64() ^ rhs.evidence_fingerprint64(),
                hardened_out.decision.keyset_bits,
                "dispatch_mode_split_contract_ok",
            );
            assert_log_contract(&log);
        }

        #[test]
        fn prop_packet_007_autograd_without_cpu_stays_fail_closed(
            backend_select in any::<bool>(),
            composite_implicit in any::<bool>(),
            composite_explicit in any::<bool>(),
        ) {
            let mut keyset = DispatchKeySet::empty();
            keyset.add(DispatchKey::AutogradCPU);
            if backend_select {
                keyset.add(DispatchKey::BackendSelect);
            }
            if composite_implicit {
                keyset.add(DispatchKey::CompositeImplicitAutograd);
            }
            if composite_explicit {
                keyset.add(DispatchKey::CompositeExplicitAutograd);
            }

            let validation = keyset.validate_for_scalar_binary();
            match validation {
                Err(DispatchKeyError::IncompatibleSet { .. }) => {}
                other => prop_assert!(false, "expected IncompatibleSet, got {other:?}"),
            }

            let bits = keyset.bits();
            let seed = det_seed(&[
                bits,
                backend_select as u64,
                composite_implicit as u64,
                composite_explicit as u64,
            ]);
            let strict_log = build_packet_007_property_log(
                "prop_packet_007_autograd_without_cpu_stays_fail_closed",
                "strict",
                seed,
                bits,
                0,
                "dispatch007_autograd_without_cpu_fail_closed",
                "dispatch_key/strict:autograd_without_cpu_fail_closed",
            );
            assert_log_contract(&strict_log);

            let hardened_log = build_packet_007_property_log(
                "prop_packet_007_autograd_without_cpu_stays_fail_closed",
                "hardened",
                seed,
                bits,
                0,
                "dispatch007_autograd_without_cpu_fail_closed",
                "dispatch_key/hardened:autograd_without_cpu_fail_closed",
            );
            assert_log_contract(&hardened_log);
        }

        #[test]
        fn prop_packet_007_dtype_or_device_mismatch_stays_fail_closed(
            lhs_value in -1_000.0f64..1_000.0f64,
            rhs_value in -1_000.0f64..1_000.0f64,
            use_dtype_mismatch in any::<bool>(),
            use_mul in any::<bool>(),
        ) {
            let (lhs, rhs, reason_code, strict_scenario, hardened_scenario) = if use_dtype_mismatch {
                (
                    ScalarTensor::new(lhs_value, DType::F64, Device::Cpu),
                    ScalarTensor::new(rhs_value, DType::F32, Device::Cpu),
                    "dispatch007_dtype_mismatch_fail_closed",
                    "dispatch_key/strict:dtype_mismatch_fail_closed",
                    "dispatch_key/hardened:dtype_mismatch_fail_closed",
                )
            } else {
                (
                    ScalarTensor::new(lhs_value, DType::F64, Device::Cpu),
                    ScalarTensor::new(rhs_value, DType::F64, Device::Cuda),
                    "dispatch007_device_mismatch_fail_closed",
                    "dispatch_key/strict:device_mismatch_fail_closed",
                    "dispatch_key/hardened:device_mismatch_fail_closed",
                )
            };

            let op = if use_mul { BinaryOp::Mul } else { BinaryOp::Add };
            let keyset = DispatchKeySet::from_keys(&[DispatchKey::CPU, DispatchKey::BackendSelect]);

            let strict_err = dispatch_scalar_binary_with_keyset(
                op,
                ExecutionMode::Strict,
                &lhs,
                &rhs,
                keyset,
            )
            .expect_err("strict mode mismatch path must fail closed");
            prop_assert!(strict_err.to_string().contains("incompatible tensors"));

            let hardened_err = dispatch_scalar_binary_with_keyset(
                op,
                ExecutionMode::Hardened,
                &lhs,
                &rhs,
                keyset,
            )
            .expect_err("hardened mode mismatch path must fail closed");
            prop_assert!(hardened_err.to_string().contains("incompatible tensors"));

            let input_digest = lhs.evidence_fingerprint64() ^ rhs.evidence_fingerprint64();
            let output_digest = det_seed(&[input_digest, use_dtype_mismatch as u64, use_mul as u64]);
            let seed = det_seed(&[
                lhs_value.to_bits(),
                rhs_value.to_bits(),
                use_dtype_mismatch as u64,
                use_mul as u64,
            ]);

            let strict_log = build_packet_007_property_log(
                "prop_packet_007_dtype_or_device_mismatch_stays_fail_closed",
                "strict",
                seed,
                input_digest,
                output_digest,
                reason_code,
                strict_scenario,
            );
            assert_log_contract(&strict_log);

            let hardened_log = build_packet_007_property_log(
                "prop_packet_007_dtype_or_device_mismatch_stays_fail_closed",
                "hardened",
                seed,
                input_digest,
                output_digest,
                reason_code,
                hardened_scenario,
            );
            assert_log_contract(&hardened_log);
        }
    }

    #[test]
    fn dispatch_scalar_unary_neg_strict_returns_negated_value() {
        let input = ScalarTensor::new(3.5, DType::F64, Device::Cpu);
        let outcome = dispatch_scalar_unary(UnaryOp::Neg, ExecutionMode::Strict, &input, false)
            .expect("neg dispatch should succeed");
        assert_eq!(outcome.tensor.value(), -3.5);
        assert_eq!(outcome.decision.op, UnaryOp::Neg);
        assert!(!outcome.decision.fallback_used);
    }

    #[test]
    fn dispatch_scalar_unary_neg_with_grad_uses_autograd_cpu() {
        let input = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let outcome = dispatch_scalar_unary(UnaryOp::Neg, ExecutionMode::Strict, &input, true)
            .expect("neg dispatch with grad should succeed");
        assert_eq!(outcome.tensor.value(), -2.0);
        assert_eq!(outcome.decision.selected_key, DispatchKey::AutogradCPU);
    }

    #[test]
    fn dispatch_tensor_unary_neg_returns_negated_values() {
        let meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let input = vec![1.0, -2.0, 3.0, -4.0];

        let outcome = dispatch_tensor_unary_contiguous_f64(
            UnaryOp::Neg,
            ExecutionMode::Strict,
            &input,
            &meta,
            false,
        )
        .expect("tensor neg dispatch should succeed");
        assert_eq!(outcome.values, vec![-1.0, 2.0, -3.0, 4.0]);
        assert_eq!(outcome.decision.op, UnaryOp::Neg);
    }

    #[test]
    fn dispatch_tensor_unary_neg_with_grad_uses_autograd_cpu() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0];

        let outcome = dispatch_tensor_unary_contiguous_f64(
            UnaryOp::Neg,
            ExecutionMode::Strict,
            &input,
            &meta,
            true,
        )
        .expect("tensor neg dispatch with grad should succeed");
        assert_eq!(outcome.values, vec![-1.0, -2.0, -3.0]);
        assert_eq!(outcome.decision.selected_key, DispatchKey::AutogradCPU);
    }

    #[test]
    fn dispatch_scalar_comparison_eq_ne_respect_ieee_special_values() {
        let pos_inf = ScalarTensor::new(f64::INFINITY, DType::F64, Device::Cpu);
        let neg_inf = ScalarTensor::new(f64::NEG_INFINITY, DType::F64, Device::Cpu);
        let nan = ScalarTensor::new(f64::NAN, DType::F64, Device::Cpu);

        let eq_inf = dispatch_scalar_comparison(
            ComparisonOp::Eq,
            ExecutionMode::Strict,
            &pos_inf,
            &pos_inf,
            false,
        )
        .expect("eq(+inf,+inf) dispatch should succeed");
        assert_eq!(eq_inf.tensor.value(), 1.0);
        assert_eq!(eq_inf.decision.kernel, "cpu::eq_scalar");

        let ne_inf = dispatch_scalar_comparison(
            ComparisonOp::Ne,
            ExecutionMode::Strict,
            &neg_inf,
            &neg_inf,
            false,
        )
        .expect("ne(-inf,-inf) dispatch should succeed");
        assert_eq!(ne_inf.tensor.value(), 0.0);
        assert_eq!(ne_inf.decision.kernel, "cpu::ne_scalar");

        let eq_nan =
            dispatch_scalar_comparison(ComparisonOp::Eq, ExecutionMode::Strict, &nan, &nan, false)
                .expect("eq(nan,nan) dispatch should succeed");
        assert_eq!(eq_nan.tensor.value(), 0.0);

        let ne_nan =
            dispatch_scalar_comparison(ComparisonOp::Ne, ExecutionMode::Strict, &nan, &nan, false)
                .expect("ne(nan,nan) dispatch should succeed");
        assert_eq!(ne_nan.tensor.value(), 1.0);
    }

    #[test]
    fn dispatch_scalar_comparison_lt_gt_le_ge_respect_ieee_special_values() {
        let pos_inf = ScalarTensor::new(f64::INFINITY, DType::F64, Device::Cpu);
        let neg_inf = ScalarTensor::new(f64::NEG_INFINITY, DType::F64, Device::Cpu);
        let nan = ScalarTensor::new(f64::NAN, DType::F64, Device::Cpu);

        let lt_nan =
            dispatch_scalar_comparison(ComparisonOp::Lt, ExecutionMode::Strict, &nan, &nan, false)
                .expect("lt(nan,nan) dispatch should succeed");
        assert_eq!(lt_nan.tensor.value(), 0.0);

        let gt_nan =
            dispatch_scalar_comparison(ComparisonOp::Gt, ExecutionMode::Strict, &nan, &nan, false)
                .expect("gt(nan,nan) dispatch should succeed");
        assert_eq!(gt_nan.tensor.value(), 0.0);

        let le_nan =
            dispatch_scalar_comparison(ComparisonOp::Le, ExecutionMode::Strict, &nan, &nan, false)
                .expect("le(nan,nan) dispatch should succeed");
        assert_eq!(le_nan.tensor.value(), 0.0);

        let ge_nan =
            dispatch_scalar_comparison(ComparisonOp::Ge, ExecutionMode::Strict, &nan, &nan, false)
                .expect("ge(nan,nan) dispatch should succeed");
        assert_eq!(ge_nan.tensor.value(), 0.0);

        let lt_inf = dispatch_scalar_comparison(
            ComparisonOp::Lt,
            ExecutionMode::Strict,
            &neg_inf,
            &pos_inf,
            false,
        )
        .expect("lt(-inf,+inf) dispatch should succeed");
        assert_eq!(lt_inf.tensor.value(), 1.0);

        let gt_inf = dispatch_scalar_comparison(
            ComparisonOp::Gt,
            ExecutionMode::Strict,
            &pos_inf,
            &neg_inf,
            false,
        )
        .expect("gt(+inf,-inf) dispatch should succeed");
        assert_eq!(gt_inf.tensor.value(), 1.0);
    }

    #[test]
    fn dispatch_scalar_comparison_with_grad_uses_autograd_kernel_labels() {
        let lhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);

        let eq_out =
            dispatch_scalar_comparison(ComparisonOp::Eq, ExecutionMode::Strict, &lhs, &rhs, true)
                .expect("eq dispatch with grad should succeed");
        assert_eq!(eq_out.tensor.value(), 1.0);
        assert_eq!(eq_out.decision.selected_key, DispatchKey::AutogradCPU);
        assert_eq!(eq_out.decision.kernel, "autograd_cpu::eq_scalar");

        let ne_out =
            dispatch_scalar_comparison(ComparisonOp::Ne, ExecutionMode::Strict, &lhs, &rhs, true)
                .expect("ne dispatch with grad should succeed");
        assert_eq!(ne_out.tensor.value(), 0.0);
        assert_eq!(ne_out.decision.selected_key, DispatchKey::AutogradCPU);
        assert_eq!(ne_out.decision.kernel, "autograd_cpu::ne_scalar");
    }

    #[test]
    fn dispatch_tensor_comparison_eq_ne_respect_ieee_special_values() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let lhs = vec![f64::INFINITY, f64::NEG_INFINITY, f64::NAN, 1.0];
        let rhs = vec![f64::INFINITY, f64::NEG_INFINITY, f64::NAN, 2.0];

        let eq_outcome = dispatch_tensor_comparison_contiguous_f64(
            ComparisonOp::Eq,
            ExecutionMode::Strict,
            &lhs,
            &rhs,
            &meta,
            &meta,
            false,
        )
        .expect("tensor eq dispatch should succeed");
        assert_eq!(eq_outcome.values, vec![1.0, 1.0, 0.0, 0.0]);
        assert_eq!(eq_outcome.decision.kernel, "cpu::eq_tensor_contiguous_f64");

        let ne_outcome = dispatch_tensor_comparison_contiguous_f64(
            ComparisonOp::Ne,
            ExecutionMode::Strict,
            &lhs,
            &rhs,
            &meta,
            &meta,
            false,
        )
        .expect("tensor ne dispatch should succeed");
        assert_eq!(ne_outcome.values, vec![0.0, 0.0, 1.0, 1.0]);
        assert_eq!(ne_outcome.decision.kernel, "cpu::ne_tensor_contiguous_f64");
    }

    #[test]
    fn dispatch_tensor_comparison_lt_gt_le_ge_respect_ieee_special_values() {
        let meta = TensorMeta::from_shape(vec![5], DType::F64, Device::Cpu);
        let lhs = vec![f64::NAN, f64::INFINITY, f64::NEG_INFINITY, 1.0, 2.0];
        let rhs = vec![f64::NAN, f64::NEG_INFINITY, f64::INFINITY, 1.0, 3.0];

        let lt_outcome = dispatch_tensor_comparison_contiguous_f64(
            ComparisonOp::Lt,
            ExecutionMode::Strict,
            &lhs,
            &rhs,
            &meta,
            &meta,
            false,
        )
        .expect("tensor lt dispatch should succeed");
        let gt_outcome = dispatch_tensor_comparison_contiguous_f64(
            ComparisonOp::Gt,
            ExecutionMode::Strict,
            &lhs,
            &rhs,
            &meta,
            &meta,
            false,
        )
        .expect("tensor gt dispatch should succeed");
        let le_outcome = dispatch_tensor_comparison_contiguous_f64(
            ComparisonOp::Le,
            ExecutionMode::Strict,
            &lhs,
            &rhs,
            &meta,
            &meta,
            false,
        )
        .expect("tensor le dispatch should succeed");
        let ge_outcome = dispatch_tensor_comparison_contiguous_f64(
            ComparisonOp::Ge,
            ExecutionMode::Strict,
            &lhs,
            &rhs,
            &meta,
            &meta,
            false,
        )
        .expect("tensor ge dispatch should succeed");

        assert_eq!(lt_outcome.values, vec![0.0, 0.0, 1.0, 0.0, 1.0]);
        assert_eq!(gt_outcome.values, vec![0.0, 1.0, 0.0, 0.0, 0.0]);
        assert_eq!(le_outcome.values, vec![0.0, 0.0, 1.0, 1.0, 1.0]);
        assert_eq!(ge_outcome.values, vec![0.0, 1.0, 0.0, 1.0, 0.0]);
    }

    #[test]
    fn dispatch_tensor_comparison_with_grad_uses_autograd_kernel_labels() {
        let meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let lhs = vec![1.0, 3.0];
        let rhs = vec![1.0, 2.0];

        let eq_out = dispatch_tensor_comparison_contiguous_f64(
            ComparisonOp::Eq,
            ExecutionMode::Strict,
            &lhs,
            &rhs,
            &meta,
            &meta,
            true,
        )
        .expect("tensor eq dispatch with grad should succeed");
        assert_eq!(eq_out.values, vec![1.0, 0.0]);
        assert_eq!(eq_out.decision.selected_key, DispatchKey::AutogradCPU);
        assert_eq!(
            eq_out.decision.kernel,
            "autograd_cpu::eq_tensor_contiguous_f64"
        );

        let ne_out = dispatch_tensor_comparison_contiguous_f64(
            ComparisonOp::Ne,
            ExecutionMode::Strict,
            &lhs,
            &rhs,
            &meta,
            &meta,
            true,
        )
        .expect("tensor ne dispatch with grad should succeed");
        assert_eq!(ne_out.values, vec![0.0, 1.0]);
        assert_eq!(ne_out.decision.selected_key, DispatchKey::AutogradCPU);
        assert_eq!(
            ne_out.decision.kernel,
            "autograd_cpu::ne_tensor_contiguous_f64"
        );
    }

    //  bd-2rfh: dispatch_tensor_reduction_contiguous_f64 

    #[test]
    fn dispatch_reduction_sum_strict_no_grad() {
        let data = vec![1.0, 2.0, 3.0, 4.0];
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let out = super::dispatch_tensor_reduction_contiguous_f64(
            super::ReductionOp::Sum,
            ExecutionMode::Strict,
            &data,
            &meta,
            false,
        )
        .expect("sum reduction dispatch");
        assert!((out.value - 10.0).abs() < 1e-12);
        assert_eq!(out.decision.op, super::ReductionOp::Sum);
        assert_eq!(out.decision.selected_key, DispatchKey::CPU);
        assert_eq!(out.decision.kernel, "cpu::sum_tensor_contiguous_f64");
        assert!(!out.decision.fallback_used);
    }

    #[test]
    fn dispatch_reduction_mean_strict_with_grad() {
        let data = vec![2.0, 4.0, 6.0];
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let out = super::dispatch_tensor_reduction_contiguous_f64(
            super::ReductionOp::Mean,
            ExecutionMode::Strict,
            &data,
            &meta,
            true,
        )
        .expect("mean reduction dispatch with grad");
        assert!((out.value - 4.0).abs() < 1e-12);
        assert_eq!(out.decision.selected_key, DispatchKey::AutogradCPU);
        assert_eq!(
            out.decision.kernel,
            "autograd_cpu::mean_tensor_contiguous_f64"
        );
    }

    #[test]
    fn dispatch_reduction_prod_rejects_global_reduction() {
        let data = vec![1.0, 2.0];
        let meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let err = super::dispatch_tensor_reduction_contiguous_f64(
            super::ReductionOp::Prod,
            ExecutionMode::Strict,
            &data,
            &meta,
            false,
        )
        .expect_err("prod global reduction should fail");
        assert!(
            matches!(
                err,
                DispatchError::Key(DispatchKeyError::IncompatibleSet { .. })
            ),
            "expected IncompatibleSet, got {err:?}"
        );
    }

    //  bd-2rfh: dispatch_tensor_reduction_dim_contiguous_f64 

    #[test]
    fn dispatch_reduction_dim_sum() {
        let data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let out = super::dispatch_tensor_reduction_dim_contiguous_f64(
            super::ReductionOp::Sum,
            ExecutionMode::Strict,
            &data,
            &meta,
            1,
            false,
        )
        .expect("sum_dim dispatch");
        assert_eq!(out.values, vec![6.0, 15.0]);
        assert_eq!(out.decision.dim, 1);
        assert_eq!(out.decision.kernel, "cpu::sum_dim_tensor_contiguous_f64");
    }

    #[test]
    fn dispatch_reduction_dim_mean_with_grad() {
        let data = vec![2.0, 4.0, 6.0, 8.0];
        let meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let out = super::dispatch_tensor_reduction_dim_contiguous_f64(
            super::ReductionOp::Mean,
            ExecutionMode::Strict,
            &data,
            &meta,
            0,
            true,
        )
        .expect("mean_dim dispatch with grad");
        assert_eq!(out.values, vec![4.0, 6.0]);
        assert_eq!(
            out.decision.kernel,
            "autograd_cpu::mean_dim_tensor_contiguous_f64"
        );
    }

    #[test]
    fn dispatch_reduction_dim_prod() {
        let data = vec![1.0, 2.0, 3.0, 4.0];
        let meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let out = super::dispatch_tensor_reduction_dim_contiguous_f64(
            super::ReductionOp::Prod,
            ExecutionMode::Strict,
            &data,
            &meta,
            1,
            false,
        )
        .expect("prod_dim dispatch");
        assert_eq!(out.values, vec![2.0, 12.0]);
        assert_eq!(out.decision.kernel, "cpu::prod_dim_tensor_contiguous_f64");
    }

    #[test]
    fn dispatch_reduction_dim_var() {
        let data = vec![1.0, 3.0, 5.0, 7.0];
        let meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let out = super::dispatch_tensor_reduction_dim_contiguous_f64(
            super::ReductionOp::Var,
            ExecutionMode::Strict,
            &data,
            &meta,
            1,
            false,
        )
        .expect("var_dim dispatch");
        // var([1,3]) = 2.0, var([5,7]) = 2.0
        assert_eq!(out.values.len(), 2);
        assert_eq!(out.decision.kernel, "cpu::var_dim_tensor_contiguous_f64");
    }

    #[test]
    fn dispatch_reduction_dim_std() {
        let data = vec![1.0, 3.0, 5.0, 7.0];
        let meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let out = super::dispatch_tensor_reduction_dim_contiguous_f64(
            super::ReductionOp::Std,
            ExecutionMode::Strict,
            &data,
            &meta,
            1,
            false,
        )
        .expect("std_dim dispatch");
        assert_eq!(out.values.len(), 2);
        assert_eq!(out.decision.kernel, "cpu::std_dim_tensor_contiguous_f64");
    }

    //  bd-2rfh: dispatch_tensor_normalize_dim_contiguous_f64 

    #[test]
    fn dispatch_normalize_softmax() {
        let data = vec![1.0, 2.0, 3.0];
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let out = super::dispatch_tensor_normalize_dim_contiguous_f64(
            super::NormalizeOp::Softmax,
            ExecutionMode::Strict,
            &data,
            &meta,
            0,
            false,
        )
        .expect("softmax dispatch");
        let total: f64 = out.values.iter().sum();
        assert!((total - 1.0).abs() < 1e-10, "softmax should sum to 1");
        assert_eq!(
            out.decision.kernel,
            "cpu::softmax_dim_tensor_contiguous_f64"
        );
        assert_eq!(out.decision.op, super::NormalizeOp::Softmax);
        assert_eq!(out.decision.dim, 0);
    }

    #[test]
    fn dispatch_normalize_log_softmax_with_grad() {
        let data = vec![1.0, 2.0, 3.0];
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let out = super::dispatch_tensor_normalize_dim_contiguous_f64(
            super::NormalizeOp::LogSoftmax,
            ExecutionMode::Strict,
            &data,
            &meta,
            0,
            true,
        )
        .expect("log_softmax dispatch with grad");
        // log_softmax values should be negative
        assert!(out.values.iter().all(|&v| v < 0.0));
        assert_eq!(
            out.decision.kernel,
            "autograd_cpu::log_softmax_dim_tensor_contiguous_f64"
        );
        assert_eq!(out.decision.selected_key, DispatchKey::AutogradCPU);
    }

    //  bd-2rfh: dispatch_tensor_join_contiguous_f64 

    #[test]
    fn dispatch_join_cat() {
        let a = vec![1.0, 2.0];
        let b = vec![3.0, 4.0, 5.0];
        let meta_a = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let meta_b = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let inputs: Vec<(&[f64], &TensorMeta)> = vec![(&a, &meta_a), (&b, &meta_b)];
        let out = super::dispatch_tensor_join_contiguous_f64(
            super::JoinOp::Cat,
            ExecutionMode::Strict,
            &inputs,
            0,
            false,
        )
        .expect("cat dispatch");
        assert_eq!(out.values, vec![1.0, 2.0, 3.0, 4.0, 5.0]);
        assert_eq!(out.decision.kernel, "cpu::cat_tensor_contiguous_f64");
        assert_eq!(out.decision.op, super::JoinOp::Cat);
        assert_eq!(out.decision.num_inputs, 2);
    }

    #[test]
    fn dispatch_join_stack_with_grad() {
        let a = vec![1.0, 2.0];
        let b = vec![3.0, 4.0];
        let meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let inputs: Vec<(&[f64], &TensorMeta)> = vec![(&a, &meta), (&b, &meta)];
        let out = super::dispatch_tensor_join_contiguous_f64(
            super::JoinOp::Stack,
            ExecutionMode::Strict,
            &inputs,
            0,
            true,
        )
        .expect("stack dispatch with grad");
        assert_eq!(out.values, vec![1.0, 2.0, 3.0, 4.0]);
        assert_eq!(
            out.decision.kernel,
            "autograd_cpu::stack_tensor_contiguous_f64"
        );
        assert_eq!(out.decision.selected_key, DispatchKey::AutogradCPU);
    }

    #[test]
    fn dispatch_join_empty_inputs_rejects() {
        let inputs: Vec<(&[f64], &TensorMeta)> = vec![];
        let err = super::dispatch_tensor_join_contiguous_f64(
            super::JoinOp::Cat,
            ExecutionMode::Strict,
            &inputs,
            0,
            false,
        )
        .expect_err("empty inputs should fail");
        assert!(
            matches!(
                err,
                DispatchError::Key(DispatchKeyError::IncompatibleSet { .. })
            ),
            "expected IncompatibleSet, got {err:?}"
        );
    }

    #[test]
    fn dispatch_join_rejects_dtype_mismatch() {
        let a = vec![1.0, 2.0];
        let b = vec![3.0, 4.0];
        let meta_a = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let meta_b = TensorMeta::from_shape(vec![2], DType::F32, Device::Cpu);
        let inputs: Vec<(&[f64], &TensorMeta)> = vec![(&a, &meta_a), (&b, &meta_b)];

        let err = super::dispatch_tensor_join_contiguous_f64(
            super::JoinOp::Cat,
            ExecutionMode::Strict,
            &inputs,
            0,
            false,
        )
        .expect_err("dtype mismatch should fail closed");

        assert!(matches!(
            err,
            DispatchError::Kernel(KernelError::Incompatible(
                TensorCompatError::DTypeMismatch { .. }
            ))
        ));
    }

    #[test]
    fn dispatch_join_rejects_device_mismatch() {
        let a = vec![1.0, 2.0];
        let b = vec![3.0, 4.0];
        let meta_a = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let meta_b = TensorMeta::from_shape(vec![2], DType::F64, Device::Cuda);
        let inputs: Vec<(&[f64], &TensorMeta)> = vec![(&a, &meta_a), (&b, &meta_b)];

        let err = super::dispatch_tensor_join_contiguous_f64(
            super::JoinOp::Cat,
            ExecutionMode::Strict,
            &inputs,
            0,
            false,
        )
        .expect_err("device mismatch should fail closed");

        assert!(matches!(
            err,
            DispatchError::Kernel(KernelError::Incompatible(
                TensorCompatError::DeviceMismatch { .. }
            ))
        ));
    }

    //  bd-2rfh: dispatch_scalar_pow + dispatch_tensor_pow_contiguous_f64 

    #[test]
    fn dispatch_scalar_pow_no_grad() {
        let input = ScalarTensor::new(3.0, DType::F64, Device::Cpu);
        let out = super::dispatch_scalar_pow(ExecutionMode::Strict, &input, 2.0, false)
            .expect("scalar pow dispatch");
        assert!((out.tensor.value() - 9.0).abs() < 1e-12);
        assert_eq!(out.decision.kernel, "cpu::pow_scalar");
        assert!((out.decision.exponent - 2.0).abs() < 1e-12);
    }

    #[test]
    fn dispatch_scalar_pow_with_grad() {
        let input = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let out = super::dispatch_scalar_pow(ExecutionMode::Strict, &input, 3.0, true)
            .expect("scalar pow dispatch with grad");
        assert!((out.tensor.value() - 8.0).abs() < 1e-12);
        assert_eq!(out.decision.kernel, "autograd_cpu::pow_scalar");
        assert_eq!(out.decision.selected_key, DispatchKey::AutogradCPU);
    }

    #[test]
    fn dispatch_tensor_pow() {
        let data = vec![2.0, 3.0, 4.0];
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let out = super::dispatch_tensor_pow_contiguous_f64(
            ExecutionMode::Strict,
            &data,
            &meta,
            2.0,
            false,
        )
        .expect("tensor pow dispatch");
        assert_eq!(out.values, vec![4.0, 9.0, 16.0]);
        assert_eq!(out.decision.kernel, "cpu::pow_tensor_contiguous_f64");
    }

    #[test]
    fn dispatch_tensor_pow_with_grad() {
        let data = vec![1.0, 2.0];
        let meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let out = super::dispatch_tensor_pow_contiguous_f64(
            ExecutionMode::Strict,
            &data,
            &meta,
            0.5,
            true,
        )
        .expect("tensor pow dispatch with grad");
        assert!((out.values[0] - 1.0).abs() < 1e-12);
        assert!((out.values[1] - f64::sqrt(2.0)).abs() < 1e-12);
        assert_eq!(
            out.decision.kernel,
            "autograd_cpu::pow_tensor_contiguous_f64"
        );
    }

    //  bd-2rfh: dispatch_scalar_clamp + dispatch_tensor_clamp_contiguous_f64 

    #[test]
    fn dispatch_scalar_clamp_no_grad() {
        let input = ScalarTensor::new(5.0, DType::F64, Device::Cpu);
        let out = super::dispatch_scalar_clamp(ExecutionMode::Strict, &input, 0.0, 3.0, false)
            .expect("scalar clamp dispatch");
        assert!((out.tensor.value() - 3.0).abs() < 1e-12);
        assert_eq!(out.decision.kernel, "cpu::clamp_scalar");
        assert!((out.decision.min_val - 0.0).abs() < 1e-12);
        assert!((out.decision.max_val - 3.0).abs() < 1e-12);
    }

    #[test]
    fn dispatch_scalar_clamp_with_grad() {
        let input = ScalarTensor::new(-5.0, DType::F64, Device::Cpu);
        let out = super::dispatch_scalar_clamp(ExecutionMode::Strict, &input, -2.0, 10.0, true)
            .expect("scalar clamp dispatch with grad");
        assert!((out.tensor.value() - (-2.0)).abs() < 1e-12);
        assert_eq!(out.decision.kernel, "autograd_cpu::clamp_scalar");
        assert_eq!(out.decision.selected_key, DispatchKey::AutogradCPU);
    }

    #[test]
    fn dispatch_tensor_clamp() {
        let data = vec![-1.0, 0.5, 2.0, 5.0];
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let out = super::dispatch_tensor_clamp_contiguous_f64(
            ExecutionMode::Strict,
            &data,
            &meta,
            0.0,
            3.0,
            false,
        )
        .expect("tensor clamp dispatch");
        assert_eq!(out.values, vec![0.0, 0.5, 2.0, 3.0]);
        assert_eq!(out.decision.kernel, "cpu::clamp_tensor_contiguous_f64");
    }

    #[test]
    fn dispatch_tensor_clamp_with_grad() {
        let data = vec![1.0, 10.0];
        let meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let out = super::dispatch_tensor_clamp_contiguous_f64(
            ExecutionMode::Strict,
            &data,
            &meta,
            2.0,
            8.0,
            true,
        )
        .expect("tensor clamp dispatch with grad");
        assert_eq!(out.values, vec![2.0, 8.0]);
        assert_eq!(
            out.decision.kernel,
            "autograd_cpu::clamp_tensor_contiguous_f64"
        );
        assert_eq!(out.decision.selected_key, DispatchKey::AutogradCPU);
    }

    //  bd-2rfh: dispatch_scalar_unary with multiple ops (not just Neg) 

    #[test]
    fn dispatch_scalar_unary_abs() {
        let input = ScalarTensor::new(-7.0, DType::F64, Device::Cpu);
        let out = dispatch_scalar_unary(UnaryOp::Abs, ExecutionMode::Strict, &input, false)
            .expect("abs scalar dispatch");
        assert!((out.tensor.value() - 7.0).abs() < 1e-12);
        assert_eq!(out.decision.kernel, "cpu::abs_scalar");
    }

    #[test]
    fn dispatch_scalar_unary_exp_with_grad() {
        let input = ScalarTensor::new(1.0, DType::F64, Device::Cpu);
        let out = dispatch_scalar_unary(UnaryOp::Exp, ExecutionMode::Strict, &input, true)
            .expect("exp scalar dispatch with grad");
        assert!((out.tensor.value() - std::f64::consts::E).abs() < 1e-12);
        assert_eq!(out.decision.kernel, "autograd_cpu::exp_scalar");
    }

    #[test]
    fn dispatch_scalar_unary_log() {
        let input = ScalarTensor::new(std::f64::consts::E, DType::F64, Device::Cpu);
        let out = dispatch_scalar_unary(UnaryOp::Log, ExecutionMode::Strict, &input, false)
            .expect("log scalar dispatch");
        assert!((out.tensor.value() - 1.0).abs() < 1e-12);
        assert_eq!(out.decision.kernel, "cpu::log_scalar");
    }

    #[test]
    fn dispatch_scalar_unary_relu() {
        let input = ScalarTensor::new(-3.0, DType::F64, Device::Cpu);
        let out = dispatch_scalar_unary(UnaryOp::Relu, ExecutionMode::Strict, &input, false)
            .expect("relu scalar dispatch");
        assert!((out.tensor.value() - 0.0).abs() < 1e-12);
    }

    #[test]
    fn dispatch_scalar_unary_sigmoid() {
        let input = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = dispatch_scalar_unary(UnaryOp::Sigmoid, ExecutionMode::Strict, &input, false)
            .expect("sigmoid scalar dispatch");
        assert!((out.tensor.value() - 0.5).abs() < 1e-12);
    }

    #[test]
    fn dispatch_scalar_unary_tanh() {
        let input = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = dispatch_scalar_unary(UnaryOp::Tanh, ExecutionMode::Strict, &input, false)
            .expect("tanh scalar dispatch");
        assert!((out.tensor.value() - 0.0).abs() < 1e-12);
    }

    #[test]
    fn dispatch_scalar_unary_sqrt() {
        let input = ScalarTensor::new(16.0, DType::F64, Device::Cpu);
        let out = dispatch_scalar_unary(UnaryOp::Sqrt, ExecutionMode::Strict, &input, false)
            .expect("sqrt scalar dispatch");
        assert!((out.tensor.value() - 4.0).abs() < 1e-12);
    }

    #[test]
    fn dispatch_scalar_unary_sin() {
        let input = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = dispatch_scalar_unary(UnaryOp::Sin, ExecutionMode::Strict, &input, false)
            .expect("sin scalar dispatch");
        assert!((out.tensor.value() - 0.0).abs() < 1e-12);
    }

    #[test]
    fn dispatch_scalar_unary_cos() {
        let input = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = dispatch_scalar_unary(UnaryOp::Cos, ExecutionMode::Strict, &input, false)
            .expect("cos scalar dispatch");
        assert!((out.tensor.value() - 1.0).abs() < 1e-12);
    }

    #[test]
    fn dispatch_scalar_unary_floor() {
        let input = ScalarTensor::new(2.7, DType::F64, Device::Cpu);
        let out = dispatch_scalar_unary(UnaryOp::Floor, ExecutionMode::Strict, &input, false)
            .expect("floor scalar dispatch");
        assert!((out.tensor.value() - 2.0).abs() < 1e-12);
    }

    #[test]
    fn dispatch_scalar_unary_ceil() {
        let input = ScalarTensor::new(2.1, DType::F64, Device::Cpu);
        let out = dispatch_scalar_unary(UnaryOp::Ceil, ExecutionMode::Strict, &input, false)
            .expect("ceil scalar dispatch");
        assert!((out.tensor.value() - 3.0).abs() < 1e-12);
    }

    //  bd-2rfh: dispatch_tensor_unary with multiple ops 

    #[test]
    fn dispatch_tensor_unary_abs() {
        let data = vec![-1.0, 2.0, -3.0];
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let out = dispatch_tensor_unary_contiguous_f64(
            UnaryOp::Abs,
            ExecutionMode::Strict,
            &data,
            &meta,
            false,
        )
        .expect("abs tensor dispatch");
        assert_eq!(out.values, vec![1.0, 2.0, 3.0]);
        assert_eq!(out.decision.kernel, "cpu::abs_tensor_contiguous_f64");
    }

    #[test]
    fn dispatch_tensor_unary_relu_with_grad() {
        let data = vec![-1.0, 0.0, 1.0, 2.0];
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let out = dispatch_tensor_unary_contiguous_f64(
            UnaryOp::Relu,
            ExecutionMode::Strict,
            &data,
            &meta,
            true,
        )
        .expect("relu tensor dispatch with grad");
        assert_eq!(out.values, vec![0.0, 0.0, 1.0, 2.0]);
        assert_eq!(
            out.decision.kernel,
            "autograd_cpu::relu_tensor_contiguous_f64"
        );
    }

    #[test]
    fn dispatch_tensor_unary_exp() {
        let data = vec![0.0, 1.0];
        let meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let out = dispatch_tensor_unary_contiguous_f64(
            UnaryOp::Exp,
            ExecutionMode::Strict,
            &data,
            &meta,
            false,
        )
        .expect("exp tensor dispatch");
        assert!((out.values[0] - 1.0).abs() < 1e-12);
        assert!((out.values[1] - std::f64::consts::E).abs() < 1e-12);
    }

    //  bd-2rfh: from_schema_base coverage for non-BinaryOp enums 

    #[test]
    fn unary_op_from_schema_base_covers_all_variants() {
        let cases = [
            ("neg", UnaryOp::Neg),
            ("abs", UnaryOp::Abs),
            ("exp", UnaryOp::Exp),
            ("log", UnaryOp::Log),
            ("relu", UnaryOp::Relu),
            ("sigmoid", UnaryOp::Sigmoid),
            ("tanh", UnaryOp::Tanh),
            ("sqrt", UnaryOp::Sqrt),
            ("reciprocal", UnaryOp::Reciprocal),
            ("sin", UnaryOp::Sin),
            ("cos", UnaryOp::Cos),
            ("tan", UnaryOp::Tan),
            ("floor", UnaryOp::Floor),
            ("ceil", UnaryOp::Ceil),
            ("round", UnaryOp::Round),
            ("log2", UnaryOp::Log2),
            ("log10", UnaryOp::Log10),
            ("log1p", UnaryOp::Log1p),
            ("expm1", UnaryOp::Expm1),
            ("sign", UnaryOp::Sign),
            ("trunc", UnaryOp::Trunc),
            ("frac", UnaryOp::Frac),
            ("asin", UnaryOp::Asin),
            ("acos", UnaryOp::Acos),
            ("atan", UnaryOp::Atan),
            ("sinh", UnaryOp::Sinh),
            ("cosh", UnaryOp::Cosh),
            ("gelu", UnaryOp::Gelu),
            ("silu", UnaryOp::Silu),
            ("leaky_relu", UnaryOp::LeakyRelu),
            ("elu", UnaryOp::Elu),
            ("rsqrt", UnaryOp::Rsqrt),
            ("erf", UnaryOp::Erf),
            ("erfc", UnaryOp::Erfc),
            ("hardswish", UnaryOp::Hardswish),
            ("hardsigmoid", UnaryOp::Hardsigmoid),
            ("hardtanh", UnaryOp::Hardtanh),
            ("softplus", UnaryOp::Softplus),
            ("mish", UnaryOp::Mish),
            ("square", UnaryOp::Square),
        ];
        for (base, expected) in &cases {
            assert_eq!(
                UnaryOp::from_schema_base(base),
                Some(*expected),
                "from_schema_base(\"{base}\") should map correctly"
            );
        }
        assert_eq!(UnaryOp::from_schema_base("nonexistent"), None);
    }

    #[test]
    fn reduction_op_from_schema_base() {
        assert_eq!(
            super::ReductionOp::from_schema_base("sum"),
            Some(super::ReductionOp::Sum)
        );
        assert_eq!(
            super::ReductionOp::from_schema_base("mean"),
            Some(super::ReductionOp::Mean)
        );
        assert_eq!(
            super::ReductionOp::from_schema_base("prod"),
            Some(super::ReductionOp::Prod)
        );
        assert_eq!(
            super::ReductionOp::from_schema_base("var"),
            Some(super::ReductionOp::Var)
        );
        assert_eq!(
            super::ReductionOp::from_schema_base("std"),
            Some(super::ReductionOp::Std)
        );
        assert_eq!(super::ReductionOp::from_schema_base("xyz"), None);
    }

    #[test]
    fn normalize_op_from_schema_base() {
        assert_eq!(
            super::NormalizeOp::from_schema_base("softmax"),
            Some(super::NormalizeOp::Softmax)
        );
        assert_eq!(
            super::NormalizeOp::from_schema_base("log_softmax"),
            Some(super::NormalizeOp::LogSoftmax)
        );
        assert_eq!(super::NormalizeOp::from_schema_base("nope"), None);
    }

    #[test]
    fn join_op_from_schema_base() {
        assert_eq!(
            super::JoinOp::from_schema_base("cat"),
            Some(super::JoinOp::Cat)
        );
        assert_eq!(
            super::JoinOp::from_schema_base("stack"),
            Some(super::JoinOp::Stack)
        );
        assert_eq!(super::JoinOp::from_schema_base("nope"), None);
    }

    #[test]
    fn comparison_op_from_schema_base() {
        assert_eq!(ComparisonOp::from_schema_base("eq"), Some(ComparisonOp::Eq));
        assert_eq!(ComparisonOp::from_schema_base("ne"), Some(ComparisonOp::Ne));
        assert_eq!(ComparisonOp::from_schema_base("lt"), Some(ComparisonOp::Lt));
        assert_eq!(ComparisonOp::from_schema_base("gt"), Some(ComparisonOp::Gt));
        assert_eq!(ComparisonOp::from_schema_base("le"), Some(ComparisonOp::Le));
        assert_eq!(ComparisonOp::from_schema_base("ge"), Some(ComparisonOp::Ge));
        assert_eq!(ComparisonOp::from_schema_base("xyz"), None);
    }

    //  bd-2rfh: SchemaRegistry len/is_empty/iter 

    #[test]
    fn schema_registry_len_and_is_empty() {
        let mut registry = SchemaRegistry::new();
        assert!(registry.is_empty());
        assert_eq!(registry.len(), 0);

        let keyset =
            super::DispatchKeySet::from_keys(&[DispatchKey::CPU, DispatchKey::AutogradCPU]);
        let parsed = parse_schema_or_name("add").unwrap();
        registry.register(&parsed, keyset).unwrap();
        assert!(!registry.is_empty());
        assert_eq!(registry.len(), 1);
    }

    #[test]
    fn schema_registry_iter() {
        let mut registry = SchemaRegistry::new();
        let keyset =
            super::DispatchKeySet::from_keys(&[DispatchKey::CPU, DispatchKey::AutogradCPU]);
        let parsed_add = parse_schema_or_name("add").unwrap();
        let parsed_sub = parse_schema_or_name("sub").unwrap();
        registry.register(&parsed_add, keyset).unwrap();
        registry.register(&parsed_sub, keyset).unwrap();

        let names: Vec<&str> = registry
            .iter()
            .map(|e| e.normalized_name.as_str())
            .collect();
        assert_eq!(names.len(), 2);
        assert!(names.contains(&"add"));
        assert!(names.contains(&"sub"));
    }

    //  bd-2rfh: DispatchKeyError::NoTypeKey and NoBackendKey 

    #[test]
    fn dispatch_keyset_no_type_key() {
        // A keyset with only Undefined bit set triggers NoTypeKey
        // but Undefined is bit 0, which is not in TYPE_PRIORITY
        let keyset = super::DispatchKeySet::from_keys(&[DispatchKey::Undefined]);
        let err = keyset
            .highest_priority_type_id()
            .expect_err("should fail with NoTypeKey");
        assert_eq!(err, DispatchKeyError::NoTypeKey);
    }

    #[test]
    fn dispatch_keyset_no_backend_key() {
        // A keyset with AutogradCPU only  has type key but no backend key
        // But validate_for_scalar_binary rejects this first. Test highest_priority_backend_type_id directly.
        let keyset = super::DispatchKeySet::from_keys(&[DispatchKey::AutogradCPU]);
        let err = keyset
            .highest_priority_backend_type_id()
            .expect_err("should fail with NoBackendKey");
        assert_eq!(err, DispatchKeyError::NoBackendKey);
    }
}
