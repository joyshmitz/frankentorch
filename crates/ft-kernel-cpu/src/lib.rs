#![forbid(unsafe_code)]

use std::fmt;

use ft_core::{ScalarTensor, TensorCompatError, TensorMeta, ensure_compatible};

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum KernelError {
    Incompatible(TensorCompatError),
    ShapeMismatch {
        lhs: Vec<usize>,
        rhs: Vec<usize>,
    },
    UnsupportedLayout {
        side: &'static str,
    },
    StorageSpanOverflow {
        side: &'static str,
        storage_offset: usize,
        numel: usize,
    },
    InsufficientStorage {
        side: &'static str,
        needed: usize,
        available: usize,
    },
    InvalidDimension {
        dim: usize,
        ndim: usize,
    },
    ShapeOverflow {
        context: &'static str,
    },
}

impl fmt::Display for KernelError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Self::Incompatible(error) => write!(f, "incompatible tensors: {error}"),
            Self::ShapeMismatch { lhs, rhs } => {
                write!(f, "shape mismatch: lhs={lhs:?}, rhs={rhs:?}")
            }
            Self::UnsupportedLayout { side } => {
                write!(f, "unsupported non-contiguous layout on {side}")
            }
            Self::StorageSpanOverflow {
                side,
                storage_offset,
                numel,
            } => write!(
                f,
                "storage span overflow on {side}: storage_offset={storage_offset}, numel={numel}"
            ),
            Self::InsufficientStorage {
                side,
                needed,
                available,
            } => write!(
                f,
                "insufficient storage on {side}: needed={needed}, available={available}"
            ),
            Self::InvalidDimension { dim, ndim } => write!(
                f,
                "invalid reduction dimension {dim} for tensor with {ndim} dimensions"
            ),
            Self::ShapeOverflow { context } => {
                write!(f, "shape arithmetic overflow: {context}")
            }
        }
    }
}

impl std::error::Error for KernelError {}

impl From<TensorCompatError> for KernelError {
    fn from(value: TensorCompatError) -> Self {
        Self::Incompatible(value)
    }
}

pub fn neg_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(-input.value())
}

pub fn abs_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().abs())
}

pub fn exp_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().exp())
}

pub fn log_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().ln())
}

pub fn relu_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(if input.value() > 0.0 {
        input.value()
    } else {
        0.0
    })
}

pub fn sigmoid_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(1.0 / (1.0 + (-input.value()).exp()))
}

pub fn tanh_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().tanh())
}

pub fn sqrt_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().sqrt())
}

pub fn reciprocal_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(1.0 / input.value())
}

pub fn sin_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().sin())
}

pub fn cos_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().cos())
}

pub fn tan_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().tan())
}

pub fn floor_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().floor())
}

pub fn ceil_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().ceil())
}

pub fn round_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().round())
}

pub fn log2_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().log2())
}

pub fn log10_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().log10())
}

pub fn log1p_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().ln_1p())
}

pub fn expm1_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().exp_m1())
}

pub fn sign_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().signum())
}

pub fn trunc_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().trunc())
}

pub fn frac_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().fract())
}

pub fn asin_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().asin())
}

pub fn acos_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().acos())
}

pub fn atan_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().atan())
}

pub fn sinh_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().sinh())
}

pub fn cosh_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(input.value().cosh())
}

fn gelu_value(x: f64) -> f64 {
    // GELU(x) = 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))
    let c = std::f64::consts::FRAC_2_SQRT_PI * std::f64::consts::FRAC_1_SQRT_2; // sqrt(2/pi)
    let k = c * (x + 0.044715 * x * x * x);
    0.5 * x * (1.0 + k.tanh())
}

pub fn gelu_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(gelu_value(input.value()))
}

fn silu_value(x: f64) -> f64 {
    x / (1.0 + (-x).exp())
}

pub fn silu_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(silu_value(input.value()))
}

fn leaky_relu_value(x: f64) -> f64 {
    if x >= 0.0 { x } else { 0.01 * x }
}

pub fn leaky_relu_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(leaky_relu_value(input.value()))
}

fn elu_value(x: f64) -> f64 {
    if x > 0.0 { x } else { x.exp() - 1.0 }
}

pub fn elu_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(elu_value(input.value()))
}

pub fn rsqrt_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(1.0 / input.value().sqrt())
}

/// Gauss error function approximation (Abramowitz and Stegun, formula 7.1.26).
/// Maximum error: |ε(x)| ≤ 1.5×10⁻⁷.
fn erf_value(x: f64) -> f64 {
    let a1 = 0.254829592;
    let a2 = -0.284496736;
    let a3 = 1.421413741;
    let a4 = -1.453152027;
    let a5 = 1.061405429;
    let p = 0.3275911;
    let sign = if x < 0.0 { -1.0 } else { 1.0 };
    let abs_x = x.abs();
    let t = 1.0 / (1.0 + p * abs_x);
    let y = 1.0 - (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t * (-abs_x * abs_x).exp();
    sign * y
}

pub fn erf_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(erf_value(input.value()))
}

pub fn erfc_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(1.0 - erf_value(input.value()))
}

fn hardswish_value(x: f64) -> f64 {
    if x <= -3.0 {
        0.0
    } else if x >= 3.0 {
        x
    } else {
        x * (x + 3.0) / 6.0
    }
}

pub fn hardswish_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(hardswish_value(input.value()))
}

fn hardsigmoid_value(x: f64) -> f64 {
    if x <= -3.0 {
        0.0
    } else if x >= 3.0 {
        1.0
    } else {
        (x + 3.0) / 6.0
    }
}

pub fn hardsigmoid_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(hardsigmoid_value(input.value()))
}

fn hardtanh_value(x: f64) -> f64 {
    x.clamp(-1.0, 1.0)
}

pub fn hardtanh_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(hardtanh_value(input.value()))
}

fn softplus_value(x: f64) -> f64 {
    // Numerically stable: for large x, softplus(x) ≈ x
    if x > 20.0 {
        x
    } else if x < -20.0 {
        0.0
    } else {
        (1.0 + x.exp()).ln()
    }
}

pub fn softplus_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(softplus_value(input.value()))
}

fn mish_value(x: f64) -> f64 {
    x * softplus_value(x).tanh()
}

pub fn mish_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(mish_value(input.value()))
}

pub fn square_scalar(input: &ScalarTensor) -> ScalarTensor {
    let v = input.value();
    input.with_value(v * v)
}

pub fn isnan_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(if input.value().is_nan() { 1.0 } else { 0.0 })
}

pub fn isinf_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(if input.value().is_infinite() {
        1.0
    } else {
        0.0
    })
}

pub fn isfinite_scalar(input: &ScalarTensor) -> ScalarTensor {
    input.with_value(if input.value().is_finite() { 1.0 } else { 0.0 })
}

pub fn pow_scalar(input: &ScalarTensor, exponent: f64) -> ScalarTensor {
    input.with_value(input.value().powf(exponent))
}

pub fn clamp_scalar(input: &ScalarTensor, min_val: f64, max_val: f64) -> ScalarTensor {
    let value = input.value();
    let clamped = if value.is_nan() {
        f64::NAN
    } else if !min_val.is_nan() && value < min_val {
        min_val
    } else if !max_val.is_nan() && value > max_val {
        max_val
    } else {
        value
    };
    input.with_value(clamped)
}

pub fn min_scalar(lhs: &ScalarTensor, rhs: &ScalarTensor) -> Result<ScalarTensor, KernelError> {
    ensure_compatible(lhs, rhs)?;
    let val = if lhs.value().is_nan() || rhs.value().is_nan() {
        f64::NAN
    } else {
        lhs.value().min(rhs.value())
    };
    Ok(lhs.with_value(val))
}

pub fn max_scalar(lhs: &ScalarTensor, rhs: &ScalarTensor) -> Result<ScalarTensor, KernelError> {
    ensure_compatible(lhs, rhs)?;
    let val = if lhs.value().is_nan() || rhs.value().is_nan() {
        f64::NAN
    } else {
        lhs.value().max(rhs.value())
    };
    Ok(lhs.with_value(val))
}

pub fn eq_scalar(lhs: &ScalarTensor, rhs: &ScalarTensor) -> Result<ScalarTensor, KernelError> {
    ensure_compatible(lhs, rhs)?;
    Ok(lhs.with_value(if lhs.value() == rhs.value() { 1.0 } else { 0.0 }))
}

pub fn ne_scalar(lhs: &ScalarTensor, rhs: &ScalarTensor) -> Result<ScalarTensor, KernelError> {
    ensure_compatible(lhs, rhs)?;
    Ok(lhs.with_value(if lhs.value() != rhs.value() { 1.0 } else { 0.0 }))
}

pub fn lt_scalar(lhs: &ScalarTensor, rhs: &ScalarTensor) -> Result<ScalarTensor, KernelError> {
    ensure_compatible(lhs, rhs)?;
    Ok(lhs.with_value(if lhs.value() < rhs.value() { 1.0 } else { 0.0 }))
}

pub fn gt_scalar(lhs: &ScalarTensor, rhs: &ScalarTensor) -> Result<ScalarTensor, KernelError> {
    ensure_compatible(lhs, rhs)?;
    Ok(lhs.with_value(if lhs.value() > rhs.value() { 1.0 } else { 0.0 }))
}

pub fn le_scalar(lhs: &ScalarTensor, rhs: &ScalarTensor) -> Result<ScalarTensor, KernelError> {
    ensure_compatible(lhs, rhs)?;
    Ok(lhs.with_value(if lhs.value() <= rhs.value() { 1.0 } else { 0.0 }))
}

pub fn ge_scalar(lhs: &ScalarTensor, rhs: &ScalarTensor) -> Result<ScalarTensor, KernelError> {
    ensure_compatible(lhs, rhs)?;
    Ok(lhs.with_value(if lhs.value() >= rhs.value() { 1.0 } else { 0.0 }))
}

pub fn add_scalar(lhs: &ScalarTensor, rhs: &ScalarTensor) -> Result<ScalarTensor, KernelError> {
    ensure_compatible(lhs, rhs)?;
    Ok(lhs.with_value(lhs.value() + rhs.value()))
}

pub fn sub_scalar(lhs: &ScalarTensor, rhs: &ScalarTensor) -> Result<ScalarTensor, KernelError> {
    ensure_compatible(lhs, rhs)?;
    Ok(lhs.with_value(lhs.value() - rhs.value()))
}

pub fn div_scalar(lhs: &ScalarTensor, rhs: &ScalarTensor) -> Result<ScalarTensor, KernelError> {
    ensure_compatible(lhs, rhs)?;
    Ok(lhs.with_value(lhs.value() / rhs.value()))
}

pub fn mul_scalar(lhs: &ScalarTensor, rhs: &ScalarTensor) -> Result<ScalarTensor, KernelError> {
    ensure_compatible(lhs, rhs)?;
    Ok(lhs.with_value(lhs.value() * rhs.value()))
}

pub fn atan2_scalar(lhs: &ScalarTensor, rhs: &ScalarTensor) -> Result<ScalarTensor, KernelError> {
    ensure_compatible(lhs, rhs)?;
    Ok(lhs.with_value(lhs.value().atan2(rhs.value())))
}

pub fn fmod_scalar(lhs: &ScalarTensor, rhs: &ScalarTensor) -> Result<ScalarTensor, KernelError> {
    ensure_compatible(lhs, rhs)?;
    Ok(lhs.with_value(lhs.value() % rhs.value()))
}

pub fn remainder_scalar(
    lhs: &ScalarTensor,
    rhs: &ScalarTensor,
) -> Result<ScalarTensor, KernelError> {
    ensure_compatible(lhs, rhs)?;
    let a = lhs.value();
    let b = rhs.value();
    Ok(lhs.with_value(a - (a / b).floor() * b))
}

fn ensure_dtype_device_and_layout(lhs: &TensorMeta, rhs: &TensorMeta) -> Result<(), KernelError> {
    if lhs.dtype() != rhs.dtype() {
        return Err(KernelError::Incompatible(
            TensorCompatError::DTypeMismatch {
                lhs: lhs.dtype(),
                rhs: rhs.dtype(),
            },
        ));
    }

    if lhs.device() != rhs.device() {
        return Err(KernelError::Incompatible(
            TensorCompatError::DeviceMismatch {
                lhs: lhs.device(),
                rhs: rhs.device(),
            },
        ));
    }

    if !lhs.is_contiguous() {
        return Err(KernelError::UnsupportedLayout { side: "lhs" });
    }

    if !rhs.is_contiguous() {
        return Err(KernelError::UnsupportedLayout { side: "rhs" });
    }

    Ok(())
}

fn ensure_meta_compatible(lhs: &TensorMeta, rhs: &TensorMeta) -> Result<(), KernelError> {
    ensure_dtype_device_and_layout(lhs, rhs)?;

    if lhs.shape() != rhs.shape() {
        return Err(KernelError::ShapeMismatch {
            lhs: lhs.shape().to_vec(),
            rhs: rhs.shape().to_vec(),
        });
    }

    Ok(())
}

fn matmul_dims(
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<(usize, usize, usize), KernelError> {
    if lhs_meta.shape().len() != 2 || rhs_meta.shape().len() != 2 {
        return Err(KernelError::ShapeMismatch {
            lhs: lhs_meta.shape().to_vec(),
            rhs: rhs_meta.shape().to_vec(),
        });
    }

    let m = lhs_meta.shape()[0];
    let k = lhs_meta.shape()[1];
    let rhs_k = rhs_meta.shape()[0];
    let n = rhs_meta.shape()[1];
    if k != rhs_k {
        return Err(KernelError::ShapeMismatch {
            lhs: lhs_meta.shape().to_vec(),
            rhs: rhs_meta.shape().to_vec(),
        });
    }

    Ok((m, k, n))
}

fn contiguous_required_len(meta: &TensorMeta, side: &'static str) -> Result<usize, KernelError> {
    let numel = meta.numel();
    if numel == 0 {
        return Ok(0);
    }

    meta.storage_offset()
        .checked_add(numel)
        .ok_or(KernelError::StorageSpanOverflow {
            side,
            storage_offset: meta.storage_offset(),
            numel,
        })
}

fn ensure_storage_len(
    buffer: &[f64],
    meta: &TensorMeta,
    side: &'static str,
) -> Result<(), KernelError> {
    let needed = contiguous_required_len(meta, side)?;
    if buffer.len() < needed {
        return Err(KernelError::InsufficientStorage {
            side,
            needed,
            available: buffer.len(),
        });
    }
    Ok(())
}

fn elementwise_contiguous_f64<F>(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
    op: F,
) -> Result<Vec<f64>, KernelError>
where
    F: Fn(f64, f64) -> f64,
{
    ensure_meta_compatible(lhs_meta, rhs_meta)?;
    ensure_storage_len(lhs, lhs_meta, "lhs")?;
    ensure_storage_len(rhs, rhs_meta, "rhs")?;

    let numel = lhs_meta.numel();
    if numel == 0 {
        return Ok(Vec::new());
    }

    let lhs_start = lhs_meta.storage_offset();
    let rhs_start = rhs_meta.storage_offset();
    let lhs_window = &lhs[lhs_start..lhs_start + numel];
    let rhs_window = &rhs[rhs_start..rhs_start + numel];

    Ok(lhs_window
        .iter()
        .zip(rhs_window.iter())
        .map(|(left, right)| op(*left, *right))
        .collect())
}

fn ensure_unary_layout_and_storage(buffer: &[f64], meta: &TensorMeta) -> Result<(), KernelError> {
    if !meta.is_contiguous() {
        return Err(KernelError::UnsupportedLayout { side: "input" });
    }
    ensure_storage_len(buffer, meta, "input")
}

fn checked_shape_numel(shape: &[usize], context: &'static str) -> Result<usize, KernelError> {
    let mut product = 1usize;
    for dim in shape.iter().copied() {
        if dim == 0 {
            return Ok(0);
        }
        product = product
            .checked_mul(dim)
            .ok_or(KernelError::ShapeOverflow { context })?;
    }
    Ok(product)
}

fn checked_mul(lhs: usize, rhs: usize, context: &'static str) -> Result<usize, KernelError> {
    lhs.checked_mul(rhs)
        .ok_or(KernelError::ShapeOverflow { context })
}

fn checked_dim_loop_sizes(
    shape: &[usize],
    dim: usize,
    context: &'static str,
) -> Result<(usize, usize, usize), KernelError> {
    let outer_size = checked_shape_numel(&shape[..dim], context)?;
    let inner_size = checked_shape_numel(&shape[dim + 1..], context)?;
    let total_size = checked_shape_numel(shape, context)?;
    Ok((outer_size, inner_size, total_size))
}

fn unary_contiguous_f64<F>(input: &[f64], meta: &TensorMeta, op: F) -> Result<Vec<f64>, KernelError>
where
    F: Fn(f64) -> f64,
{
    ensure_unary_layout_and_storage(input, meta)?;

    let numel = meta.numel();
    if numel == 0 {
        return Ok(Vec::new());
    }

    let start = meta.storage_offset();
    let window = &input[start..start + numel];

    Ok(window.iter().map(|value| op(*value)).collect())
}

fn broadcast_strides(
    input_shape: &[usize],
    target_shape: &[usize],
    context: &'static str,
) -> Result<Vec<usize>, KernelError> {
    let ndim = input_shape.len();
    debug_assert_eq!(ndim, target_shape.len());

    let mut strides = vec![0usize; ndim];
    let mut running_stride = 1usize;
    for d in (0..ndim).rev() {
        strides[d] = if input_shape[d] == 1 && target_shape[d] > 1 {
            0
        } else {
            running_stride
        };
        running_stride = running_stride
            .checked_mul(input_shape[d])
            .ok_or(KernelError::ShapeOverflow { context })?;
    }

    Ok(strides)
}

pub fn reduce_sum_for_broadcast(
    expanded_grad: &[f64],
    expanded_shape: &[usize],
    original_shape: &[usize],
) -> Result<Vec<f64>, KernelError> {
    let ndim = expanded_shape.len();
    if ndim != original_shape.len() {
        return Err(KernelError::ShapeMismatch {
            lhs: expanded_shape.to_vec(),
            rhs: original_shape.to_vec(),
        });
    }

    for d in 0..ndim {
        let expanded = expanded_shape[d];
        let original = original_shape[d];
        if original != expanded && original != 1 {
            return Err(KernelError::ShapeMismatch {
                lhs: expanded_shape.to_vec(),
                rhs: original_shape.to_vec(),
            });
        }
    }

    let expanded_numel = checked_shape_numel(expanded_shape, "broadcast reduction expanded shape")?;
    if expanded_grad.len() != expanded_numel {
        return Err(KernelError::ShapeMismatch {
            lhs: vec![expanded_grad.len()],
            rhs: vec![expanded_numel],
        });
    }

    let original_numel = checked_shape_numel(original_shape, "broadcast reduction original shape")?;
    if original_numel == 0 {
        return Ok(Vec::new());
    }

    let original_strides =
        broadcast_strides(original_shape, expanded_shape, "broadcast strides overflow")?;
    let mut reduced = vec![0.0; original_numel];
    let mut coords = vec![0usize; ndim];

    for grad in expanded_grad {
        let mut original_idx = 0usize;
        for d in 0..ndim {
            original_idx += coords[d] * original_strides[d];
        }
        reduced[original_idx] += *grad;

        for d in (0..ndim).rev() {
            coords[d] += 1;
            if coords[d] < expanded_shape[d] {
                break;
            }
            coords[d] = 0;
        }
    }

    Ok(reduced)
}

pub fn neg_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| -value)
}

pub fn abs_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.abs())
}

pub fn exp_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.exp())
}

pub fn log_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.ln())
}

pub fn relu_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| if value > 0.0 { value } else { 0.0 })
}

pub fn sigmoid_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| 1.0 / (1.0 + (-value).exp()))
}

pub fn tanh_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.tanh())
}

pub fn sqrt_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.sqrt())
}

pub fn reciprocal_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| 1.0 / value)
}

pub fn sin_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.sin())
}

pub fn cos_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.cos())
}

pub fn tan_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.tan())
}

pub fn floor_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.floor())
}

pub fn ceil_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.ceil())
}

pub fn round_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.round())
}

pub fn log2_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.log2())
}

pub fn log10_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.log10())
}

pub fn log1p_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.ln_1p())
}

pub fn expm1_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.exp_m1())
}

pub fn sign_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.signum())
}

pub fn trunc_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.trunc())
}

pub fn frac_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.fract())
}

pub fn asin_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.asin())
}

pub fn acos_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.acos())
}

pub fn atan_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.atan())
}

pub fn sinh_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.sinh())
}

pub fn cosh_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |value| value.cosh())
}

pub fn gelu_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, gelu_value)
}

pub fn silu_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, silu_value)
}

pub fn leaky_relu_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, leaky_relu_value)
}

pub fn elu_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, elu_value)
}

pub fn rsqrt_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |v| 1.0 / v.sqrt())
}

pub fn erf_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, erf_value)
}

pub fn erfc_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |v| 1.0 - erf_value(v))
}

pub fn hardswish_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, hardswish_value)
}

pub fn hardsigmoid_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, hardsigmoid_value)
}

pub fn hardtanh_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, hardtanh_value)
}

pub fn softplus_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, softplus_value)
}

pub fn mish_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, mish_value)
}

pub fn square_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |v| v * v)
}

pub fn isnan_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |v| if v.is_nan() { 1.0 } else { 0.0 })
}

pub fn isinf_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |v| if v.is_infinite() { 1.0 } else { 0.0 })
}

pub fn isfinite_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    unary_contiguous_f64(input, meta, |v| if v.is_finite() { 1.0 } else { 0.0 })
}

pub fn pow_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    exponent: f64,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;

    let numel = meta.numel();
    if numel == 0 {
        return Ok(Vec::new());
    }

    let start = meta.storage_offset();
    let window = &input[start..start + numel];

    Ok(window.iter().map(|value| value.powf(exponent)).collect())
}

pub fn clamp_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    min_val: f64,
    max_val: f64,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;

    let numel = meta.numel();
    if numel == 0 {
        return Ok(Vec::new());
    }

    let start = meta.storage_offset();
    let window = &input[start..start + numel];

    Ok(window
        .iter()
        .map(|value| {
            if value.is_nan() {
                f64::NAN
            } else if !min_val.is_nan() && *value < min_val {
                min_val
            } else if !max_val.is_nan() && *value > max_val {
                max_val
            } else {
                *value
            }
        })
        .collect())
}

pub fn min_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    elementwise_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta, |l, r| {
        if l.is_nan() || r.is_nan() {
            f64::NAN
        } else {
            l.min(r)
        }
    })
}

pub fn max_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    elementwise_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta, |l, r| {
        if l.is_nan() || r.is_nan() {
            f64::NAN
        } else {
            l.max(r)
        }
    })
}

pub fn atan2_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    elementwise_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta, |y, x| y.atan2(x))
}

pub fn fmod_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    elementwise_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta, |a, b| a % b)
}

pub fn remainder_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    elementwise_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta, |a, b| a - (a / b).floor() * b)
}

pub fn eq_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    elementwise_contiguous_f64(
        lhs,
        rhs,
        lhs_meta,
        rhs_meta,
        |l, r| {
            if l == r { 1.0 } else { 0.0 }
        },
    )
}

pub fn ne_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    elementwise_contiguous_f64(
        lhs,
        rhs,
        lhs_meta,
        rhs_meta,
        |l, r| {
            if l != r { 1.0 } else { 0.0 }
        },
    )
}

pub fn lt_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    elementwise_contiguous_f64(
        lhs,
        rhs,
        lhs_meta,
        rhs_meta,
        |l, r| {
            if l < r { 1.0 } else { 0.0 }
        },
    )
}

pub fn gt_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    elementwise_contiguous_f64(
        lhs,
        rhs,
        lhs_meta,
        rhs_meta,
        |l, r| {
            if l > r { 1.0 } else { 0.0 }
        },
    )
}

pub fn le_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    elementwise_contiguous_f64(
        lhs,
        rhs,
        lhs_meta,
        rhs_meta,
        |l, r| {
            if l <= r { 1.0 } else { 0.0 }
        },
    )
}

pub fn ge_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    elementwise_contiguous_f64(
        lhs,
        rhs,
        lhs_meta,
        rhs_meta,
        |l, r| {
            if l >= r { 1.0 } else { 0.0 }
        },
    )
}

pub fn sum_tensor_contiguous_f64(input: &[f64], meta: &TensorMeta) -> Result<f64, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let offset = meta.storage_offset();
    let numel = meta.numel();
    Ok(input[offset..offset + numel].iter().sum())
}

pub fn mean_tensor_contiguous_f64(input: &[f64], meta: &TensorMeta) -> Result<f64, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let offset = meta.storage_offset();
    let numel = meta.numel();
    if numel == 0 {
        return Ok(f64::NAN);
    }
    let sum: f64 = input[offset..offset + numel].iter().sum();
    Ok(sum / numel as f64)
}

pub fn sum_dim_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "sum_dim shape volume overflow")?;
    let out_numel = checked_mul(
        outer_size,
        inner_size,
        "sum_dim shape multiplication overflow",
    )?;
    let mut output = vec![0.0; out_numel];
    let data = &input[offset..];

    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut sum = 0.0;
            for r in 0..reduce_size {
                sum += data[outer * reduce_size * inner_size + r * inner_size + inner];
            }
            output[outer * inner_size + inner] = sum;
        }
    }

    Ok(output)
}

pub fn mean_dim_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "mean_dim shape volume overflow")?;
    if reduce_size == 0 {
        let out_numel = checked_mul(
            outer_size,
            inner_size,
            "mean_dim shape multiplication overflow",
        )?;
        return Ok(vec![f64::NAN; out_numel]);
    }
    let mut output = sum_dim_tensor_contiguous_f64(input, meta, dim)?;
    let scale = 1.0 / reduce_size as f64;
    for v in &mut output {
        *v *= scale;
    }
    Ok(output)
}

pub fn add_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    elementwise_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta, |left, right| left + right)
}

pub fn sub_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    elementwise_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta, |left, right| left - right)
}

pub fn div_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    elementwise_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta, |left, right| left / right)
}

pub fn mul_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    elementwise_contiguous_f64(lhs, rhs, lhs_meta, rhs_meta, |left, right| left * right)
}

pub fn matmul_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    ensure_dtype_device_and_layout(lhs_meta, rhs_meta)?;
    let (m, k, n) = matmul_dims(lhs_meta, rhs_meta)?;
    checked_mul(m, k, "matmul lhs shape multiplication overflow")?;
    checked_mul(k, n, "matmul rhs shape multiplication overflow")?;
    let out_numel = checked_mul(m, n, "matmul output shape multiplication overflow")?;

    ensure_storage_len(lhs, lhs_meta, "lhs")?;
    ensure_storage_len(rhs, rhs_meta, "rhs")?;

    let lhs_start = lhs_meta.storage_offset();
    let rhs_start = rhs_meta.storage_offset();
    let mut out = vec![0.0; out_numel];

    for row in 0..m {
        let out_row_base = row * n;
        let lhs_row_base = lhs_start + row * k;
        for col in 0..n {
            let mut acc = 0.0;
            for inner in 0..k {
                let lhs_idx = lhs_row_base + inner;
                let rhs_idx = rhs_start + inner * n + col;
                acc += lhs[lhs_idx] * rhs[rhs_idx];
            }
            out[out_row_base + col] = acc;
        }
    }

    Ok(out)
}

pub fn lerp_tensor_contiguous_f64(
    start: &[f64],
    end: &[f64],
    weight: f64,
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(start, meta)?;
    let numel = meta.numel();
    let offset = meta.storage_offset();
    if end.len() < offset + numel {
        return Err(KernelError::ShapeMismatch {
            lhs: meta.shape().to_vec(),
            rhs: vec![end.len()],
        });
    }
    let s = &start[offset..offset + numel];
    let e = &end[offset..offset + numel];
    Ok(s.iter()
        .zip(e.iter())
        .map(|(&sv, &ev)| sv + weight * (ev - sv))
        .collect())
}

#[allow(clippy::too_many_arguments)]
pub fn addmm_tensor_contiguous_f64(
    input: &[f64],
    mat1: &[f64],
    mat2: &[f64],
    input_meta: &TensorMeta,
    mat1_meta: &TensorMeta,
    mat2_meta: &TensorMeta,
    beta: f64,
    alpha: f64,
) -> Result<Vec<f64>, KernelError> {
    // mat1: [m, k], mat2: [k, n] => output: [m, n]
    // input must be broadcastable to [m, n]
    let (m, k, n) = matmul_dims(mat1_meta, mat2_meta)?;
    let out_numel = checked_mul(m, n, "addmm output shape multiplication overflow")?;
    ensure_storage_len(mat1, mat1_meta, "mat1")?;
    ensure_storage_len(mat2, mat2_meta, "mat2")?;

    let mat1_start = mat1_meta.storage_offset();
    let mat2_start = mat2_meta.storage_offset();
    let input_offset = input_meta.storage_offset();

    // input can be 1-D [n] or 2-D [m,n]
    let input_shape = input_meta.shape();
    let input_1d = input_shape.len() == 1 && input_shape[0] == n;
    let input_2d = input_shape.len() == 2 && input_shape[0] == m && input_shape[1] == n;
    if !input_1d && !input_2d {
        return Err(KernelError::ShapeMismatch {
            lhs: input_shape.to_vec(),
            rhs: vec![m, n],
        });
    }

    let mut out = vec![0.0; out_numel];
    for row in 0..m {
        for col in 0..n {
            let mut acc = 0.0;
            for inner in 0..k {
                acc += mat1[mat1_start + row * k + inner] * mat2[mat2_start + inner * n + col];
            }
            let bias_idx = if input_1d {
                input_offset + col
            } else {
                input_offset + row * n + col
            };
            out[row * n + col] = beta * input[bias_idx] + alpha * acc;
        }
    }
    Ok(out)
}

#[allow(clippy::too_many_arguments)]
pub fn addmv_tensor_contiguous_f64(
    input: &[f64],
    mat: &[f64],
    vec_data: &[f64],
    input_meta: &TensorMeta,
    mat_meta: &TensorMeta,
    vec_meta: &TensorMeta,
    beta: f64,
    alpha: f64,
) -> Result<Vec<f64>, KernelError> {
    // mat: [m, k], vec: [k] => output: [m]
    // input must be [m]
    let mat_shape = mat_meta.shape();
    let vec_shape = vec_meta.shape();
    if mat_shape.len() != 2 {
        return Err(KernelError::ShapeMismatch {
            lhs: mat_shape.to_vec(),
            rhs: vec![0, 0],
        });
    }
    if vec_shape.len() != 1 {
        return Err(KernelError::ShapeMismatch {
            lhs: vec_shape.to_vec(),
            rhs: vec![mat_shape[1]],
        });
    }
    let m = mat_shape[0];
    let k = mat_shape[1];
    if vec_shape[0] != k {
        return Err(KernelError::ShapeMismatch {
            lhs: vec_shape.to_vec(),
            rhs: vec![k],
        });
    }
    let input_shape = input_meta.shape();
    if input_shape.len() != 1 || input_shape[0] != m {
        return Err(KernelError::ShapeMismatch {
            lhs: input_shape.to_vec(),
            rhs: vec![m],
        });
    }
    ensure_storage_len(mat, mat_meta, "mat")?;
    ensure_storage_len(vec_data, vec_meta, "vec")?;
    ensure_storage_len(input, input_meta, "input")?;

    let mat_start = mat_meta.storage_offset();
    let vec_start = vec_meta.storage_offset();
    let input_start = input_meta.storage_offset();

    let mut out = vec![0.0; m];
    for row in 0..m {
        let mut acc = 0.0;
        for col in 0..k {
            acc += mat[mat_start + row * k + col] * vec_data[vec_start + col];
        }
        out[row] = beta * input[input_start + row] + alpha * acc;
    }
    Ok(out)
}

pub fn dot_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<f64, KernelError> {
    ensure_dtype_device_and_layout(lhs_meta, rhs_meta)?;
    if lhs_meta.shape().len() != 1 || rhs_meta.shape().len() != 1 {
        return Err(KernelError::ShapeMismatch {
            lhs: lhs_meta.shape().to_vec(),
            rhs: rhs_meta.shape().to_vec(),
        });
    }
    if lhs_meta.shape()[0] != rhs_meta.shape()[0] {
        return Err(KernelError::ShapeMismatch {
            lhs: lhs_meta.shape().to_vec(),
            rhs: rhs_meta.shape().to_vec(),
        });
    }
    ensure_storage_len(lhs, lhs_meta, "lhs")?;
    ensure_storage_len(rhs, rhs_meta, "rhs")?;

    let n = lhs_meta.shape()[0];
    let lhs_start = lhs_meta.storage_offset();
    let rhs_start = rhs_meta.storage_offset();
    let mut acc = 0.0;
    for i in 0..n {
        acc += lhs[lhs_start + i] * rhs[rhs_start + i];
    }
    Ok(acc)
}

pub fn outer_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    ensure_dtype_device_and_layout(lhs_meta, rhs_meta)?;
    if lhs_meta.shape().len() != 1 || rhs_meta.shape().len() != 1 {
        return Err(KernelError::ShapeMismatch {
            lhs: lhs_meta.shape().to_vec(),
            rhs: rhs_meta.shape().to_vec(),
        });
    }
    let m = lhs_meta.shape()[0];
    let n = rhs_meta.shape()[0];
    let out_numel = checked_mul(m, n, "outer output shape multiplication overflow")?;
    ensure_storage_len(lhs, lhs_meta, "lhs")?;
    ensure_storage_len(rhs, rhs_meta, "rhs")?;
    let lhs_start = lhs_meta.storage_offset();
    let rhs_start = rhs_meta.storage_offset();
    let mut out = vec![0.0; out_numel];

    for i in 0..m {
        for j in 0..n {
            out[i * n + j] = lhs[lhs_start + i] * rhs[rhs_start + j];
        }
    }
    Ok(out)
}

pub fn bmm_tensor_contiguous_f64(
    lhs: &[f64],
    rhs: &[f64],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    ensure_dtype_device_and_layout(lhs_meta, rhs_meta)?;
    if lhs_meta.shape().len() != 3 || rhs_meta.shape().len() != 3 {
        return Err(KernelError::ShapeMismatch {
            lhs: lhs_meta.shape().to_vec(),
            rhs: rhs_meta.shape().to_vec(),
        });
    }
    let batch = lhs_meta.shape()[0];
    let m = lhs_meta.shape()[1];
    let k = lhs_meta.shape()[2];
    let rhs_batch = rhs_meta.shape()[0];
    let rhs_k = rhs_meta.shape()[1];
    let n = rhs_meta.shape()[2];
    if batch != rhs_batch || k != rhs_k {
        return Err(KernelError::ShapeMismatch {
            lhs: lhs_meta.shape().to_vec(),
            rhs: rhs_meta.shape().to_vec(),
        });
    }
    let lhs_batch_stride = checked_mul(m, k, "bmm lhs batch stride overflow")?;
    let rhs_batch_stride = checked_mul(k, n, "bmm rhs batch stride overflow")?;
    let out_batch_stride = checked_mul(m, n, "bmm output batch stride overflow")?;
    checked_mul(
        batch,
        lhs_batch_stride,
        "bmm lhs shape multiplication overflow",
    )?;
    checked_mul(
        batch,
        rhs_batch_stride,
        "bmm rhs shape multiplication overflow",
    )?;
    let out_numel = checked_mul(
        batch,
        out_batch_stride,
        "bmm output shape multiplication overflow",
    )?;
    ensure_storage_len(lhs, lhs_meta, "lhs")?;
    ensure_storage_len(rhs, rhs_meta, "rhs")?;

    let lhs_start = lhs_meta.storage_offset();
    let rhs_start = rhs_meta.storage_offset();
    let mut out = vec![0.0; out_numel];

    for b in 0..batch {
        let lhs_base = lhs_start + b * lhs_batch_stride;
        let rhs_base = rhs_start + b * rhs_batch_stride;
        let out_base = b * out_batch_stride;
        for row in 0..m {
            for col in 0..n {
                let mut acc = 0.0;
                for inner in 0..k {
                    acc += lhs[lhs_base + row * k + inner] * rhs[rhs_base + inner * n + col];
                }
                out[out_base + row * n + col] = acc;
            }
        }
    }
    Ok(out)
}

pub fn trace_tensor_contiguous_f64(input: &[f64], meta: &TensorMeta) -> Result<f64, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    if meta.shape().len() != 2 {
        return Err(KernelError::InvalidDimension {
            dim: meta.shape().len(),
            ndim: 2,
        });
    }
    let rows = meta.shape()[0];
    let cols = meta.shape()[1];
    let diag_len = rows.min(cols);
    let offset = meta.storage_offset();
    let mut acc = 0.0;
    for i in 0..diag_len {
        acc += input[offset + i * cols + i];
    }
    Ok(acc)
}

pub fn prod_dim_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "prod_dim shape volume overflow")?;
    let out_numel = checked_mul(
        outer_size,
        inner_size,
        "prod_dim shape multiplication overflow",
    )?;
    let mut output = vec![1.0; out_numel];
    let data = &input[offset..];

    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut prod = 1.0;
            for r in 0..reduce_size {
                prod *= data[outer * reduce_size * inner_size + r * inner_size + inner];
            }
            output[outer * inner_size + inner] = prod;
        }
    }

    Ok(output)
}

pub fn var_dim_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "var_dim shape volume overflow")?;
    let out_numel = checked_mul(
        outer_size,
        inner_size,
        "var_dim shape multiplication overflow",
    )?;
    let data = &input[offset..];

    if reduce_size < 2 {
        return Ok(vec![f64::NAN; out_numel]);
    }

    let mut output = vec![0.0; out_numel];
    let correction = (reduce_size - 1) as f64; // Bessel's correction

    for outer in 0..outer_size {
        for inner in 0..inner_size {
            // Compute mean
            let mut sum = 0.0;
            for r in 0..reduce_size {
                sum += data[outer * reduce_size * inner_size + r * inner_size + inner];
            }
            let mean = sum / reduce_size as f64;
            // Compute variance
            let mut var_sum = 0.0;
            for r in 0..reduce_size {
                let diff = data[outer * reduce_size * inner_size + r * inner_size + inner] - mean;
                var_sum += diff * diff;
            }
            output[outer * inner_size + inner] = var_sum / correction;
        }
    }

    Ok(output)
}

pub fn std_dim_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f64>, KernelError> {
    let mut output = var_dim_tensor_contiguous_f64(input, meta, dim)?;
    for v in &mut output {
        *v = v.sqrt();
    }
    Ok(output)
}

pub fn norm_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    p: f64,
) -> Result<f64, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let offset = meta.storage_offset();
    let numel = meta.numel();
    if numel == 0 {
        return Ok(0.0);
    }
    let data = &input[offset..offset + numel];

    if p == f64::INFINITY {
        Ok(data.iter().fold(0.0_f64, |acc, &x| acc.max(x.abs())))
    } else if p == f64::NEG_INFINITY {
        Ok(data.iter().fold(f64::INFINITY, |acc, &x| acc.min(x.abs())))
    } else if p == 0.0 {
        // L0 "norm": count of non-zero elements
        Ok(data.iter().filter(|&&x| x != 0.0).count() as f64)
    } else if p == 1.0 {
        Ok(data.iter().map(|x| x.abs()).sum())
    } else if p == 2.0 {
        let sum_sq: f64 = data.iter().map(|x| x * x).sum();
        Ok(sum_sq.sqrt())
    } else {
        let sum_pow: f64 = data.iter().map(|x| x.abs().powf(p)).sum();
        Ok(sum_pow.powf(1.0 / p))
    }
}

pub fn norm_dim_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    p: f64,
    dim: usize,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "norm_dim shape volume overflow")?;
    let out_numel = checked_mul(
        outer_size,
        inner_size,
        "norm_dim shape multiplication overflow",
    )?;
    let data = &input[offset..];

    let mut output = vec![0.0; out_numel];

    if p == f64::INFINITY {
        for outer in 0..outer_size {
            for inner in 0..inner_size {
                let mut max_abs = 0.0_f64;
                for r in 0..reduce_size {
                    max_abs = max_abs
                        .max(data[outer * reduce_size * inner_size + r * inner_size + inner].abs());
                }
                output[outer * inner_size + inner] = max_abs;
            }
        }
    } else if p == f64::NEG_INFINITY {
        for outer in 0..outer_size {
            for inner in 0..inner_size {
                let mut min_abs = f64::INFINITY;
                for r in 0..reduce_size {
                    min_abs = min_abs
                        .min(data[outer * reduce_size * inner_size + r * inner_size + inner].abs());
                }
                output[outer * inner_size + inner] = min_abs;
            }
        }
    } else if p == 0.0 {
        for outer in 0..outer_size {
            for inner in 0..inner_size {
                let mut count = 0.0;
                for r in 0..reduce_size {
                    if data[outer * reduce_size * inner_size + r * inner_size + inner] != 0.0 {
                        count += 1.0;
                    }
                }
                output[outer * inner_size + inner] = count;
            }
        }
    } else if p == 1.0 {
        for outer in 0..outer_size {
            for inner in 0..inner_size {
                let mut sum = 0.0;
                for r in 0..reduce_size {
                    sum += data[outer * reduce_size * inner_size + r * inner_size + inner].abs();
                }
                output[outer * inner_size + inner] = sum;
            }
        }
    } else if p == 2.0 {
        for outer in 0..outer_size {
            for inner in 0..inner_size {
                let mut sum_sq = 0.0;
                for r in 0..reduce_size {
                    let v = data[outer * reduce_size * inner_size + r * inner_size + inner];
                    sum_sq += v * v;
                }
                output[outer * inner_size + inner] = sum_sq.sqrt();
            }
        }
    } else {
        for outer in 0..outer_size {
            for inner in 0..inner_size {
                let mut sum_pow = 0.0;
                for r in 0..reduce_size {
                    sum_pow += data[outer * reduce_size * inner_size + r * inner_size + inner]
                        .abs()
                        .powf(p);
                }
                output[outer * inner_size + inner] = sum_pow.powf(1.0 / p);
            }
        }
    }

    Ok(output)
}

pub fn softmax_dim_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, numel) =
        checked_dim_loop_sizes(shape, dim, "softmax shape volume overflow")?;
    let mut output = vec![0.0; numel];
    let data = &input[offset..];

    for outer in 0..outer_size {
        for inner in 0..inner_size {
            // Find max for numerical stability
            let mut max_val = f64::NEG_INFINITY;
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                if data[idx] > max_val {
                    max_val = data[idx];
                }
            }
            // Compute exp(x - max) and sum
            let mut sum = 0.0;
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                let e = (data[idx] - max_val).exp();
                output[idx] = e;
                sum += e;
            }
            // Normalize
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                output[idx] /= sum;
            }
        }
    }

    Ok(output)
}

pub fn log_softmax_dim_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, numel) =
        checked_dim_loop_sizes(shape, dim, "log_softmax shape volume overflow")?;
    let mut output = vec![0.0; numel];
    let data = &input[offset..];

    for outer in 0..outer_size {
        for inner in 0..inner_size {
            // Find max for numerical stability
            let mut max_val = f64::NEG_INFINITY;
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                if data[idx] > max_val {
                    max_val = data[idx];
                }
            }
            // Compute log(sum(exp(x - max)))
            let mut sum_exp = 0.0;
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                sum_exp += (data[idx] - max_val).exp();
            }
            let log_sum_exp = max_val + sum_exp.ln();
            // output = x - log_sum_exp
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                output[idx] = data[idx] - log_sum_exp;
            }
        }
    }

    Ok(output)
}

pub fn argmax_dim_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "argmax shape volume overflow")?;
    let out_numel = checked_mul(
        outer_size,
        inner_size,
        "argmax shape multiplication overflow",
    )?;
    let mut output = vec![0.0; out_numel];
    let data = &input[offset..];

    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut best_idx = 0usize;
            let mut best_val = f64::NEG_INFINITY;
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                let val = data[idx];
                if val.is_nan() {
                    best_idx = r;
                    break;
                } else if val > best_val {
                    best_val = val;
                    best_idx = r;
                }
            }
            output[outer * inner_size + inner] = best_idx as f64;
        }
    }

    Ok(output)
}

pub fn argmin_dim_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "argmin shape volume overflow")?;
    let out_numel = checked_mul(
        outer_size,
        inner_size,
        "argmin shape multiplication overflow",
    )?;
    let mut output = vec![0.0; out_numel];
    let data = &input[offset..];

    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut best_idx = 0usize;
            let mut best_val = f64::INFINITY;
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                let val = data[idx];
                if val.is_nan() {
                    best_idx = r;
                    break;
                } else if val < best_val {
                    best_val = val;
                    best_idx = r;
                }
            }
            output[outer * inner_size + inner] = best_idx as f64;
        }
    }

    Ok(output)
}

pub fn max_dim_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
) -> Result<(Vec<f64>, Vec<f64>), KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "max_dim shape volume overflow")?;
    let out_numel = checked_mul(
        outer_size,
        inner_size,
        "max_dim shape multiplication overflow",
    )?;
    let mut values = vec![f64::NEG_INFINITY; out_numel];
    let mut indices = vec![0.0; out_numel];
    let data = &input[offset..];

    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let out_idx = outer * inner_size + inner;
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                let val = data[idx];
                if val.is_nan() {
                    values[out_idx] = f64::NAN;
                    indices[out_idx] = r as f64;
                    break;
                } else if val > values[out_idx] {
                    values[out_idx] = val;
                    indices[out_idx] = r as f64;
                }
            }
        }
    }

    Ok((values, indices))
}

pub fn min_dim_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
) -> Result<(Vec<f64>, Vec<f64>), KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "min_dim shape volume overflow")?;
    let out_numel = checked_mul(
        outer_size,
        inner_size,
        "min_dim shape multiplication overflow",
    )?;
    let mut values = vec![f64::INFINITY; out_numel];
    let mut indices = vec![0.0; out_numel];
    let data = &input[offset..];

    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let out_idx = outer * inner_size + inner;
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                let val = data[idx];
                if val.is_nan() {
                    values[out_idx] = f64::NAN;
                    indices[out_idx] = r as f64;
                    break;
                } else if val < values[out_idx] {
                    values[out_idx] = val;
                    indices[out_idx] = r as f64;
                }
            }
        }
    }

    Ok((values, indices))
}

pub fn cat_tensor_contiguous_f64(
    inputs: &[(&[f64], &TensorMeta)],
    dim: usize,
) -> Result<Vec<f64>, KernelError> {
    if inputs.is_empty() {
        return Err(KernelError::ShapeMismatch {
            lhs: vec![],
            rhs: vec![],
        });
    }
    let first_shape = inputs[0].1.shape();
    let ndim = first_shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    // Validate shapes match except along cat dim
    for (data, meta) in inputs {
        ensure_unary_layout_and_storage(data, meta)?;
        let shape = meta.shape();
        if shape.len() != ndim {
            return Err(KernelError::ShapeMismatch {
                lhs: first_shape.to_vec(),
                rhs: shape.to_vec(),
            });
        }
        for d in 0..ndim {
            if d != dim && shape[d] != first_shape[d] {
                return Err(KernelError::ShapeMismatch {
                    lhs: first_shape.to_vec(),
                    rhs: shape.to_vec(),
                });
            }
        }
    }

    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(first_shape, dim, "cat shape volume overflow")?;
    let total_cat_size: usize = inputs.iter().map(|(_, m)| m.shape()[dim]).sum();
    let out_numel = checked_mul(
        checked_mul(
            outer_size,
            total_cat_size,
            "cat shape multiplication overflow",
        )?,
        inner_size,
        "cat shape multiplication overflow",
    )?;
    let mut output = Vec::with_capacity(out_numel);

    for outer in 0..outer_size {
        for (data, meta) in inputs {
            let shape = meta.shape();
            let cat_size = shape[dim];
            let offset = meta.storage_offset();
            let d = &data[offset..];
            for r in 0..cat_size {
                for inner in 0..inner_size {
                    output.push(d[outer * cat_size * inner_size + r * inner_size + inner]);
                }
            }
        }
    }

    Ok(output)
}

pub fn stack_tensor_contiguous_f64(
    inputs: &[(&[f64], &TensorMeta)],
    dim: usize,
) -> Result<Vec<f64>, KernelError> {
    if inputs.is_empty() {
        return Err(KernelError::ShapeMismatch {
            lhs: vec![],
            rhs: vec![],
        });
    }
    let first_shape = inputs[0].1.shape();
    let ndim = first_shape.len();
    // dim can be 0..=ndim (inserting a new dimension)
    if dim > ndim {
        return Err(KernelError::InvalidDimension {
            dim,
            ndim: ndim + 1,
        });
    }
    // Validate all shapes are identical
    for (data, meta) in inputs {
        ensure_unary_layout_and_storage(data, meta)?;
        if meta.shape() != first_shape {
            return Err(KernelError::ShapeMismatch {
                lhs: first_shape.to_vec(),
                rhs: meta.shape().to_vec(),
            });
        }
    }

    let num_inputs = inputs.len();
    let outer_size = checked_shape_numel(&first_shape[..dim], "stack outer shape overflow")?;
    let inner_size = checked_shape_numel(&first_shape[dim..], "stack inner shape overflow")?;
    let out_numel = checked_mul(
        checked_mul(
            outer_size,
            num_inputs,
            "stack shape multiplication overflow",
        )?,
        inner_size,
        "stack shape multiplication overflow",
    )?;
    let mut output = Vec::with_capacity(out_numel);

    for outer in 0..outer_size {
        for (data, meta) in inputs {
            let offset = meta.storage_offset();
            let d = &data[offset..];
            for inner in 0..inner_size {
                output.push(d[outer * inner_size + inner]);
            }
        }
    }

    Ok(output)
}

/// Extracts a contiguous sub-range along `dim` from a contiguous tensor.
///
/// Given input with shape `S`, this selects elements `[start..start+length]`
/// along dimension `dim`, producing a tensor whose shape is `S` with
/// `S[dim]` replaced by `length`.
pub fn narrow_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
    start: usize,
    length: usize,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let end = start
        .checked_add(length)
        .ok_or(KernelError::InvalidDimension {
            dim: start,
            ndim: shape[dim],
        })?;
    if end > shape[dim] {
        return Err(KernelError::InvalidDimension {
            dim: end,
            ndim: shape[dim],
        });
    }
    if length == 0 {
        return Ok(Vec::new());
    }

    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "narrow shape volume overflow")?;
    let dim_size = shape[dim];
    let out_numel = checked_mul(
        checked_mul(outer_size, length, "narrow shape multiplication overflow")?,
        inner_size,
        "narrow shape multiplication overflow",
    )?;
    let offset = meta.storage_offset();
    let data = &input[offset..];
    let mut output = Vec::with_capacity(out_numel);

    for outer in 0..outer_size {
        for r in 0..length {
            for inner in 0..inner_size {
                let idx = outer * dim_size * inner_size + (start + r) * inner_size + inner;
                output.push(data[idx]);
            }
        }
    }

    Ok(output)
}

fn normalize_wrapped_index_value(idx_f: f64, dim_size: usize) -> Result<usize, KernelError> {
    if !idx_f.is_finite() || idx_f.fract().abs() > f64::EPSILON {
        return Err(KernelError::InvalidDimension {
            dim: dim_size,
            ndim: dim_size,
        });
    }

    let dim_size_i = isize::try_from(dim_size).map_err(|_| KernelError::InvalidDimension {
        dim: dim_size,
        ndim: dim_size,
    })?;

    if idx_f < isize::MIN as f64 || idx_f > isize::MAX as f64 {
        return Err(KernelError::InvalidDimension {
            dim: dim_size,
            ndim: dim_size,
        });
    }

    let mut idx_i = idx_f as isize;
    if idx_i < 0 {
        idx_i += dim_size_i;
    }
    if idx_i < 0 || idx_i >= dim_size_i {
        return Err(KernelError::InvalidDimension {
            dim: dim_size,
            ndim: dim_size,
        });
    }

    usize::try_from(idx_i).map_err(|_| KernelError::InvalidDimension {
        dim: dim_size,
        ndim: dim_size,
    })
}

/// Expands singleton dimensions of a contiguous tensor to a target shape.
///
/// Only dimensions of size 1 in the input can be expanded to a larger size.
/// Dimensions that already match are copied through. The input and target
/// must have the same number of dimensions.
pub fn expand_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    target_shape: &[usize],
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if target_shape.len() != ndim {
        return Err(KernelError::ShapeMismatch {
            lhs: shape.to_vec(),
            rhs: target_shape.to_vec(),
        });
    }
    // Validate: each dim must either match or be 1 in input
    for d in 0..ndim {
        if shape[d] != target_shape[d] && shape[d] != 1 {
            return Err(KernelError::ShapeMismatch {
                lhs: shape.to_vec(),
                rhs: target_shape.to_vec(),
            });
        }
    }

    let out_numel = checked_shape_numel(target_shape, "expand output shape volume overflow")?;
    if out_numel == 0 {
        return Ok(Vec::new());
    }

    let input_strides = broadcast_strides(shape, target_shape, "expand input strides overflow")?;
    let offset = meta.storage_offset();
    let mut output = Vec::with_capacity(out_numel);
    let mut coords = vec![0usize; ndim];

    for _ in 0..out_numel {
        let mut idx = offset;
        for d in 0..ndim {
            idx += coords[d] * input_strides[d];
        }
        output.push(input[idx]);

        // Increment coordinates (row-major order)
        for d in (0..ndim).rev() {
            coords[d] += 1;
            if coords[d] < target_shape[d] {
                break;
            }
            coords[d] = 0;
        }
    }

    Ok(output)
}

/// Selects rows/columns along a given dimension using the provided index array.
///
/// `indices` are stored as `f64` (matching the convention used by argmax/argmin)
/// and are cast to `usize` internally. The output shape is the input shape with
/// `shape[dim]` replaced by `indices.len()`.
pub fn index_select_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
    indices: &[f64],
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }

    let dim_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "index_select shape volume overflow")?;
    let num_indices = indices.len();
    let out_numel = checked_mul(
        checked_mul(
            outer_size,
            num_indices,
            "index_select shape multiplication overflow",
        )?,
        inner_size,
        "index_select shape multiplication overflow",
    )?;
    let offset = meta.storage_offset();
    let data = &input[offset..];
    let mut output = Vec::with_capacity(out_numel);

    for outer in 0..outer_size {
        for &idx_f in indices {
            let idx = normalize_wrapped_index_value(idx_f, dim_size)?;
            for inner in 0..inner_size {
                let src = outer * dim_size * inner_size + idx * inner_size + inner;
                output.push(data[src]);
            }
        }
    }

    Ok(output)
}

/// Gathers values along a dimension using an index tensor.
///
/// For each position in the output (which has the same shape as `index_meta`),
/// the value is taken from `input` at the position indicated by the
/// corresponding index value along `dim`.
pub fn gather_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
    index: &[f64],
    index_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    ensure_unary_layout_and_storage(index, index_meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let idx_shape = index_meta.shape();
    if idx_shape.len() != ndim {
        return Err(KernelError::ShapeMismatch {
            lhs: shape.to_vec(),
            rhs: idx_shape.to_vec(),
        });
    }
    // All dimensions except `dim` must match between input and index.
    for d in 0..ndim {
        if d != dim && idx_shape[d] != shape[d] {
            return Err(KernelError::ShapeMismatch {
                lhs: shape.to_vec(),
                rhs: idx_shape.to_vec(),
            });
        }
    }

    let dim_size = shape[dim];
    let idx_dim_size = idx_shape[dim];
    let (outer_size, inner_size, out_numel) =
        checked_dim_loop_sizes(idx_shape, dim, "gather index shape volume overflow")?;
    let offset = meta.storage_offset();
    let data = &input[offset..];
    let idx_offset = index_meta.storage_offset();
    let index_data = &index[idx_offset..];
    let mut output = Vec::with_capacity(out_numel);

    for outer in 0..outer_size {
        for r in 0..idx_dim_size {
            for inner in 0..inner_size {
                let idx_pos = outer * idx_dim_size * inner_size + r * inner_size + inner;
                let selected = normalize_wrapped_index_value(index_data[idx_pos], dim_size)?;
                let src = outer * dim_size * inner_size + selected * inner_size + inner;
                output.push(data[src]);
            }
        }
    }

    Ok(output)
}

/// Scatters source values into a copy of `input` along a dimension using an
/// index tensor.
///
/// For each position in the `index` tensor (whose shape must match `src`),
/// the corresponding value from `src` is placed into the output at the
/// position indicated by `index` along `dim`.
pub fn scatter_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
    index: &[f64],
    index_meta: &TensorMeta,
    src: &[f64],
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    ensure_unary_layout_and_storage(index, index_meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let idx_shape = index_meta.shape();
    if idx_shape.len() != ndim {
        return Err(KernelError::ShapeMismatch {
            lhs: shape.to_vec(),
            rhs: idx_shape.to_vec(),
        });
    }
    // All dimensions except `dim` must match between input and index.
    for d in 0..ndim {
        if d != dim && idx_shape[d] != shape[d] {
            return Err(KernelError::ShapeMismatch {
                lhs: shape.to_vec(),
                rhs: idx_shape.to_vec(),
            });
        }
    }
    let src_numel = checked_shape_numel(idx_shape, "scatter index shape volume overflow")?;
    if src.len() < src_numel {
        return Err(KernelError::InsufficientStorage {
            side: "src",
            needed: src_numel,
            available: src.len(),
        });
    }

    let dim_size = shape[dim];
    let idx_dim_size = idx_shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(idx_shape, dim, "scatter index shape volume overflow")?;
    let offset = meta.storage_offset();
    let numel = meta.numel();
    let mut output = input[offset..offset + numel].to_vec();
    let idx_offset = index_meta.storage_offset();
    let index_data = &index[idx_offset..];

    for outer in 0..outer_size {
        for r in 0..idx_dim_size {
            for inner in 0..inner_size {
                let idx_pos = outer * idx_dim_size * inner_size + r * inner_size + inner;
                let selected = normalize_wrapped_index_value(index_data[idx_pos], dim_size)?;
                let dst = outer * dim_size * inner_size + selected * inner_size + inner;
                output[dst] = src[idx_pos];
            }
        }
    }

    Ok(output)
}

/// Fills positions in the tensor where `mask` is non-zero with the given
/// `value`. `mask` is expected to contain 0.0 or 1.0 values (as produced by
/// comparison ops) and must have the same number of elements as `input`.
pub fn masked_fill_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    mask: &[f64],
    value: f64,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let numel = meta.numel();
    let offset = meta.storage_offset();
    if mask.len() < offset + numel {
        return Err(KernelError::ShapeMismatch {
            lhs: meta.shape().to_vec(),
            rhs: vec![mask.len()],
        });
    }

    let data = &input[offset..offset + numel];
    let output = data
        .iter()
        .zip(mask[offset..offset + numel].iter())
        .map(|(&d, &m)| if m != 0.0 { value } else { d })
        .collect();

    Ok(output)
}

/// Conditional selection: `torch.where(condition, x, y)`.
///
/// Selects elements from `x` where `condition != 0.0` and from `y` otherwise.
/// All three tensors must have the same shape.
pub fn where_tensor_contiguous_f64(
    condition: &[f64],
    x: &[f64],
    y: &[f64],
    meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(x, meta)?;
    let numel = meta.numel();
    let offset = meta.storage_offset();
    if condition.len() < offset + numel {
        return Err(KernelError::ShapeMismatch {
            lhs: meta.shape().to_vec(),
            rhs: vec![condition.len()],
        });
    }
    if y.len() < offset + numel {
        return Err(KernelError::ShapeMismatch {
            lhs: meta.shape().to_vec(),
            rhs: vec![y.len()],
        });
    }

    let cond = &condition[offset..offset + numel];
    let x_data = &x[offset..offset + numel];
    let y_data = &y[offset..offset + numel];

    let output = cond
        .iter()
        .zip(x_data.iter())
        .zip(y_data.iter())
        .map(|((&c, &xv), &yv)| if c != 0.0 { xv } else { yv })
        .collect();

    Ok(output)
}

/// Cumulative sum along a given dimension.
///
/// For a 1-D input [a, b, c] with dim=0, returns [a, a+b, a+b+c].
/// For higher-dimensional tensors, the accumulation happens along the specified dimension
/// while iterating over all other dimensions.
pub fn cumsum_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let dim_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "cumsum shape volume overflow")?;
    let numel = checked_mul(
        checked_mul(outer_size, dim_size, "cumsum shape multiplication overflow")?,
        inner_size,
        "cumsum shape multiplication overflow",
    )?;
    let mut output = vec![0.0; numel];
    let data = &input[offset..];

    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut acc = 0.0;
            for d in 0..dim_size {
                let idx = outer * dim_size * inner_size + d * inner_size + inner;
                acc += data[idx];
                output[idx] = acc;
            }
        }
    }

    Ok(output)
}

/// Backward pass for cumsum: reverse cumulative sum.
///
/// If forward is cumsum along dim, the gradient is a reverse cumsum along the same dim.
pub fn cumsum_backward_tensor_contiguous_f64(
    grad_output: &[f64],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(grad_output, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let dim_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "cumsum backward shape volume overflow")?;
    let numel = checked_mul(
        checked_mul(
            outer_size,
            dim_size,
            "cumsum backward shape multiplication overflow",
        )?,
        inner_size,
        "cumsum backward shape multiplication overflow",
    )?;
    let mut grad_input = vec![0.0; numel];
    let data = &grad_output[offset..];

    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut acc = 0.0;
            // Reverse iteration for reverse cumsum
            for d in (0..dim_size).rev() {
                let idx = outer * dim_size * inner_size + d * inner_size + inner;
                acc += data[idx];
                grad_input[idx] = acc;
            }
        }
    }

    Ok(grad_input)
}

/// Cumulative product along a given dimension.
///
/// For a 1-D input [a, b, c] with dim=0, returns [a, a*b, a*b*c].
pub fn cumprod_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let dim_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "cumprod shape volume overflow")?;
    let numel = checked_mul(
        checked_mul(
            outer_size,
            dim_size,
            "cumprod shape multiplication overflow",
        )?,
        inner_size,
        "cumprod shape multiplication overflow",
    )?;
    let mut output = vec![0.0; numel];
    let data = &input[offset..];

    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut acc = 1.0;
            for d in 0..dim_size {
                let idx = outer * dim_size * inner_size + d * inner_size + inner;
                acc *= data[idx];
                output[idx] = acc;
            }
        }
    }

    Ok(output)
}

/// Backward pass for cumprod.
///
/// The gradient of cumprod is computed using the relationship:
/// grad_input[i] = sum_{j>=i} grad_output[j] * output[j] / input[i]
/// which can be rewritten as a reverse cumsum of (grad_output * output) divided by input,
/// with special handling for zeros in the input.
pub fn cumprod_backward_tensor_contiguous_f64(
    grad_output: &[f64],
    input: &[f64],
    output: &[f64],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let dim_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "cumprod backward shape volume overflow")?;
    let numel = checked_mul(
        checked_mul(
            outer_size,
            dim_size,
            "cumprod backward shape multiplication overflow",
        )?,
        inner_size,
        "cumprod backward shape multiplication overflow",
    )?;
    let mut grad_input = vec![0.0; numel];
    let in_data = &input[offset..];
    let out_data = &output[offset..];
    let go_data = &grad_output[offset..];

    for outer in 0..outer_size {
        for inner in 0..inner_size {
            // Compute reverse cumsum of (grad_output * output)
            let mut acc = 0.0;
            for d in (0..dim_size).rev() {
                let idx = outer * dim_size * inner_size + d * inner_size + inner;
                acc += go_data[idx] * out_data[idx];
                let inp = in_data[idx];
                if inp.abs() > f64::EPSILON {
                    grad_input[idx] = acc / inp;
                } else {
                    // When input is zero, compute gradient by direct summation
                    // to avoid division by zero
                    let mut sum = 0.0;
                    for j in d..dim_size {
                        let j_idx = outer * dim_size * inner_size + j * inner_size + inner;
                        // Product of all elements except position d, from d to j
                        let mut prod = 1.0;
                        for k in d..=j {
                            if k != d {
                                let k_idx = outer * dim_size * inner_size + k * inner_size + inner;
                                prod *= in_data[k_idx];
                            }
                        }
                        // Also include the prefix product (elements before d)
                        if d > 0 {
                            let prev_idx =
                                outer * dim_size * inner_size + (d - 1) * inner_size + inner;
                            prod *= out_data[prev_idx];
                        }
                        sum += go_data[j_idx] * prod;
                    }
                    grad_input[idx] = sum;
                }
            }
        }
    }

    Ok(grad_input)
}

/// Sort a contiguous f64 tensor along the given dimension.
///
/// Returns `(sorted_values, indices)` where `indices[i]` is the original position
/// of `sorted_values[i]` along the sorted dimension.
///
/// `descending = true` sorts from largest to smallest.
pub fn sort_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    dim: usize,
    descending: bool,
) -> Result<(Vec<f64>, Vec<usize>), KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let dim_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "sort shape volume overflow")?;
    let numel = checked_mul(
        checked_mul(outer_size, dim_size, "sort shape multiplication overflow")?,
        inner_size,
        "sort shape multiplication overflow",
    )?;
    let data = &input[offset..];

    let mut sorted_values = vec![0.0; numel];
    let mut indices = vec![0usize; numel];

    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut lane: Vec<(usize, f64)> = (0..dim_size)
                .map(|d| {
                    let idx = outer * dim_size * inner_size + d * inner_size + inner;
                    (d, data[idx])
                })
                .collect();

            if descending {
                lane.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
            } else {
                lane.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));
            }

            for (out_d, (orig_d, val)) in lane.into_iter().enumerate() {
                let out_idx = outer * dim_size * inner_size + out_d * inner_size + inner;
                sorted_values[out_idx] = val;
                indices[out_idx] = orig_d;
            }
        }
    }

    Ok((sorted_values, indices))
}

/// Return the k largest (or smallest) elements along the given dimension.
///
/// Returns `(values, indices)` of shape `[..., k, ...]` where the dim-th
/// dimension is replaced by k. `largest = true` returns the k largest elements.
pub fn topk_tensor_contiguous_f64(
    input: &[f64],
    meta: &TensorMeta,
    k: usize,
    dim: usize,
    largest: bool,
    sorted: bool,
) -> Result<(Vec<f64>, Vec<usize>), KernelError> {
    ensure_unary_layout_and_storage(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let dim_size = shape[dim];
    if k > dim_size {
        return Err(KernelError::InvalidDimension {
            dim: k,
            ndim: dim_size,
        });
    }
    let offset = meta.storage_offset();
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "topk shape volume overflow")?;
    let out_numel = checked_mul(
        checked_mul(outer_size, k, "topk shape multiplication overflow")?,
        inner_size,
        "topk shape multiplication overflow",
    )?;
    let data = &input[offset..];

    let mut out_values = vec![0.0; out_numel];
    let mut out_indices = vec![0usize; out_numel];

    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut lane: Vec<(usize, f64)> = (0..dim_size)
                .map(|d| {
                    let idx = outer * dim_size * inner_size + d * inner_size + inner;
                    (d, data[idx])
                })
                .collect();

            if largest {
                lane.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
            } else {
                lane.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));
            }

            let top = &lane[..k];

            let mut selected: Vec<(usize, f64)> = top.to_vec();
            if sorted {
                // Already sorted by value from the full sort above
            } else {
                // Return in original index order
                selected.sort_by_key(|(orig_idx, _)| *orig_idx);
            }

            for (out_d, (orig_d, val)) in selected.into_iter().enumerate() {
                let out_idx = outer * k * inner_size + out_d * inner_size + inner;
                out_values[out_idx] = val;
                out_indices[out_idx] = orig_d;
            }
        }
    }

    Ok((out_values, out_indices))
}

// ---------------------------------------------------------------------------
// Linear Algebra: LU Decomposition with Partial Pivoting
// ---------------------------------------------------------------------------

/// Result of LU factorization in compact form.
///
/// The `lu` matrix stores L (lower triangle, unit diagonal implied) and U
/// (upper triangle including diagonal) packed together.
#[derive(Debug, Clone)]
pub struct LuFactorResult {
    /// Packed LU matrix in row-major order (n x n).
    pub lu: Vec<f64>,
    /// Pivot indices: row i was swapped with row pivots[i] during elimination.
    pub pivots: Vec<usize>,
    /// Matrix dimension (n x n).
    pub n: usize,
}

/// Result of full LU decomposition: P, L, U as separate matrices.
#[derive(Debug, Clone)]
pub struct LuResult {
    /// Permutation matrix (n x n) in row-major order.
    pub p: Vec<f64>,
    /// Lower triangular matrix with unit diagonal (n x n) in row-major order.
    pub l: Vec<f64>,
    /// Upper triangular matrix (n x n) in row-major order.
    pub u: Vec<f64>,
    /// Matrix dimension (n x n).
    pub n: usize,
}

/// Compute the LU factorization of a square matrix with partial pivoting.
///
/// Returns the packed LU matrix and pivot indices. The input matrix is given
/// as a contiguous row-major f64 buffer with the given `TensorMeta`.
///
/// The matrix must be 2-D and square (n x n).
pub fn lu_factor_contiguous_f64(
    data: &[f64],
    meta: &TensorMeta,
) -> Result<LuFactorResult, KernelError> {
    ensure_unary_layout_and_storage(data, meta)?;
    let shape = meta.shape();
    if shape.len() != 2 {
        return Err(KernelError::ShapeMismatch {
            lhs: shape.to_vec(),
            rhs: vec![shape.len()],
        });
    }
    let n = shape[0];
    if n != shape[1] {
        return Err(KernelError::ShapeMismatch {
            lhs: vec![n],
            rhs: vec![shape[1]],
        });
    }
    if n == 0 {
        return Ok(LuFactorResult {
            lu: Vec::new(),
            pivots: Vec::new(),
            n: 0,
        });
    }

    let offset = meta.storage_offset();
    let mut lu: Vec<f64> = data[offset..offset + n * n].to_vec();
    let mut pivots: Vec<usize> = (0..n).collect();

    for k in 0..n {
        // Find pivot: row with max |lu[i][k]| for i >= k
        let mut max_val = lu[k * n + k].abs();
        let mut max_row = k;
        for i in (k + 1)..n {
            let val = lu[i * n + k].abs();
            if val > max_val {
                max_val = val;
                max_row = i;
            }
        }

        // Swap rows k and max_row
        if max_row != k {
            pivots.swap(k, max_row);
            for j in 0..n {
                let a = k * n + j;
                let b = max_row * n + j;
                lu.swap(a, b);
            }
        }

        let diag = lu[k * n + k];
        if diag.abs() < f64::EPSILON * 1e3 {
            // Near-singular: we continue but the result may be unreliable.
            // Downstream consumers (det, inv, solve) should check for this.
            continue;
        }

        // Elimination
        for i in (k + 1)..n {
            let multiplier = lu[i * n + k] / diag;
            lu[i * n + k] = multiplier; // Store L factor
            for j in (k + 1)..n {
                lu[i * n + j] -= multiplier * lu[k * n + j];
            }
        }
    }

    Ok(LuFactorResult { lu, pivots, n })
}

/// Unpack a compact LU factorization into separate P, L, U matrices.
pub fn lu_unpack(factor: &LuFactorResult) -> LuResult {
    let n = factor.n;
    if n == 0 {
        return LuResult {
            p: Vec::new(),
            l: Vec::new(),
            u: Vec::new(),
            n: 0,
        };
    }

    // Build permutation matrix from pivot vector.
    // pivots[i] = original row that ended up at position i in the LU matrix.
    // For the convention P @ L @ U = A, we need P[pivots[i]][i] = 1.
    let mut p = vec![0.0; n * n];
    for (i, &orig_row) in factor.pivots.iter().enumerate() {
        p[orig_row * n + i] = 1.0;
    }

    // Extract L (lower triangular with unit diagonal)
    let mut l = vec![0.0; n * n];
    for i in 0..n {
        l[i * n + i] = 1.0; // unit diagonal
        for j in 0..i {
            l[i * n + j] = factor.lu[i * n + j];
        }
    }

    // Extract U (upper triangular including diagonal)
    let mut u = vec![0.0; n * n];
    for i in 0..n {
        for j in i..n {
            u[i * n + j] = factor.lu[i * n + j];
        }
    }

    LuResult { p, l, u, n }
}

/// Solve A * X = B using a pre-computed LU factorization.
///
/// `b` is a column vector of length n (or an n x m matrix for multiple RHS).
/// Returns the solution vector/matrix X.
pub fn lu_solve_contiguous_f64(
    factor: &LuFactorResult,
    b_data: &[f64],
    b_meta: &TensorMeta,
) -> Result<Vec<f64>, KernelError> {
    ensure_unary_layout_and_storage(b_data, b_meta)?;
    let b_shape = b_meta.shape();
    let n = factor.n;

    // b can be [n] (single RHS) or [n, m] (multiple RHS)
    let (b_rows, num_rhs) = match b_shape.len() {
        1 => (b_shape[0], 1usize),
        2 => (b_shape[0], b_shape[1]),
        _ => {
            return Err(KernelError::ShapeMismatch {
                lhs: vec![n],
                rhs: b_shape.to_vec(),
            });
        }
    };
    if b_rows != n {
        return Err(KernelError::ShapeMismatch {
            lhs: vec![n],
            rhs: vec![b_rows],
        });
    }
    if n == 0 {
        return Ok(Vec::new());
    }

    let offset = b_meta.storage_offset();
    let mut x: Vec<f64> = b_data[offset..offset + n * num_rhs].to_vec();

    // Apply pivot permutation to each RHS column.
    // pivots[i] = original row at LU position i, so we need:
    // permuted[i] = b[pivots[i]] (reorder b to match LU row order)
    for rhs in 0..num_rhs {
        let mut permuted = vec![0.0; n];
        for i in 0..n {
            permuted[i] = x[factor.pivots[i] * num_rhs + rhs];
        }
        for i in 0..n {
            x[i * num_rhs + rhs] = permuted[i];
        }
    }

    // Forward substitution: L * y = P * b
    for k in 0..n {
        for i in (k + 1)..n {
            let l_ik = factor.lu[i * n + k];
            for rhs in 0..num_rhs {
                x[i * num_rhs + rhs] -= l_ik * x[k * num_rhs + rhs];
            }
        }
    }

    // Back substitution: U * x = y
    for k in (0..n).rev() {
        let diag = factor.lu[k * n + k];
        if diag.abs() < f64::EPSILON * 1e3 {
            // Singular or near-singular — set solution to 0 for this row
            for rhs in 0..num_rhs {
                x[k * num_rhs + rhs] = 0.0;
            }
            continue;
        }
        for rhs in 0..num_rhs {
            x[k * num_rhs + rhs] /= diag;
        }
        for i in 0..k {
            let u_ik = factor.lu[i * n + k];
            for rhs in 0..num_rhs {
                x[i * num_rhs + rhs] -= u_ik * x[k * num_rhs + rhs];
            }
        }
    }

    Ok(x)
}

/// Result of QR decomposition.
#[derive(Debug, Clone)]
pub struct QrResult {
    /// Orthogonal matrix Q in row-major order.
    pub q: Vec<f64>,
    /// Upper triangular matrix R in row-major order.
    pub r: Vec<f64>,
    /// Number of rows (m).
    pub m: usize,
    /// Number of columns (n).
    pub n: usize,
}

/// Compute the QR decomposition of an (m x n) matrix via Householder reflections.
///
/// Returns `(Q, R)` such that `A = Q @ R`:
/// - Q: orthogonal matrix (m x m) for `reduced=false`, or (m x k) for `reduced=true` where k=min(m,n)
/// - R: upper triangular matrix (m x n) for `reduced=false`, or (k x n) for `reduced=true`
///
/// The matrix must be 2-D.
pub fn qr_contiguous_f64(
    data: &[f64],
    meta: &TensorMeta,
    reduced: bool,
) -> Result<QrResult, KernelError> {
    ensure_unary_layout_and_storage(data, meta)?;
    let shape = meta.shape();
    if shape.len() != 2 {
        return Err(KernelError::ShapeMismatch {
            lhs: shape.to_vec(),
            rhs: vec![shape.len()],
        });
    }
    let m = shape[0];
    let n = shape[1];

    if m == 0 || n == 0 {
        let k = if reduced { m.min(n) } else { m };
        return Ok(QrResult {
            q: vec![0.0; m * k],
            r: vec![0.0; k * n],
            m: k,
            n,
        });
    }

    let k = m.min(n);
    let offset = meta.storage_offset();

    // Copy A into working matrix R (m x n, row-major)
    let mut r_mat = data[offset..offset + m * n].to_vec();

    // Build Q as product of Householder reflections, starting as identity (m x m)
    let mut q_mat = vec![0.0; m * m];
    for i in 0..m {
        q_mat[i * m + i] = 1.0;
    }

    for j in 0..k {
        // Extract the column vector below the diagonal: v = R[j:m, j]
        let col_len = m - j;
        let mut v = vec![0.0; col_len];
        for i in 0..col_len {
            v[i] = r_mat[(i + j) * n + j];
        }

        // Compute the Householder reflection: v[0] += sign(v[0]) * ||v||
        let norm_v: f64 = v.iter().map(|x| x * x).sum::<f64>().sqrt();
        if norm_v < f64::EPSILON * 1e6 {
            continue; // Skip near-zero column
        }
        let sign = if v[0] >= 0.0 { 1.0 } else { -1.0 };
        v[0] += sign * norm_v;

        // Normalize v
        let norm_v2: f64 = v.iter().map(|x| x * x).sum();
        if norm_v2 < f64::EPSILON * 1e6 {
            continue;
        }
        let inv_norm = 1.0 / norm_v2;

        // Apply Householder to R: R[j:m, :] -= 2 * v * (v^T @ R[j:m, :])
        for col in 0..n {
            let mut dot = 0.0;
            for i in 0..col_len {
                dot += v[i] * r_mat[(i + j) * n + col];
            }
            let factor = 2.0 * dot * inv_norm;
            for i in 0..col_len {
                r_mat[(i + j) * n + col] -= factor * v[i];
            }
        }

        // Apply Householder to Q: Q[:, j:m] -= 2 * Q[:, j:m] @ v * v^T
        for row in 0..m {
            let mut dot = 0.0;
            for i in 0..col_len {
                dot += q_mat[row * m + (i + j)] * v[i];
            }
            let factor = 2.0 * dot * inv_norm;
            for i in 0..col_len {
                q_mat[row * m + (i + j)] -= factor * v[i];
            }
        }
    }

    // Clean up R: zero out below-diagonal elements
    for i in 0..m {
        for j in 0..i.min(n) {
            r_mat[i * n + j] = 0.0;
        }
    }

    if reduced {
        // Q is m x k (first k columns of full Q)
        let mut q_reduced = vec![0.0; m * k];
        for i in 0..m {
            for j in 0..k {
                q_reduced[i * k + j] = q_mat[i * m + j];
            }
        }
        // R is k x n (first k rows of full R)
        let r_reduced = r_mat[..k * n].to_vec();
        Ok(QrResult {
            q: q_reduced,
            r: r_reduced,
            m: k,
            n,
        })
    } else {
        // Q is m x m, R is m x n
        Ok(QrResult {
            q: q_mat,
            r: r_mat,
            m,
            n,
        })
    }
}

// =========================================================================
// f32 kernel functions (bd-2do9.3)
// =========================================================================

fn ensure_storage_len_f32(
    buffer: &[f32],
    meta: &TensorMeta,
    side: &'static str,
) -> Result<(), KernelError> {
    let needed = contiguous_required_len(meta, side)?;
    if buffer.len() < needed {
        return Err(KernelError::InsufficientStorage {
            side,
            needed,
            available: buffer.len(),
        });
    }
    Ok(())
}

fn ensure_unary_layout_and_storage_f32(
    buffer: &[f32],
    meta: &TensorMeta,
) -> Result<(), KernelError> {
    if !meta.is_contiguous() {
        return Err(KernelError::UnsupportedLayout { side: "input" });
    }
    ensure_storage_len_f32(buffer, meta, "input")
}

fn unary_contiguous_f32<F>(
    input: &[f32],
    meta: &TensorMeta,
    op: F,
) -> Result<Vec<f32>, KernelError>
where
    F: Fn(f32) -> f32,
{
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let numel = meta.numel();
    if numel == 0 {
        return Ok(Vec::new());
    }
    let start = meta.storage_offset();
    let window = &input[start..start + numel];
    Ok(window.iter().map(|value| op(*value)).collect())
}

fn elementwise_contiguous_f32<F>(
    lhs: &[f32],
    rhs: &[f32],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
    op: F,
) -> Result<Vec<f32>, KernelError>
where
    F: Fn(f32, f32) -> f32,
{
    ensure_meta_compatible(lhs_meta, rhs_meta)?;
    ensure_storage_len_f32(lhs, lhs_meta, "lhs")?;
    ensure_storage_len_f32(rhs, rhs_meta, "rhs")?;
    let numel = lhs_meta.numel();
    if numel == 0 {
        return Ok(Vec::new());
    }
    let lhs_start = lhs_meta.storage_offset();
    let rhs_start = rhs_meta.storage_offset();
    let lhs_window = &lhs[lhs_start..lhs_start + numel];
    let rhs_window = &rhs[rhs_start..rhs_start + numel];
    Ok(lhs_window
        .iter()
        .zip(rhs_window.iter())
        .map(|(left, right)| op(*left, *right))
        .collect())
}

fn ensure_dtype_device_and_layout_f32(
    lhs: &TensorMeta,
    rhs: &TensorMeta,
) -> Result<(), KernelError> {
    if lhs.dtype() != rhs.dtype() {
        return Err(KernelError::Incompatible(
            TensorCompatError::DTypeMismatch {
                lhs: lhs.dtype(),
                rhs: rhs.dtype(),
            },
        ));
    }
    if lhs.device() != rhs.device() {
        return Err(KernelError::Incompatible(
            TensorCompatError::DeviceMismatch {
                lhs: lhs.device(),
                rhs: rhs.device(),
            },
        ));
    }
    if !lhs.is_contiguous() {
        return Err(KernelError::UnsupportedLayout { side: "lhs" });
    }
    if !rhs.is_contiguous() {
        return Err(KernelError::UnsupportedLayout { side: "rhs" });
    }
    Ok(())
}

// f32 activation helpers

fn gelu_value_f32(x: f32) -> f32 {
    let c = std::f32::consts::FRAC_2_SQRT_PI * std::f32::consts::FRAC_1_SQRT_2;
    let k = c * (x + 0.044715f32 * x * x * x);
    0.5f32 * x * (1.0f32 + k.tanh())
}

fn silu_value_f32(x: f32) -> f32 {
    x / (1.0f32 + (-x).exp())
}

fn leaky_relu_value_f32(x: f32) -> f32 {
    if x >= 0.0f32 { x } else { 0.01f32 * x }
}

fn elu_value_f32(x: f32) -> f32 {
    if x > 0.0f32 { x } else { x.exp() - 1.0f32 }
}

fn erf_value_f32(x: f32) -> f32 {
    let a1 = 0.254829592f32;
    let a2 = -0.284496736f32;
    let a3 = 1.421413741f32;
    let a4 = -1.453152027f32;
    let a5 = 1.061405429f32;
    let p = 0.3275911f32;
    let sign = if x < 0.0f32 { -1.0f32 } else { 1.0f32 };
    let abs_x = x.abs();
    let t = 1.0f32 / (1.0f32 + p * abs_x);
    let y = 1.0f32
        - (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t * (-abs_x * abs_x).exp();
    sign * y
}

fn hardswish_value_f32(x: f32) -> f32 {
    if x <= -3.0f32 {
        0.0f32
    } else if x >= 3.0f32 {
        x
    } else {
        x * (x + 3.0f32) / 6.0f32
    }
}

fn hardsigmoid_value_f32(x: f32) -> f32 {
    if x <= -3.0f32 {
        0.0f32
    } else if x >= 3.0f32 {
        1.0f32
    } else {
        (x + 3.0f32) / 6.0f32
    }
}

fn hardtanh_value_f32(x: f32) -> f32 {
    x.clamp(-1.0f32, 1.0f32)
}

fn softplus_value_f32(x: f32) -> f32 {
    if x > 20.0f32 {
        x
    } else if x < -20.0f32 {
        0.0f32
    } else {
        (1.0f32 + x.exp()).ln()
    }
}

fn mish_value_f32(x: f32) -> f32 {
    x * softplus_value_f32(x).tanh()
}

// ── Macro-generated simple f32 unary kernels ────────────────────────────

macro_rules! define_unary_f32 {
    ($name:ident, $op:expr) => {
        pub fn $name(input: &[f32], meta: &TensorMeta) -> Result<Vec<f32>, KernelError> {
            unary_contiguous_f32(input, meta, $op)
        }
    };
}

define_unary_f32!(neg_tensor_contiguous_f32, |v: f32| -v);
define_unary_f32!(abs_tensor_contiguous_f32, f32::abs);
define_unary_f32!(exp_tensor_contiguous_f32, f32::exp);
define_unary_f32!(log_tensor_contiguous_f32, f32::ln);
define_unary_f32!(relu_tensor_contiguous_f32, |v: f32| if v > 0.0f32 {
    v
} else {
    0.0f32
});
define_unary_f32!(sigmoid_tensor_contiguous_f32, |v: f32| 1.0f32
    / (1.0f32 + (-v).exp()));
define_unary_f32!(tanh_tensor_contiguous_f32, f32::tanh);
define_unary_f32!(sqrt_tensor_contiguous_f32, f32::sqrt);
define_unary_f32!(reciprocal_tensor_contiguous_f32, |v: f32| 1.0f32 / v);
define_unary_f32!(sin_tensor_contiguous_f32, f32::sin);
define_unary_f32!(cos_tensor_contiguous_f32, f32::cos);
define_unary_f32!(tan_tensor_contiguous_f32, f32::tan);
define_unary_f32!(floor_tensor_contiguous_f32, f32::floor);
define_unary_f32!(ceil_tensor_contiguous_f32, f32::ceil);
define_unary_f32!(round_tensor_contiguous_f32, f32::round);
define_unary_f32!(log2_tensor_contiguous_f32, f32::log2);
define_unary_f32!(log10_tensor_contiguous_f32, f32::log10);
define_unary_f32!(log1p_tensor_contiguous_f32, f32::ln_1p);
define_unary_f32!(expm1_tensor_contiguous_f32, f32::exp_m1);
define_unary_f32!(sign_tensor_contiguous_f32, f32::signum);
define_unary_f32!(trunc_tensor_contiguous_f32, f32::trunc);
define_unary_f32!(frac_tensor_contiguous_f32, f32::fract);
define_unary_f32!(asin_tensor_contiguous_f32, f32::asin);
define_unary_f32!(acos_tensor_contiguous_f32, f32::acos);
define_unary_f32!(atan_tensor_contiguous_f32, f32::atan);
define_unary_f32!(sinh_tensor_contiguous_f32, f32::sinh);
define_unary_f32!(cosh_tensor_contiguous_f32, f32::cosh);
define_unary_f32!(gelu_tensor_contiguous_f32, gelu_value_f32);
define_unary_f32!(silu_tensor_contiguous_f32, silu_value_f32);
define_unary_f32!(leaky_relu_tensor_contiguous_f32, leaky_relu_value_f32);
define_unary_f32!(elu_tensor_contiguous_f32, elu_value_f32);
define_unary_f32!(rsqrt_tensor_contiguous_f32, |v: f32| 1.0f32 / v.sqrt());
define_unary_f32!(erf_tensor_contiguous_f32, erf_value_f32);
define_unary_f32!(erfc_tensor_contiguous_f32, |v: f32| 1.0f32
    - erf_value_f32(v));
define_unary_f32!(hardswish_tensor_contiguous_f32, hardswish_value_f32);
define_unary_f32!(hardsigmoid_tensor_contiguous_f32, hardsigmoid_value_f32);
define_unary_f32!(hardtanh_tensor_contiguous_f32, hardtanh_value_f32);
define_unary_f32!(softplus_tensor_contiguous_f32, softplus_value_f32);
define_unary_f32!(mish_tensor_contiguous_f32, mish_value_f32);
define_unary_f32!(square_tensor_contiguous_f32, |v: f32| v * v);
define_unary_f32!(isnan_tensor_contiguous_f32, |v: f32| if v.is_nan() {
    1.0f32
} else {
    0.0f32
});
define_unary_f32!(isinf_tensor_contiguous_f32, |v: f32| if v.is_infinite() {
    1.0f32
} else {
    0.0f32
});
define_unary_f32!(isfinite_tensor_contiguous_f32, |v: f32| if v.is_finite() {
    1.0f32
} else {
    0.0f32
});

// ── Macro-generated simple f32 binary kernels ───────────────────────────

macro_rules! define_binary_f32 {
    ($name:ident, $op:expr) => {
        pub fn $name(
            lhs: &[f32],
            rhs: &[f32],
            lhs_meta: &TensorMeta,
            rhs_meta: &TensorMeta,
        ) -> Result<Vec<f32>, KernelError> {
            elementwise_contiguous_f32(lhs, rhs, lhs_meta, rhs_meta, $op)
        }
    };
}

define_binary_f32!(add_tensor_contiguous_f32, |l: f32, r: f32| l + r);
define_binary_f32!(sub_tensor_contiguous_f32, |l: f32, r: f32| l - r);
define_binary_f32!(mul_tensor_contiguous_f32, |l: f32, r: f32| l * r);
define_binary_f32!(div_tensor_contiguous_f32, |l: f32, r: f32| l / r);
define_binary_f32!(min_tensor_contiguous_f32, |l: f32, r: f32| {
    if l.is_nan() || r.is_nan() {
        f32::NAN
    } else {
        l.min(r)
    }
});
define_binary_f32!(max_tensor_contiguous_f32, |l: f32, r: f32| {
    if l.is_nan() || r.is_nan() {
        f32::NAN
    } else {
        l.max(r)
    }
});
define_binary_f32!(atan2_tensor_contiguous_f32, |y: f32, x: f32| y.atan2(x));
define_binary_f32!(fmod_tensor_contiguous_f32, |a: f32, b: f32| a % b);
define_binary_f32!(remainder_tensor_contiguous_f32, |a: f32, b: f32| a
    - (a / b).floor() * b);

// ── Comparison ops f32 ──────────────────────────────────────────────────

define_binary_f32!(eq_tensor_contiguous_f32, |l: f32, r: f32| if l == r {
    1.0f32
} else {
    0.0f32
});
define_binary_f32!(ne_tensor_contiguous_f32, |l: f32, r: f32| if l != r {
    1.0f32
} else {
    0.0f32
});
define_binary_f32!(lt_tensor_contiguous_f32, |l: f32, r: f32| if l < r {
    1.0f32
} else {
    0.0f32
});
define_binary_f32!(gt_tensor_contiguous_f32, |l: f32, r: f32| if l > r {
    1.0f32
} else {
    0.0f32
});
define_binary_f32!(le_tensor_contiguous_f32, |l: f32, r: f32| if l <= r {
    1.0f32
} else {
    0.0f32
});
define_binary_f32!(ge_tensor_contiguous_f32, |l: f32, r: f32| if l >= r {
    1.0f32
} else {
    0.0f32
});

// ── Hand-written complex f32 kernels ────────────────────────────────────

pub fn pow_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    exponent: f32,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let numel = meta.numel();
    if numel == 0 {
        return Ok(Vec::new());
    }
    let start = meta.storage_offset();
    let window = &input[start..start + numel];
    Ok(window.iter().map(|value| value.powf(exponent)).collect())
}

pub fn clamp_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    min_val: f32,
    max_val: f32,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let numel = meta.numel();
    if numel == 0 {
        return Ok(Vec::new());
    }
    let start = meta.storage_offset();
    let window = &input[start..start + numel];
    Ok(window
        .iter()
        .map(|value| {
            if value.is_nan() {
                f32::NAN
            } else if !min_val.is_nan() && *value < min_val {
                min_val
            } else if !max_val.is_nan() && *value > max_val {
                max_val
            } else {
                *value
            }
        })
        .collect())
}

pub fn sum_tensor_contiguous_f32(input: &[f32], meta: &TensorMeta) -> Result<f32, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let offset = meta.storage_offset();
    let numel = meta.numel();
    Ok(input[offset..offset + numel].iter().sum())
}

pub fn mean_tensor_contiguous_f32(input: &[f32], meta: &TensorMeta) -> Result<f32, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let offset = meta.storage_offset();
    let numel = meta.numel();
    if numel == 0 {
        return Ok(f32::NAN);
    }
    let sum: f32 = input[offset..offset + numel].iter().sum();
    Ok(sum / numel as f32)
}

pub fn sum_dim_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "sum_dim_f32 shape volume overflow")?;
    let out_numel = checked_mul(outer_size, inner_size, "sum_dim_f32 overflow")?;
    let mut output = vec![0.0f32; out_numel];
    let data = &input[offset..];
    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut sum = 0.0f32;
            for r in 0..reduce_size {
                sum += data[outer * reduce_size * inner_size + r * inner_size + inner];
            }
            output[outer * inner_size + inner] = sum;
        }
    }
    Ok(output)
}

pub fn mean_dim_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "mean_dim_f32 shape volume overflow")?;
    if reduce_size == 0 {
        let out_numel = checked_mul(outer_size, inner_size, "mean_dim_f32 overflow")?;
        return Ok(vec![f32::NAN; out_numel]);
    }
    let mut output = sum_dim_tensor_contiguous_f32(input, meta, dim)?;
    let scale = 1.0f32 / reduce_size as f32;
    for v in &mut output {
        *v *= scale;
    }
    Ok(output)
}

pub fn matmul_tensor_contiguous_f32(
    lhs: &[f32],
    rhs: &[f32],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f32>, KernelError> {
    ensure_dtype_device_and_layout_f32(lhs_meta, rhs_meta)?;
    let (m, k, n) = matmul_dims(lhs_meta, rhs_meta)?;
    checked_mul(m, k, "matmul_f32 lhs overflow")?;
    checked_mul(k, n, "matmul_f32 rhs overflow")?;
    let out_numel = checked_mul(m, n, "matmul_f32 output overflow")?;
    ensure_storage_len_f32(lhs, lhs_meta, "lhs")?;
    ensure_storage_len_f32(rhs, rhs_meta, "rhs")?;
    let lhs_start = lhs_meta.storage_offset();
    let rhs_start = rhs_meta.storage_offset();
    let mut out = vec![0.0f32; out_numel];
    for row in 0..m {
        let out_row_base = row * n;
        let lhs_row_base = lhs_start + row * k;
        for col in 0..n {
            let mut acc = 0.0f32;
            for inner in 0..k {
                acc += lhs[lhs_row_base + inner] * rhs[rhs_start + inner * n + col];
            }
            out[out_row_base + col] = acc;
        }
    }
    Ok(out)
}

pub fn dot_tensor_contiguous_f32(
    lhs: &[f32],
    rhs: &[f32],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<f32, KernelError> {
    ensure_dtype_device_and_layout_f32(lhs_meta, rhs_meta)?;
    if lhs_meta.shape().len() != 1 || rhs_meta.shape().len() != 1 {
        return Err(KernelError::ShapeMismatch {
            lhs: lhs_meta.shape().to_vec(),
            rhs: rhs_meta.shape().to_vec(),
        });
    }
    if lhs_meta.shape()[0] != rhs_meta.shape()[0] {
        return Err(KernelError::ShapeMismatch {
            lhs: lhs_meta.shape().to_vec(),
            rhs: rhs_meta.shape().to_vec(),
        });
    }
    ensure_storage_len_f32(lhs, lhs_meta, "lhs")?;
    ensure_storage_len_f32(rhs, rhs_meta, "rhs")?;
    let n = lhs_meta.shape()[0];
    let lhs_start = lhs_meta.storage_offset();
    let rhs_start = rhs_meta.storage_offset();
    let mut acc = 0.0f32;
    for i in 0..n {
        acc += lhs[lhs_start + i] * rhs[rhs_start + i];
    }
    Ok(acc)
}

pub fn outer_tensor_contiguous_f32(
    lhs: &[f32],
    rhs: &[f32],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f32>, KernelError> {
    ensure_dtype_device_and_layout_f32(lhs_meta, rhs_meta)?;
    if lhs_meta.shape().len() != 1 || rhs_meta.shape().len() != 1 {
        return Err(KernelError::ShapeMismatch {
            lhs: lhs_meta.shape().to_vec(),
            rhs: rhs_meta.shape().to_vec(),
        });
    }
    let m = lhs_meta.shape()[0];
    let n = rhs_meta.shape()[0];
    let out_numel = checked_mul(m, n, "outer_f32 output overflow")?;
    ensure_storage_len_f32(lhs, lhs_meta, "lhs")?;
    ensure_storage_len_f32(rhs, rhs_meta, "rhs")?;
    let lhs_start = lhs_meta.storage_offset();
    let rhs_start = rhs_meta.storage_offset();
    let mut out = vec![0.0f32; out_numel];
    for i in 0..m {
        for j in 0..n {
            out[i * n + j] = lhs[lhs_start + i] * rhs[rhs_start + j];
        }
    }
    Ok(out)
}

pub fn bmm_tensor_contiguous_f32(
    lhs: &[f32],
    rhs: &[f32],
    lhs_meta: &TensorMeta,
    rhs_meta: &TensorMeta,
) -> Result<Vec<f32>, KernelError> {
    ensure_dtype_device_and_layout_f32(lhs_meta, rhs_meta)?;
    if lhs_meta.shape().len() != 3 || rhs_meta.shape().len() != 3 {
        return Err(KernelError::ShapeMismatch {
            lhs: lhs_meta.shape().to_vec(),
            rhs: rhs_meta.shape().to_vec(),
        });
    }
    let batch = lhs_meta.shape()[0];
    let m = lhs_meta.shape()[1];
    let k = lhs_meta.shape()[2];
    let rhs_batch = rhs_meta.shape()[0];
    let rhs_k = rhs_meta.shape()[1];
    let n = rhs_meta.shape()[2];
    if batch != rhs_batch || k != rhs_k {
        return Err(KernelError::ShapeMismatch {
            lhs: lhs_meta.shape().to_vec(),
            rhs: rhs_meta.shape().to_vec(),
        });
    }
    let lhs_batch_stride = checked_mul(m, k, "bmm_f32 lhs batch stride overflow")?;
    let rhs_batch_stride = checked_mul(k, n, "bmm_f32 rhs batch stride overflow")?;
    let out_batch_stride = checked_mul(m, n, "bmm_f32 output batch stride overflow")?;
    checked_mul(batch, lhs_batch_stride, "bmm_f32 lhs overflow")?;
    checked_mul(batch, rhs_batch_stride, "bmm_f32 rhs overflow")?;
    let out_numel = checked_mul(batch, out_batch_stride, "bmm_f32 output overflow")?;
    ensure_storage_len_f32(lhs, lhs_meta, "lhs")?;
    ensure_storage_len_f32(rhs, rhs_meta, "rhs")?;
    let lhs_start = lhs_meta.storage_offset();
    let rhs_start = rhs_meta.storage_offset();
    let mut out = vec![0.0f32; out_numel];
    for b in 0..batch {
        let lhs_base = lhs_start + b * lhs_batch_stride;
        let rhs_base = rhs_start + b * rhs_batch_stride;
        let out_base = b * out_batch_stride;
        for row in 0..m {
            for col in 0..n {
                let mut acc = 0.0f32;
                for inner in 0..k {
                    acc += lhs[lhs_base + row * k + inner] * rhs[rhs_base + inner * n + col];
                }
                out[out_base + row * n + col] = acc;
            }
        }
    }
    Ok(out)
}

pub fn trace_tensor_contiguous_f32(input: &[f32], meta: &TensorMeta) -> Result<f32, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    if meta.shape().len() != 2 {
        return Err(KernelError::InvalidDimension {
            dim: meta.shape().len(),
            ndim: 2,
        });
    }
    let rows = meta.shape()[0];
    let cols = meta.shape()[1];
    let diag_len = rows.min(cols);
    let offset = meta.storage_offset();
    let mut acc = 0.0f32;
    for i in 0..diag_len {
        acc += input[offset + i * cols + i];
    }
    Ok(acc)
}

pub fn prod_dim_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "prod_dim_f32 overflow")?;
    let out_numel = checked_mul(outer_size, inner_size, "prod_dim_f32 overflow")?;
    let mut output = vec![1.0f32; out_numel];
    let data = &input[offset..];
    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut prod = 1.0f32;
            for r in 0..reduce_size {
                prod *= data[outer * reduce_size * inner_size + r * inner_size + inner];
            }
            output[outer * inner_size + inner] = prod;
        }
    }
    Ok(output)
}

pub fn var_dim_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "var_dim_f32 overflow")?;
    let out_numel = checked_mul(outer_size, inner_size, "var_dim_f32 overflow")?;
    let data = &input[offset..];
    if reduce_size < 2 {
        return Ok(vec![f32::NAN; out_numel]);
    }
    let mut output = vec![0.0f32; out_numel];
    let correction = (reduce_size - 1) as f32;
    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut sum = 0.0f32;
            for r in 0..reduce_size {
                sum += data[outer * reduce_size * inner_size + r * inner_size + inner];
            }
            let mean = sum / reduce_size as f32;
            let mut var_sum = 0.0f32;
            for r in 0..reduce_size {
                let diff = data[outer * reduce_size * inner_size + r * inner_size + inner] - mean;
                var_sum += diff * diff;
            }
            output[outer * inner_size + inner] = var_sum / correction;
        }
    }
    Ok(output)
}

pub fn std_dim_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f32>, KernelError> {
    let mut output = var_dim_tensor_contiguous_f32(input, meta, dim)?;
    for v in &mut output {
        *v = v.sqrt();
    }
    Ok(output)
}

pub fn norm_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    p: f32,
) -> Result<f32, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let offset = meta.storage_offset();
    let numel = meta.numel();
    if numel == 0 {
        return Ok(0.0f32);
    }
    let data = &input[offset..offset + numel];
    if p == f32::INFINITY {
        Ok(data.iter().fold(0.0f32, |acc, &x| acc.max(x.abs())))
    } else if p == f32::NEG_INFINITY {
        Ok(data
            .iter()
            .fold(f32::INFINITY, |acc, &x| acc.min(x.abs())))
    } else if p == 0.0f32 {
        Ok(data.iter().filter(|&&x| x != 0.0f32).count() as f32)
    } else if p == 1.0f32 {
        Ok(data.iter().map(|x| x.abs()).sum())
    } else if p == 2.0f32 {
        let sum_sq: f32 = data.iter().map(|x| x * x).sum();
        Ok(sum_sq.sqrt())
    } else {
        let sum_pow: f32 = data.iter().map(|x| x.abs().powf(p)).sum();
        Ok(sum_pow.powf(1.0f32 / p))
    }
}

pub fn norm_dim_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    p: f32,
    dim: usize,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "norm_dim_f32 overflow")?;
    let out_numel = checked_mul(outer_size, inner_size, "norm_dim_f32 overflow")?;
    let data = &input[offset..];
    let mut output = vec![0.0f32; out_numel];

    if p == f32::INFINITY {
        for outer in 0..outer_size {
            for inner in 0..inner_size {
                let mut max_abs = 0.0f32;
                for r in 0..reduce_size {
                    max_abs = max_abs
                        .max(data[outer * reduce_size * inner_size + r * inner_size + inner].abs());
                }
                output[outer * inner_size + inner] = max_abs;
            }
        }
    } else if p == f32::NEG_INFINITY {
        for outer in 0..outer_size {
            for inner in 0..inner_size {
                let mut min_abs = f32::INFINITY;
                for r in 0..reduce_size {
                    min_abs = min_abs
                        .min(data[outer * reduce_size * inner_size + r * inner_size + inner].abs());
                }
                output[outer * inner_size + inner] = min_abs;
            }
        }
    } else if p == 0.0f32 {
        for outer in 0..outer_size {
            for inner in 0..inner_size {
                let mut count = 0.0f32;
                for r in 0..reduce_size {
                    if data[outer * reduce_size * inner_size + r * inner_size + inner] != 0.0f32 {
                        count += 1.0f32;
                    }
                }
                output[outer * inner_size + inner] = count;
            }
        }
    } else if p == 1.0f32 {
        for outer in 0..outer_size {
            for inner in 0..inner_size {
                let mut sum = 0.0f32;
                for r in 0..reduce_size {
                    sum += data[outer * reduce_size * inner_size + r * inner_size + inner].abs();
                }
                output[outer * inner_size + inner] = sum;
            }
        }
    } else if p == 2.0f32 {
        for outer in 0..outer_size {
            for inner in 0..inner_size {
                let mut sum_sq = 0.0f32;
                for r in 0..reduce_size {
                    let v = data[outer * reduce_size * inner_size + r * inner_size + inner];
                    sum_sq += v * v;
                }
                output[outer * inner_size + inner] = sum_sq.sqrt();
            }
        }
    } else {
        for outer in 0..outer_size {
            for inner in 0..inner_size {
                let mut sum_pow = 0.0f32;
                for r in 0..reduce_size {
                    sum_pow += data[outer * reduce_size * inner_size + r * inner_size + inner]
                        .abs()
                        .powf(p);
                }
                output[outer * inner_size + inner] = sum_pow.powf(1.0f32 / p);
            }
        }
    }
    Ok(output)
}

pub fn softmax_dim_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, numel) =
        checked_dim_loop_sizes(shape, dim, "softmax_f32 overflow")?;
    let mut output = vec![0.0f32; numel];
    let data = &input[offset..];
    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut max_val = f32::NEG_INFINITY;
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                if data[idx] > max_val {
                    max_val = data[idx];
                }
            }
            let mut sum = 0.0f32;
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                let e = (data[idx] - max_val).exp();
                output[idx] = e;
                sum += e;
            }
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                output[idx] /= sum;
            }
        }
    }
    Ok(output)
}

pub fn log_softmax_dim_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, numel) =
        checked_dim_loop_sizes(shape, dim, "log_softmax_f32 overflow")?;
    let mut output = vec![0.0f32; numel];
    let data = &input[offset..];
    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut max_val = f32::NEG_INFINITY;
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                if data[idx] > max_val {
                    max_val = data[idx];
                }
            }
            let mut sum_exp = 0.0f32;
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                sum_exp += (data[idx] - max_val).exp();
            }
            let log_sum_exp = max_val + sum_exp.ln();
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                output[idx] = data[idx] - log_sum_exp;
            }
        }
    }
    Ok(output)
}

pub fn argmax_dim_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "argmax_f32 overflow")?;
    let out_numel = checked_mul(outer_size, inner_size, "argmax_f32 overflow")?;
    let mut output = vec![0.0f32; out_numel];
    let data = &input[offset..];
    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut best_idx = 0usize;
            let mut best_val = f32::NEG_INFINITY;
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                let val = data[idx];
                if val.is_nan() {
                    best_idx = r;
                    break;
                } else if val > best_val {
                    best_val = val;
                    best_idx = r;
                }
            }
            output[outer * inner_size + inner] = best_idx as f32;
        }
    }
    Ok(output)
}

pub fn argmin_dim_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "argmin_f32 overflow")?;
    let out_numel = checked_mul(outer_size, inner_size, "argmin_f32 overflow")?;
    let mut output = vec![0.0f32; out_numel];
    let data = &input[offset..];
    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut best_idx = 0usize;
            let mut best_val = f32::INFINITY;
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                let val = data[idx];
                if val.is_nan() {
                    best_idx = r;
                    break;
                } else if val < best_val {
                    best_val = val;
                    best_idx = r;
                }
            }
            output[outer * inner_size + inner] = best_idx as f32;
        }
    }
    Ok(output)
}

pub fn max_dim_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
) -> Result<(Vec<f32>, Vec<f32>), KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "max_dim_f32 overflow")?;
    let out_numel = checked_mul(outer_size, inner_size, "max_dim_f32 overflow")?;
    let mut values = vec![f32::NEG_INFINITY; out_numel];
    let mut indices = vec![0.0f32; out_numel];
    let data = &input[offset..];
    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let out_idx = outer * inner_size + inner;
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                let val = data[idx];
                if val.is_nan() {
                    values[out_idx] = f32::NAN;
                    indices[out_idx] = r as f32;
                    break;
                } else if val > values[out_idx] {
                    values[out_idx] = val;
                    indices[out_idx] = r as f32;
                }
            }
        }
    }
    Ok((values, indices))
}

pub fn min_dim_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
) -> Result<(Vec<f32>, Vec<f32>), KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let reduce_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "min_dim_f32 overflow")?;
    let out_numel = checked_mul(outer_size, inner_size, "min_dim_f32 overflow")?;
    let mut values = vec![f32::INFINITY; out_numel];
    let mut indices = vec![0.0f32; out_numel];
    let data = &input[offset..];
    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let out_idx = outer * inner_size + inner;
            for r in 0..reduce_size {
                let idx = outer * reduce_size * inner_size + r * inner_size + inner;
                let val = data[idx];
                if val.is_nan() {
                    values[out_idx] = f32::NAN;
                    indices[out_idx] = r as f32;
                    break;
                } else if val < values[out_idx] {
                    values[out_idx] = val;
                    indices[out_idx] = r as f32;
                }
            }
        }
    }
    Ok((values, indices))
}

pub fn cat_tensor_contiguous_f32(
    inputs: &[(&[f32], &TensorMeta)],
    dim: usize,
) -> Result<Vec<f32>, KernelError> {
    if inputs.is_empty() {
        return Err(KernelError::ShapeMismatch {
            lhs: vec![],
            rhs: vec![],
        });
    }
    let first_shape = inputs[0].1.shape();
    let ndim = first_shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    for (data, meta) in inputs {
        ensure_unary_layout_and_storage_f32(data, meta)?;
        let shape = meta.shape();
        if shape.len() != ndim {
            return Err(KernelError::ShapeMismatch {
                lhs: first_shape.to_vec(),
                rhs: shape.to_vec(),
            });
        }
        for d in 0..ndim {
            if d != dim && shape[d] != first_shape[d] {
                return Err(KernelError::ShapeMismatch {
                    lhs: first_shape.to_vec(),
                    rhs: shape.to_vec(),
                });
            }
        }
    }
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(first_shape, dim, "cat_f32 overflow")?;
    let total_cat_size: usize = inputs.iter().map(|(_, m)| m.shape()[dim]).sum();
    let out_numel = checked_mul(
        checked_mul(outer_size, total_cat_size, "cat_f32 overflow")?,
        inner_size,
        "cat_f32 overflow",
    )?;
    let mut output = Vec::with_capacity(out_numel);
    for outer in 0..outer_size {
        for (data, meta) in inputs {
            let cat_size = meta.shape()[dim];
            let offset = meta.storage_offset();
            let d = &data[offset..];
            for r in 0..cat_size {
                for inner in 0..inner_size {
                    output.push(d[outer * cat_size * inner_size + r * inner_size + inner]);
                }
            }
        }
    }
    Ok(output)
}

pub fn stack_tensor_contiguous_f32(
    inputs: &[(&[f32], &TensorMeta)],
    dim: usize,
) -> Result<Vec<f32>, KernelError> {
    if inputs.is_empty() {
        return Err(KernelError::ShapeMismatch {
            lhs: vec![],
            rhs: vec![],
        });
    }
    let first_shape = inputs[0].1.shape();
    let ndim = first_shape.len();
    if dim > ndim {
        return Err(KernelError::InvalidDimension {
            dim,
            ndim: ndim + 1,
        });
    }
    for (data, meta) in inputs {
        ensure_unary_layout_and_storage_f32(data, meta)?;
        if meta.shape() != first_shape {
            return Err(KernelError::ShapeMismatch {
                lhs: first_shape.to_vec(),
                rhs: meta.shape().to_vec(),
            });
        }
    }
    let num_inputs = inputs.len();
    let outer_size = checked_shape_numel(&first_shape[..dim], "stack_f32 overflow")?;
    let inner_size = checked_shape_numel(&first_shape[dim..], "stack_f32 overflow")?;
    let out_numel = checked_mul(
        checked_mul(outer_size, num_inputs, "stack_f32 overflow")?,
        inner_size,
        "stack_f32 overflow",
    )?;
    let mut output = Vec::with_capacity(out_numel);
    for outer in 0..outer_size {
        for (data, meta) in inputs {
            let offset = meta.storage_offset();
            let d = &data[offset..];
            for inner in 0..inner_size {
                output.push(d[outer * inner_size + inner]);
            }
        }
    }
    Ok(output)
}

pub fn narrow_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
    start: usize,
    length: usize,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let end = start
        .checked_add(length)
        .ok_or(KernelError::InvalidDimension {
            dim: start,
            ndim: shape[dim],
        })?;
    if end > shape[dim] {
        return Err(KernelError::InvalidDimension {
            dim: end,
            ndim: shape[dim],
        });
    }
    if length == 0 {
        return Ok(Vec::new());
    }
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "narrow_f32 overflow")?;
    let dim_size = shape[dim];
    let out_numel = checked_mul(
        checked_mul(outer_size, length, "narrow_f32 overflow")?,
        inner_size,
        "narrow_f32 overflow",
    )?;
    let offset = meta.storage_offset();
    let data = &input[offset..];
    let mut output = Vec::with_capacity(out_numel);
    for outer in 0..outer_size {
        for r in 0..length {
            for inner in 0..inner_size {
                let idx = outer * dim_size * inner_size + (start + r) * inner_size + inner;
                output.push(data[idx]);
            }
        }
    }
    Ok(output)
}

pub fn expand_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    target_shape: &[usize],
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if target_shape.len() != ndim {
        return Err(KernelError::ShapeMismatch {
            lhs: shape.to_vec(),
            rhs: target_shape.to_vec(),
        });
    }
    for d in 0..ndim {
        if shape[d] != target_shape[d] && shape[d] != 1 {
            return Err(KernelError::ShapeMismatch {
                lhs: shape.to_vec(),
                rhs: target_shape.to_vec(),
            });
        }
    }
    let out_numel = checked_shape_numel(target_shape, "expand_f32 overflow")?;
    if out_numel == 0 {
        return Ok(Vec::new());
    }
    let input_strides = broadcast_strides(shape, target_shape, "expand_f32 strides overflow")?;
    let offset = meta.storage_offset();
    let mut output = Vec::with_capacity(out_numel);
    let mut coords = vec![0usize; ndim];
    for _ in 0..out_numel {
        let mut idx = offset;
        for d in 0..ndim {
            idx += coords[d] * input_strides[d];
        }
        output.push(input[idx]);
        for d in (0..ndim).rev() {
            coords[d] += 1;
            if coords[d] < target_shape[d] {
                break;
            }
            coords[d] = 0;
        }
    }
    Ok(output)
}

pub fn index_select_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
    indices: &[f64],
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let dim_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "index_select_f32 overflow")?;
    let num_indices = indices.len();
    let out_numel = checked_mul(
        checked_mul(outer_size, num_indices, "index_select_f32 overflow")?,
        inner_size,
        "index_select_f32 overflow",
    )?;
    let offset = meta.storage_offset();
    let data = &input[offset..];
    let mut output = Vec::with_capacity(out_numel);
    for outer in 0..outer_size {
        for &idx_f in indices {
            let idx = normalize_wrapped_index_value(idx_f, dim_size)?;
            for inner in 0..inner_size {
                let src = outer * dim_size * inner_size + idx * inner_size + inner;
                output.push(data[src]);
            }
        }
    }
    Ok(output)
}

pub fn gather_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
    index: &[f64],
    index_meta: &TensorMeta,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    ensure_unary_layout_and_storage(index, index_meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let idx_shape = index_meta.shape();
    if idx_shape.len() != ndim {
        return Err(KernelError::ShapeMismatch {
            lhs: shape.to_vec(),
            rhs: idx_shape.to_vec(),
        });
    }
    for d in 0..ndim {
        if d != dim && idx_shape[d] != shape[d] {
            return Err(KernelError::ShapeMismatch {
                lhs: shape.to_vec(),
                rhs: idx_shape.to_vec(),
            });
        }
    }
    let dim_size = shape[dim];
    let idx_dim_size = idx_shape[dim];
    let (outer_size, inner_size, out_numel) =
        checked_dim_loop_sizes(idx_shape, dim, "gather_f32 overflow")?;
    let offset = meta.storage_offset();
    let data = &input[offset..];
    let idx_offset = index_meta.storage_offset();
    let index_data = &index[idx_offset..];
    let mut output = Vec::with_capacity(out_numel);
    for outer in 0..outer_size {
        for r in 0..idx_dim_size {
            for inner in 0..inner_size {
                let idx_pos = outer * idx_dim_size * inner_size + r * inner_size + inner;
                let selected = normalize_wrapped_index_value(index_data[idx_pos], dim_size)?;
                let src = outer * dim_size * inner_size + selected * inner_size + inner;
                output.push(data[src]);
            }
        }
    }
    Ok(output)
}

pub fn scatter_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
    index: &[f64],
    index_meta: &TensorMeta,
    src: &[f32],
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    ensure_unary_layout_and_storage(index, index_meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let idx_shape = index_meta.shape();
    if idx_shape.len() != ndim {
        return Err(KernelError::ShapeMismatch {
            lhs: shape.to_vec(),
            rhs: idx_shape.to_vec(),
        });
    }
    for d in 0..ndim {
        if d != dim && idx_shape[d] != shape[d] {
            return Err(KernelError::ShapeMismatch {
                lhs: shape.to_vec(),
                rhs: idx_shape.to_vec(),
            });
        }
    }
    let src_numel = checked_shape_numel(idx_shape, "scatter_f32 overflow")?;
    if src.len() < src_numel {
        return Err(KernelError::InsufficientStorage {
            side: "src",
            needed: src_numel,
            available: src.len(),
        });
    }
    let dim_size = shape[dim];
    let idx_dim_size = idx_shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(idx_shape, dim, "scatter_f32 overflow")?;
    let offset = meta.storage_offset();
    let numel = meta.numel();
    let mut output = input[offset..offset + numel].to_vec();
    let idx_offset = index_meta.storage_offset();
    let index_data = &index[idx_offset..];
    for outer in 0..outer_size {
        for r in 0..idx_dim_size {
            for inner in 0..inner_size {
                let idx_pos = outer * idx_dim_size * inner_size + r * inner_size + inner;
                let selected = normalize_wrapped_index_value(index_data[idx_pos], dim_size)?;
                let dst = outer * dim_size * inner_size + selected * inner_size + inner;
                output[dst] = src[idx_pos];
            }
        }
    }
    Ok(output)
}

pub fn masked_fill_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    mask: &[f64],
    value: f32,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let numel = meta.numel();
    let offset = meta.storage_offset();
    if mask.len() < offset + numel {
        return Err(KernelError::ShapeMismatch {
            lhs: meta.shape().to_vec(),
            rhs: vec![mask.len()],
        });
    }
    let data = &input[offset..offset + numel];
    let output = data
        .iter()
        .zip(mask[offset..offset + numel].iter())
        .map(|(&d, &m)| if m != 0.0 { value } else { d })
        .collect();
    Ok(output)
}

pub fn where_tensor_contiguous_f32(
    condition: &[f64],
    x: &[f32],
    y: &[f32],
    meta: &TensorMeta,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(x, meta)?;
    let numel = meta.numel();
    let offset = meta.storage_offset();
    if condition.len() < offset + numel {
        return Err(KernelError::ShapeMismatch {
            lhs: meta.shape().to_vec(),
            rhs: vec![condition.len()],
        });
    }
    if y.len() < offset + numel {
        return Err(KernelError::ShapeMismatch {
            lhs: meta.shape().to_vec(),
            rhs: vec![y.len()],
        });
    }
    let cond = &condition[offset..offset + numel];
    let x_data = &x[offset..offset + numel];
    let y_data = &y[offset..offset + numel];
    let output = cond
        .iter()
        .zip(x_data.iter())
        .zip(y_data.iter())
        .map(|((&c, &xv), &yv)| if c != 0.0 { xv } else { yv })
        .collect();
    Ok(output)
}

pub fn cumsum_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let dim_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "cumsum_f32 overflow")?;
    let numel = checked_mul(
        checked_mul(outer_size, dim_size, "cumsum_f32 overflow")?,
        inner_size,
        "cumsum_f32 overflow",
    )?;
    let mut output = vec![0.0f32; numel];
    let data = &input[offset..];
    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut acc = 0.0f32;
            for d in 0..dim_size {
                let idx = outer * dim_size * inner_size + d * inner_size + inner;
                acc += data[idx];
                output[idx] = acc;
            }
        }
    }
    Ok(output)
}

pub fn cumsum_backward_tensor_contiguous_f32(
    grad_output: &[f32],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(grad_output, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let dim_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "cumsum_backward_f32 overflow")?;
    let numel = checked_mul(
        checked_mul(outer_size, dim_size, "cumsum_backward_f32 overflow")?,
        inner_size,
        "cumsum_backward_f32 overflow",
    )?;
    let mut grad_input = vec![0.0f32; numel];
    let data = &grad_output[offset..];
    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut acc = 0.0f32;
            for d in (0..dim_size).rev() {
                let idx = outer * dim_size * inner_size + d * inner_size + inner;
                acc += data[idx];
                grad_input[idx] = acc;
            }
        }
    }
    Ok(grad_input)
}

pub fn cumprod_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let dim_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "cumprod_f32 overflow")?;
    let numel = checked_mul(
        checked_mul(outer_size, dim_size, "cumprod_f32 overflow")?,
        inner_size,
        "cumprod_f32 overflow",
    )?;
    let mut output = vec![0.0f32; numel];
    let data = &input[offset..];
    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut acc = 1.0f32;
            for d in 0..dim_size {
                let idx = outer * dim_size * inner_size + d * inner_size + inner;
                acc *= data[idx];
                output[idx] = acc;
            }
        }
    }
    Ok(output)
}

pub fn cumprod_backward_tensor_contiguous_f32(
    grad_output: &[f32],
    input: &[f32],
    output: &[f32],
    meta: &TensorMeta,
    dim: usize,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let dim_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "cumprod_backward_f32 overflow")?;
    let numel = checked_mul(
        checked_mul(outer_size, dim_size, "cumprod_backward_f32 overflow")?,
        inner_size,
        "cumprod_backward_f32 overflow",
    )?;
    let mut grad_input = vec![0.0f32; numel];
    let in_data = &input[offset..];
    let out_data = &output[offset..];
    let go_data = &grad_output[offset..];
    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut acc = 0.0f32;
            for d in (0..dim_size).rev() {
                let idx = outer * dim_size * inner_size + d * inner_size + inner;
                acc += go_data[idx] * out_data[idx];
                let inp = in_data[idx];
                if inp.abs() > f32::EPSILON {
                    grad_input[idx] = acc / inp;
                } else {
                    let mut sum = 0.0f32;
                    for j in d..dim_size {
                        let j_idx = outer * dim_size * inner_size + j * inner_size + inner;
                        let mut prod = 1.0f32;
                        for kk in d..=j {
                            if kk != d {
                                let k_idx =
                                    outer * dim_size * inner_size + kk * inner_size + inner;
                                prod *= in_data[k_idx];
                            }
                        }
                        if d > 0 {
                            let prev_idx =
                                outer * dim_size * inner_size + (d - 1) * inner_size + inner;
                            prod *= out_data[prev_idx];
                        }
                        sum += go_data[j_idx] * prod;
                    }
                    grad_input[idx] = sum;
                }
            }
        }
    }
    Ok(grad_input)
}

pub fn sort_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    dim: usize,
    descending: bool,
) -> Result<(Vec<f32>, Vec<usize>), KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let offset = meta.storage_offset();
    let dim_size = shape[dim];
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "sort_f32 overflow")?;
    let numel = checked_mul(
        checked_mul(outer_size, dim_size, "sort_f32 overflow")?,
        inner_size,
        "sort_f32 overflow",
    )?;
    let data = &input[offset..];
    let mut sorted_values = vec![0.0f32; numel];
    let mut indices = vec![0usize; numel];
    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut lane: Vec<(usize, f32)> = (0..dim_size)
                .map(|d| {
                    let idx = outer * dim_size * inner_size + d * inner_size + inner;
                    (d, data[idx])
                })
                .collect();
            if descending {
                lane.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
            } else {
                lane.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));
            }
            for (out_d, (orig_d, val)) in lane.into_iter().enumerate() {
                let out_idx = outer * dim_size * inner_size + out_d * inner_size + inner;
                sorted_values[out_idx] = val;
                indices[out_idx] = orig_d;
            }
        }
    }
    Ok((sorted_values, indices))
}

pub fn topk_tensor_contiguous_f32(
    input: &[f32],
    meta: &TensorMeta,
    k: usize,
    dim: usize,
    largest: bool,
    sorted: bool,
) -> Result<(Vec<f32>, Vec<usize>), KernelError> {
    ensure_unary_layout_and_storage_f32(input, meta)?;
    let shape = meta.shape();
    let ndim = shape.len();
    if dim >= ndim {
        return Err(KernelError::InvalidDimension { dim, ndim });
    }
    let dim_size = shape[dim];
    if k > dim_size {
        return Err(KernelError::InvalidDimension {
            dim: k,
            ndim: dim_size,
        });
    }
    let offset = meta.storage_offset();
    let (outer_size, inner_size, _) =
        checked_dim_loop_sizes(shape, dim, "topk_f32 overflow")?;
    let out_numel = checked_mul(
        checked_mul(outer_size, k, "topk_f32 overflow")?,
        inner_size,
        "topk_f32 overflow",
    )?;
    let data = &input[offset..];
    let mut out_values = vec![0.0f32; out_numel];
    let mut out_indices = vec![0usize; out_numel];
    for outer in 0..outer_size {
        for inner in 0..inner_size {
            let mut lane: Vec<(usize, f32)> = (0..dim_size)
                .map(|d| {
                    let idx = outer * dim_size * inner_size + d * inner_size + inner;
                    (d, data[idx])
                })
                .collect();
            if largest {
                lane.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
            } else {
                lane.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));
            }
            let top = &lane[..k];
            let mut selected: Vec<(usize, f32)> = top.to_vec();
            if !sorted {
                selected.sort_by_key(|(orig_idx, _)| *orig_idx);
            }
            for (out_d, (orig_d, val)) in selected.into_iter().enumerate() {
                let out_idx = outer * k * inner_size + out_d * inner_size + inner;
                out_values[out_idx] = val;
                out_indices[out_idx] = orig_d;
            }
        }
    }
    Ok((out_values, out_indices))
}

pub fn lerp_tensor_contiguous_f32(
    start: &[f32],
    end: &[f32],
    weight: f32,
    meta: &TensorMeta,
) -> Result<Vec<f32>, KernelError> {
    ensure_unary_layout_and_storage_f32(start, meta)?;
    let numel = meta.numel();
    let offset = meta.storage_offset();
    if end.len() < offset + numel {
        return Err(KernelError::ShapeMismatch {
            lhs: meta.shape().to_vec(),
            rhs: vec![end.len()],
        });
    }
    let s = &start[offset..offset + numel];
    let e = &end[offset..offset + numel];
    Ok(s.iter()
        .zip(e.iter())
        .map(|(&sv, &ev)| sv + weight * (ev - sv))
        .collect())
}

#[allow(clippy::too_many_arguments)]
pub fn addmm_tensor_contiguous_f32(
    input: &[f32],
    mat1: &[f32],
    mat2: &[f32],
    input_meta: &TensorMeta,
    mat1_meta: &TensorMeta,
    mat2_meta: &TensorMeta,
    beta: f32,
    alpha: f32,
) -> Result<Vec<f32>, KernelError> {
    let (m, k, n) = matmul_dims(mat1_meta, mat2_meta)?;
    let out_numel = checked_mul(m, n, "addmm_f32 output overflow")?;
    ensure_storage_len_f32(mat1, mat1_meta, "mat1")?;
    ensure_storage_len_f32(mat2, mat2_meta, "mat2")?;
    let mat1_start = mat1_meta.storage_offset();
    let mat2_start = mat2_meta.storage_offset();
    let input_offset = input_meta.storage_offset();
    let input_shape = input_meta.shape();
    let input_1d = input_shape.len() == 1 && input_shape[0] == n;
    let input_2d = input_shape.len() == 2 && input_shape[0] == m && input_shape[1] == n;
    if !input_1d && !input_2d {
        return Err(KernelError::ShapeMismatch {
            lhs: input_shape.to_vec(),
            rhs: vec![m, n],
        });
    }
    let mut out = vec![0.0f32; out_numel];
    for row in 0..m {
        for col in 0..n {
            let mut acc = 0.0f32;
            for inner in 0..k {
                acc += mat1[mat1_start + row * k + inner] * mat2[mat2_start + inner * n + col];
            }
            let bias_idx = if input_1d {
                input_offset + col
            } else {
                input_offset + row * n + col
            };
            out[row * n + col] = beta * input[bias_idx] + alpha * acc;
        }
    }
    Ok(out)
}

#[allow(clippy::too_many_arguments)]
pub fn addmv_tensor_contiguous_f32(
    input: &[f32],
    mat: &[f32],
    vec_data: &[f32],
    input_meta: &TensorMeta,
    mat_meta: &TensorMeta,
    vec_meta: &TensorMeta,
    beta: f32,
    alpha: f32,
) -> Result<Vec<f32>, KernelError> {
    let mat_shape = mat_meta.shape();
    let vec_shape = vec_meta.shape();
    if mat_shape.len() != 2 {
        return Err(KernelError::ShapeMismatch {
            lhs: mat_shape.to_vec(),
            rhs: vec![0, 0],
        });
    }
    if vec_shape.len() != 1 {
        return Err(KernelError::ShapeMismatch {
            lhs: vec_shape.to_vec(),
            rhs: vec![mat_shape[1]],
        });
    }
    let m = mat_shape[0];
    let k = mat_shape[1];
    if vec_shape[0] != k {
        return Err(KernelError::ShapeMismatch {
            lhs: vec_shape.to_vec(),
            rhs: vec![k],
        });
    }
    let input_shape = input_meta.shape();
    if input_shape.len() != 1 || input_shape[0] != m {
        return Err(KernelError::ShapeMismatch {
            lhs: input_shape.to_vec(),
            rhs: vec![m],
        });
    }
    ensure_storage_len_f32(mat, mat_meta, "mat")?;
    ensure_storage_len_f32(vec_data, vec_meta, "vec")?;
    ensure_storage_len_f32(input, input_meta, "input")?;
    let mat_start = mat_meta.storage_offset();
    let vec_start = vec_meta.storage_offset();
    let input_start = input_meta.storage_offset();
    let mut out = vec![0.0f32; m];
    for row in 0..m {
        let mut acc = 0.0f32;
        for col in 0..k {
            acc += mat[mat_start + row * k + col] * vec_data[vec_start + col];
        }
        out[row] = beta * input[input_start + row] + alpha * acc;
    }
    Ok(out)
}

pub fn reduce_sum_for_broadcast_f32(
    expanded_grad: &[f32],
    expanded_shape: &[usize],
    original_shape: &[usize],
) -> Result<Vec<f32>, KernelError> {
    let ndim = expanded_shape.len();
    if ndim != original_shape.len() {
        return Err(KernelError::ShapeMismatch {
            lhs: expanded_shape.to_vec(),
            rhs: original_shape.to_vec(),
        });
    }
    for d in 0..ndim {
        let expanded = expanded_shape[d];
        let original = original_shape[d];
        if original != expanded && original != 1 {
            return Err(KernelError::ShapeMismatch {
                lhs: expanded_shape.to_vec(),
                rhs: original_shape.to_vec(),
            });
        }
    }
    let expanded_numel =
        checked_shape_numel(expanded_shape, "broadcast_f32 reduction expanded shape")?;
    if expanded_grad.len() != expanded_numel {
        return Err(KernelError::ShapeMismatch {
            lhs: vec![expanded_grad.len()],
            rhs: vec![expanded_numel],
        });
    }
    let original_numel =
        checked_shape_numel(original_shape, "broadcast_f32 reduction original shape")?;
    if original_numel == 0 {
        return Ok(Vec::new());
    }
    let original_strides =
        broadcast_strides(original_shape, expanded_shape, "broadcast_f32 strides overflow")?;
    let mut reduced = vec![0.0f32; original_numel];
    let mut coords = vec![0usize; ndim];
    for grad in expanded_grad {
        let mut original_idx = 0usize;
        for d in 0..ndim {
            original_idx += coords[d] * original_strides[d];
        }
        reduced[original_idx] += *grad;
        for d in (0..ndim).rev() {
            coords[d] += 1;
            if coords[d] < expanded_shape[d] {
                break;
            }
            coords[d] = 0;
        }
    }
    Ok(reduced)
}

#[cfg(test)]
mod tests {
    use ft_core::{DType, Device, ScalarTensor, TensorCompatError, TensorMeta};

    use super::{
        KernelError, abs_scalar, abs_tensor_contiguous_f64, acos_scalar,
        acos_tensor_contiguous_f64, add_scalar, add_tensor_contiguous_f64,
        argmax_dim_tensor_contiguous_f64, argmin_dim_tensor_contiguous_f64, asin_scalar,
        asin_tensor_contiguous_f64, atan_scalar, atan_tensor_contiguous_f64,
        bmm_tensor_contiguous_f64, cat_tensor_contiguous_f64, ceil_scalar,
        ceil_tensor_contiguous_f64, clamp_scalar, clamp_tensor_contiguous_f64, cos_scalar,
        cos_tensor_contiguous_f64, cosh_scalar, cosh_tensor_contiguous_f64, div_scalar,
        div_tensor_contiguous_f64, eq_scalar, eq_tensor_contiguous_f64, exp_scalar,
        exp_tensor_contiguous_f64, expand_tensor_contiguous_f64, expm1_scalar,
        expm1_tensor_contiguous_f64, floor_scalar, floor_tensor_contiguous_f64,
        gather_tensor_contiguous_f64, ge_scalar, ge_tensor_contiguous_f64, gelu_scalar,
        gelu_tensor_contiguous_f64, gt_scalar, gt_tensor_contiguous_f64,
        index_select_tensor_contiguous_f64, le_scalar, le_tensor_contiguous_f64, leaky_relu_scalar,
        leaky_relu_tensor_contiguous_f64, log_scalar, log_softmax_dim_tensor_contiguous_f64,
        log_tensor_contiguous_f64, log1p_scalar, log1p_tensor_contiguous_f64, log2_scalar,
        log2_tensor_contiguous_f64, log10_scalar, log10_tensor_contiguous_f64, lt_scalar,
        lt_tensor_contiguous_f64, masked_fill_tensor_contiguous_f64, matmul_tensor_contiguous_f64,
        max_dim_tensor_contiguous_f64, max_scalar, max_tensor_contiguous_f64,
        mean_dim_tensor_contiguous_f64, mean_tensor_contiguous_f64, min_dim_tensor_contiguous_f64,
        min_scalar, min_tensor_contiguous_f64, mul_scalar, mul_tensor_contiguous_f64,
        narrow_tensor_contiguous_f64, ne_scalar, ne_tensor_contiguous_f64, neg_scalar,
        neg_tensor_contiguous_f64, outer_tensor_contiguous_f64, pow_scalar,
        pow_tensor_contiguous_f64, prod_dim_tensor_contiguous_f64, reciprocal_scalar,
        reciprocal_tensor_contiguous_f64, relu_scalar, relu_tensor_contiguous_f64,
        scatter_tensor_contiguous_f64, sigmoid_scalar, sigmoid_tensor_contiguous_f64, sign_scalar,
        sign_tensor_contiguous_f64, silu_scalar, silu_tensor_contiguous_f64, sinh_scalar,
        sinh_tensor_contiguous_f64, softmax_dim_tensor_contiguous_f64, sqrt_scalar,
        sqrt_tensor_contiguous_f64, stack_tensor_contiguous_f64, std_dim_tensor_contiguous_f64,
        sub_scalar, sub_tensor_contiguous_f64, sum_dim_tensor_contiguous_f64,
        sum_tensor_contiguous_f64, tanh_scalar, tanh_tensor_contiguous_f64, trunc_scalar,
        trunc_tensor_contiguous_f64, var_dim_tensor_contiguous_f64,
    };

    #[test]
    fn neg_scalar_returns_expected_value() {
        let input = ScalarTensor::new(3.5, DType::F64, Device::Cpu);
        let out = neg_scalar(&input);
        assert_eq!(out.value(), -3.5);
    }

    #[test]
    fn neg_scalar_double_negation_identity() {
        let input = ScalarTensor::new(-7.0, DType::F64, Device::Cpu);
        let out = neg_scalar(&neg_scalar(&input));
        assert_eq!(out.value(), -7.0);
    }

    #[test]
    fn neg_scalar_zero_is_zero() {
        let input = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = neg_scalar(&input);
        assert_eq!(out.value(), 0.0);
    }

    #[test]
    fn neg_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let input = vec![1.0, -2.0, 3.0, -4.0];

        let out = neg_tensor_contiguous_f64(&input, &meta).expect("contiguous neg should succeed");
        assert_eq!(out, vec![-1.0, 2.0, -3.0, 4.0]);
    }

    #[test]
    fn neg_tensor_contiguous_respects_storage_offset() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu).with_storage_offset(1);
        let input = vec![99.0, 5.0, -7.0, 9.0];

        let out = neg_tensor_contiguous_f64(&input, &meta).expect("offset neg should succeed");
        assert_eq!(out, vec![-5.0, 7.0, -9.0]);
    }

    #[test]
    fn neg_tensor_contiguous_empty_shape_returns_empty_output() {
        let meta =
            TensorMeta::from_shape(vec![0, 3], DType::F64, Device::Cpu).with_storage_offset(8);
        let input = Vec::new();

        let out =
            neg_tensor_contiguous_f64(&input, &meta).expect("empty tensor neg should succeed");
        assert!(out.is_empty());
    }

    #[test]
    fn neg_tensor_contiguous_rejects_non_contiguous_layout() {
        let meta =
            TensorMeta::from_shape_and_strides(vec![2, 2], vec![1, 2], 0, DType::F64, Device::Cpu)
                .expect("test meta should be valid");
        let input = vec![1.0, 2.0, 3.0, 4.0];

        let err = neg_tensor_contiguous_f64(&input, &meta)
            .expect_err("non-contiguous input must be rejected");
        assert!(matches!(
            err,
            KernelError::UnsupportedLayout { side: "input" }
        ));
    }

    #[test]
    fn neg_tensor_contiguous_rejects_insufficient_storage() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu).with_storage_offset(1);
        let input = vec![10.0, 11.0, 12.0];

        let err = neg_tensor_contiguous_f64(&input, &meta)
            .expect_err("insufficient storage should fail closed");
        assert!(matches!(
            err,
            KernelError::InsufficientStorage {
                side: "input",
                needed: 4,
                available: 3
            }
        ));
    }

    #[test]
    fn abs_scalar_returns_expected_value() {
        let input = ScalarTensor::new(-3.5, DType::F64, Device::Cpu);
        let out = abs_scalar(&input);
        assert_eq!(out.value(), 3.5);
    }

    #[test]
    fn abs_scalar_positive_unchanged() {
        let input = ScalarTensor::new(7.0, DType::F64, Device::Cpu);
        let out = abs_scalar(&input);
        assert_eq!(out.value(), 7.0);
    }

    #[test]
    fn abs_scalar_zero_is_zero() {
        let input = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = abs_scalar(&input);
        assert_eq!(out.value(), 0.0);
    }

    #[test]
    fn abs_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let input = vec![-1.0, 2.0, -3.0, 4.0];

        let out = abs_tensor_contiguous_f64(&input, &meta).expect("contiguous abs should succeed");
        assert_eq!(out, vec![1.0, 2.0, 3.0, 4.0]);
    }

    #[test]
    fn abs_tensor_contiguous_empty_returns_empty() {
        let meta = TensorMeta::from_shape(vec![0], DType::F64, Device::Cpu);
        let input = Vec::new();

        let out = abs_tensor_contiguous_f64(&input, &meta).expect("empty abs should succeed");
        assert!(out.is_empty());
    }

    #[test]
    fn exp_scalar_returns_expected_value() {
        let input = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = exp_scalar(&input);
        assert_eq!(out.value(), 1.0);
    }

    #[test]
    fn exp_scalar_of_one() {
        let input = ScalarTensor::new(1.0, DType::F64, Device::Cpu);
        let out = exp_scalar(&input);
        assert!((out.value() - std::f64::consts::E).abs() < 1e-10);
    }

    #[test]
    fn log_scalar_returns_expected_value() {
        let input = ScalarTensor::new(1.0, DType::F64, Device::Cpu);
        let out = log_scalar(&input);
        assert_eq!(out.value(), 0.0);
    }

    #[test]
    fn log_scalar_of_e() {
        let input = ScalarTensor::new(std::f64::consts::E, DType::F64, Device::Cpu);
        let out = log_scalar(&input);
        assert!((out.value() - 1.0).abs() < 1e-10);
    }

    #[test]
    fn exp_log_roundtrip() {
        let input = ScalarTensor::new(2.5, DType::F64, Device::Cpu);
        let exp_result = exp_scalar(&input);
        let log_result = log_scalar(&exp_result);
        assert!((log_result.value() - 2.5).abs() < 1e-10);
    }

    #[test]
    fn exp_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![0.0, 1.0, 2.0];

        let out = exp_tensor_contiguous_f64(&input, &meta).expect("contiguous exp should succeed");
        assert_eq!(out.len(), 3);
        assert!((out[0] - 1.0).abs() < 1e-10);
        assert!((out[1] - std::f64::consts::E).abs() < 1e-10);
        assert!((out[2] - 2.0_f64.exp()).abs() < 1e-10);
    }

    #[test]
    fn log_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let input = vec![1.0, std::f64::consts::E];

        let out = log_tensor_contiguous_f64(&input, &meta).expect("contiguous log should succeed");
        assert!((out[0] - 0.0).abs() < 1e-10);
        assert!((out[1] - 1.0).abs() < 1e-10);
    }

    #[test]
    fn relu_scalar_positive_passes_through() {
        let input = ScalarTensor::new(3.5, DType::F64, Device::Cpu);
        let out = relu_scalar(&input);
        assert_eq!(out.value(), 3.5);
    }

    #[test]
    fn relu_scalar_negative_returns_zero() {
        let input = ScalarTensor::new(-2.0, DType::F64, Device::Cpu);
        let out = relu_scalar(&input);
        assert_eq!(out.value(), 0.0);
    }

    #[test]
    fn relu_scalar_zero_returns_zero() {
        let input = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = relu_scalar(&input);
        assert_eq!(out.value(), 0.0);
    }

    #[test]
    fn relu_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let input = vec![-1.0, 0.0, 2.0, -3.0];

        let out =
            relu_tensor_contiguous_f64(&input, &meta).expect("contiguous relu should succeed");
        assert_eq!(out, vec![0.0, 0.0, 2.0, 0.0]);
    }

    #[test]
    fn sigmoid_scalar_at_zero_returns_half() {
        let input = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = sigmoid_scalar(&input);
        assert_eq!(out.value(), 0.5);
    }

    #[test]
    fn sigmoid_scalar_large_positive_approaches_one() {
        let input = ScalarTensor::new(10.0, DType::F64, Device::Cpu);
        let out = sigmoid_scalar(&input);
        assert!((out.value() - 1.0).abs() < 1e-4);
    }

    #[test]
    fn sigmoid_scalar_large_negative_approaches_zero() {
        let input = ScalarTensor::new(-10.0, DType::F64, Device::Cpu);
        let out = sigmoid_scalar(&input);
        assert!(out.value().abs() < 1e-4);
    }

    #[test]
    fn sigmoid_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![0.0, 10.0, -10.0];

        let out = sigmoid_tensor_contiguous_f64(&input, &meta)
            .expect("contiguous sigmoid should succeed");
        assert!((out[0] - 0.5).abs() < 1e-10);
        assert!((out[1] - 1.0).abs() < 1e-4);
        assert!(out[2].abs() < 1e-4);
    }

    #[test]
    fn tanh_scalar_at_zero_returns_zero() {
        let input = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = tanh_scalar(&input);
        assert_eq!(out.value(), 0.0);
    }

    #[test]
    fn tanh_scalar_large_positive_approaches_one() {
        let input = ScalarTensor::new(10.0, DType::F64, Device::Cpu);
        let out = tanh_scalar(&input);
        assert!((out.value() - 1.0).abs() < 1e-6);
    }

    #[test]
    fn tanh_scalar_is_odd_function() {
        let pos = ScalarTensor::new(1.5, DType::F64, Device::Cpu);
        let neg = ScalarTensor::new(-1.5, DType::F64, Device::Cpu);
        let out_pos = tanh_scalar(&pos);
        let out_neg = tanh_scalar(&neg);
        assert!((out_pos.value() + out_neg.value()).abs() < 1e-10);
    }

    #[test]
    fn tanh_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![0.0, 10.0, -10.0];

        let out =
            tanh_tensor_contiguous_f64(&input, &meta).expect("contiguous tanh should succeed");
        assert!((out[0]).abs() < 1e-10);
        assert!((out[1] - 1.0).abs() < 1e-6);
        assert!((out[2] + 1.0).abs() < 1e-6);
    }

    #[test]
    fn sum_tensor_contiguous_returns_expected_value() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0];
        let out = sum_tensor_contiguous_f64(&input, &meta).expect("contiguous sum should succeed");
        assert_eq!(out, 10.0);
    }

    #[test]
    fn sum_tensor_contiguous_empty_returns_zero() {
        let meta = TensorMeta::from_shape(vec![0], DType::F64, Device::Cpu);
        let input: Vec<f64> = vec![];
        let out = sum_tensor_contiguous_f64(&input, &meta).expect("empty sum should succeed");
        assert_eq!(out, 0.0);
    }

    #[test]
    fn sum_tensor_contiguous_respects_storage_offset() {
        let meta = TensorMeta::from_shape_and_strides(vec![2], vec![1], 2, DType::F64, Device::Cpu)
            .expect("offset meta should validate");
        let storage = vec![100.0, 200.0, 3.0, 7.0];
        let out = sum_tensor_contiguous_f64(&storage, &meta).expect("offset sum should succeed");
        assert_eq!(out, 10.0);
    }

    #[test]
    fn mean_tensor_contiguous_returns_expected_value() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0];
        let out =
            mean_tensor_contiguous_f64(&input, &meta).expect("contiguous mean should succeed");
        assert_eq!(out, 2.5);
    }

    #[test]
    fn mean_tensor_contiguous_empty_returns_nan() {
        let meta = TensorMeta::from_shape(vec![0], DType::F64, Device::Cpu);
        let input: Vec<f64> = vec![];
        let out = mean_tensor_contiguous_f64(&input, &meta).expect("empty mean should succeed");
        assert!(out.is_nan());
    }

    #[test]
    fn mean_tensor_contiguous_single_element() {
        let meta = TensorMeta::from_shape(vec![1], DType::F64, Device::Cpu);
        let input = vec![42.0];
        let out = mean_tensor_contiguous_f64(&input, &meta).expect("single mean should succeed");
        assert_eq!(out, 42.0);
    }

    #[test]
    fn add_scalar_returns_expected_value() {
        let lhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(3.5, DType::F64, Device::Cpu);

        let out = add_scalar(&lhs, &rhs).expect("add should succeed");
        assert_eq!(out.value(), 5.5);
    }

    #[test]
    fn mul_scalar_returns_expected_value() {
        let lhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(4.0, DType::F64, Device::Cpu);

        let out = mul_scalar(&lhs, &rhs).expect("mul should succeed");
        assert_eq!(out.value(), 8.0);
    }

    #[test]
    fn sub_scalar_returns_expected_value() {
        let lhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(4.0, DType::F64, Device::Cpu);

        let out = sub_scalar(&lhs, &rhs).expect("sub should succeed");
        assert_eq!(out.value(), -2.0);
    }

    #[test]
    fn div_scalar_returns_expected_value() {
        let lhs = ScalarTensor::new(7.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);

        let out = div_scalar(&lhs, &rhs).expect("div should succeed");
        assert_eq!(out.value(), 3.5);
    }

    #[test]
    fn add_scalar_rejects_dtype_mismatch() {
        let lhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(4.0, DType::F32, Device::Cpu);

        let err = add_scalar(&lhs, &rhs).expect_err("dtype mismatch must fail closed");
        assert!(matches!(
            err,
            KernelError::Incompatible(TensorCompatError::DTypeMismatch { .. })
        ));
    }

    #[test]
    fn mul_scalar_rejects_device_mismatch() {
        let lhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(4.0, DType::F64, Device::Cuda);

        let err = mul_scalar(&lhs, &rhs).expect_err("device mismatch must fail closed");
        assert!(matches!(
            err,
            KernelError::Incompatible(TensorCompatError::DeviceMismatch { .. })
        ));
    }

    #[test]
    fn sub_scalar_rejects_dtype_mismatch() {
        let lhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(4.0, DType::F32, Device::Cpu);

        let err = sub_scalar(&lhs, &rhs).expect_err("dtype mismatch must fail closed");
        assert!(matches!(
            err,
            KernelError::Incompatible(TensorCompatError::DTypeMismatch { .. })
        ));
    }

    #[test]
    fn div_scalar_rejects_device_mismatch() {
        let lhs = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let rhs = ScalarTensor::new(4.0, DType::F64, Device::Cuda);

        let err = div_scalar(&lhs, &rhs).expect_err("device mismatch must fail closed");
        assert!(matches!(
            err,
            KernelError::Incompatible(TensorCompatError::DeviceMismatch { .. })
        ));
    }

    #[test]
    fn add_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let lhs = vec![1.0, 2.0, 3.0, 4.0];
        let rhs = vec![0.5, 1.5, 2.5, 3.5];

        let out = add_tensor_contiguous_f64(&lhs, &rhs, &meta, &meta)
            .expect("contiguous add should succeed");
        assert_eq!(out, vec![1.5, 3.5, 5.5, 7.5]);
    }

    #[test]
    fn sub_tensor_contiguous_respects_storage_offsets() {
        let lhs_meta =
            TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu).with_storage_offset(1);
        let rhs_meta =
            TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu).with_storage_offset(2);
        let lhs = vec![99.0, 5.0, 7.0, 9.0];
        let rhs = vec![11.0, 22.0, 3.0, 4.0, 5.0];

        let out = sub_tensor_contiguous_f64(&lhs, &rhs, &lhs_meta, &rhs_meta)
            .expect("offset add should succeed");
        assert_eq!(out, vec![2.0, 3.0, 4.0]);
    }

    #[test]
    fn mul_tensor_contiguous_rejects_shape_mismatch() {
        let lhs_meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let rhs_meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let lhs = vec![1.0, 2.0, 3.0, 4.0];
        let rhs = vec![1.0, 2.0, 3.0, 4.0];

        let err = mul_tensor_contiguous_f64(&lhs, &rhs, &lhs_meta, &rhs_meta)
            .expect_err("shape mismatch must fail closed");
        assert!(matches!(err, KernelError::ShapeMismatch { .. }));
    }

    #[test]
    fn div_tensor_contiguous_rejects_non_contiguous_layout() {
        let lhs_meta =
            TensorMeta::from_shape_and_strides(vec![2, 2], vec![1, 2], 0, DType::F64, Device::Cpu)
                .expect("test meta should be valid");
        let rhs_meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let lhs = vec![2.0, 4.0, 6.0, 8.0];
        let rhs = vec![1.0, 2.0, 3.0, 4.0];

        let err = div_tensor_contiguous_f64(&lhs, &rhs, &lhs_meta, &rhs_meta)
            .expect_err("non-contiguous input must be rejected");
        assert!(matches!(
            err,
            KernelError::UnsupportedLayout { side: "lhs" }
        ));
    }

    #[test]
    fn add_tensor_contiguous_rejects_insufficient_storage() {
        let lhs_meta =
            TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu).with_storage_offset(1);
        let rhs_meta =
            TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu).with_storage_offset(1);
        let lhs = vec![10.0, 11.0, 12.0];
        let rhs = vec![20.0, 21.0, 22.0, 23.0];

        let err = add_tensor_contiguous_f64(&lhs, &rhs, &lhs_meta, &rhs_meta)
            .expect_err("insufficient storage should fail closed");
        assert!(matches!(
            err,
            KernelError::InsufficientStorage {
                side: "lhs",
                needed: 4,
                available: 3
            }
        ));
    }

    #[test]
    fn add_tensor_contiguous_rejects_dtype_mismatch() {
        let lhs_meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let rhs_meta = TensorMeta::from_shape(vec![2], DType::F32, Device::Cpu);
        let lhs = vec![1.0, 2.0];
        let rhs = vec![3.0, 4.0];

        let err = add_tensor_contiguous_f64(&lhs, &rhs, &lhs_meta, &rhs_meta)
            .expect_err("dtype mismatch must fail closed");
        assert!(matches!(
            err,
            KernelError::Incompatible(TensorCompatError::DTypeMismatch { .. })
        ));
    }

    #[test]
    fn add_tensor_contiguous_rejects_device_mismatch() {
        let lhs_meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let rhs_meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cuda);
        let lhs = vec![1.0, 2.0];
        let rhs = vec![3.0, 4.0];

        let err = add_tensor_contiguous_f64(&lhs, &rhs, &lhs_meta, &rhs_meta)
            .expect_err("device mismatch must fail closed");
        assert!(matches!(
            err,
            KernelError::Incompatible(TensorCompatError::DeviceMismatch { .. })
        ));
    }

    #[test]
    fn add_tensor_contiguous_empty_shape_returns_empty_output() {
        let meta =
            TensorMeta::from_shape(vec![0, 3], DType::F64, Device::Cpu).with_storage_offset(8);
        let lhs = Vec::new();
        let rhs = Vec::new();

        let out = add_tensor_contiguous_f64(&lhs, &rhs, &meta, &meta)
            .expect("empty tensors should succeed without touching storage");
        assert!(out.is_empty());
    }

    #[test]
    fn matmul_tensor_contiguous_returns_expected_values() {
        let lhs_meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let rhs_meta = TensorMeta::from_shape(vec![3, 2], DType::F64, Device::Cpu);
        let lhs = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let rhs = vec![7.0, 8.0, 9.0, 10.0, 11.0, 12.0];

        let out = matmul_tensor_contiguous_f64(&lhs, &rhs, &lhs_meta, &rhs_meta)
            .expect("contiguous matmul should succeed");
        assert_eq!(out, vec![58.0, 64.0, 139.0, 154.0]);
    }

    #[test]
    fn matmul_tensor_contiguous_rejects_rank_mismatch() {
        let lhs_meta = TensorMeta::from_shape(vec![6], DType::F64, Device::Cpu);
        let rhs_meta = TensorMeta::from_shape(vec![3, 2], DType::F64, Device::Cpu);
        let lhs = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let rhs = vec![7.0, 8.0, 9.0, 10.0, 11.0, 12.0];

        let err = matmul_tensor_contiguous_f64(&lhs, &rhs, &lhs_meta, &rhs_meta)
            .expect_err("rank mismatch must fail closed");
        assert!(matches!(err, KernelError::ShapeMismatch { .. }));
    }

    #[test]
    fn matmul_tensor_contiguous_rejects_inner_dimension_mismatch() {
        let lhs_meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let rhs_meta = TensorMeta::from_shape(vec![3, 2], DType::F64, Device::Cpu);
        let lhs = vec![1.0, 2.0, 3.0, 4.0];
        let rhs = vec![7.0, 8.0, 9.0, 10.0, 11.0, 12.0];

        let err = matmul_tensor_contiguous_f64(&lhs, &rhs, &lhs_meta, &rhs_meta)
            .expect_err("inner-dimension mismatch must fail closed");
        assert!(matches!(err, KernelError::ShapeMismatch { .. }));
    }

    #[test]
    fn matmul_tensor_contiguous_rejects_output_shape_overflow() {
        let lhs_meta = TensorMeta::from_shape(vec![usize::MAX, 1], DType::F64, Device::Cpu);
        let rhs_meta = TensorMeta::from_shape(vec![1, 2], DType::F64, Device::Cpu);
        let err = matmul_tensor_contiguous_f64(&[], &[], &lhs_meta, &rhs_meta)
            .expect_err("output shape multiplication overflow must fail closed");
        assert!(matches!(
            err,
            KernelError::ShapeOverflow {
                context: "matmul output shape multiplication overflow"
            }
        ));
    }

    #[test]
    fn outer_tensor_contiguous_rejects_output_shape_overflow() {
        let lhs_meta = TensorMeta::from_shape(vec![usize::MAX], DType::F64, Device::Cpu);
        let rhs_meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let err = outer_tensor_contiguous_f64(&[], &[], &lhs_meta, &rhs_meta)
            .expect_err("output shape multiplication overflow must fail closed");
        assert!(
            matches!(err, KernelError::ShapeOverflow { .. }),
            "unexpected error: {err:?}"
        );
    }

    #[test]
    fn bmm_tensor_contiguous_rejects_output_shape_overflow() {
        let lhs_meta = TensorMeta::from_shape(vec![1, usize::MAX, 1], DType::F64, Device::Cpu);
        let rhs_meta = TensorMeta::from_shape(vec![1, 1, 2], DType::F64, Device::Cpu);
        let err = bmm_tensor_contiguous_f64(&[], &[], &lhs_meta, &rhs_meta)
            .expect_err("output shape multiplication overflow must fail closed");
        assert!(matches!(
            err,
            KernelError::ShapeOverflow {
                context: "bmm output batch stride overflow"
            }
        ));
    }

    #[test]
    fn eq_scalar_returns_expected_values() {
        let a = ScalarTensor::new(3.0, DType::F64, Device::Cpu);
        let b = ScalarTensor::new(3.0, DType::F64, Device::Cpu);
        let c = ScalarTensor::new(4.0, DType::F64, Device::Cpu);
        assert_eq!(eq_scalar(&a, &b).expect("eq should succeed").value(), 1.0);
        assert_eq!(eq_scalar(&a, &c).expect("eq should succeed").value(), 0.0);
    }

    #[test]
    fn ne_scalar_returns_expected_values() {
        let a = ScalarTensor::new(3.0, DType::F64, Device::Cpu);
        let b = ScalarTensor::new(3.0, DType::F64, Device::Cpu);
        let c = ScalarTensor::new(4.0, DType::F64, Device::Cpu);
        assert_eq!(ne_scalar(&a, &b).expect("ne should succeed").value(), 0.0);
        assert_eq!(ne_scalar(&a, &c).expect("ne should succeed").value(), 1.0);
    }

    #[test]
    fn eq_ne_scalar_respect_ieee_special_values() {
        let pos_inf = ScalarTensor::new(f64::INFINITY, DType::F64, Device::Cpu);
        let neg_inf = ScalarTensor::new(f64::NEG_INFINITY, DType::F64, Device::Cpu);
        let nan = ScalarTensor::new(f64::NAN, DType::F64, Device::Cpu);

        assert_eq!(
            eq_scalar(&pos_inf, &pos_inf)
                .expect("eq(+inf,+inf) should succeed")
                .value(),
            1.0
        );
        assert_eq!(
            eq_scalar(&neg_inf, &neg_inf)
                .expect("eq(-inf,-inf) should succeed")
                .value(),
            1.0
        );
        assert_eq!(
            ne_scalar(&pos_inf, &pos_inf)
                .expect("ne(+inf,+inf) should succeed")
                .value(),
            0.0
        );
        assert_eq!(
            ne_scalar(&neg_inf, &neg_inf)
                .expect("ne(-inf,-inf) should succeed")
                .value(),
            0.0
        );
        assert_eq!(
            eq_scalar(&nan, &nan)
                .expect("eq(nan,nan) should succeed")
                .value(),
            0.0
        );
        assert_eq!(
            ne_scalar(&nan, &nan)
                .expect("ne(nan,nan) should succeed")
                .value(),
            1.0
        );
        assert_eq!(
            eq_scalar(&pos_inf, &neg_inf)
                .expect("eq(+inf,-inf) should succeed")
                .value(),
            0.0
        );
        assert_eq!(
            ne_scalar(&pos_inf, &neg_inf)
                .expect("ne(+inf,-inf) should succeed")
                .value(),
            1.0
        );
    }

    #[test]
    fn lt_gt_scalar_returns_expected_values() {
        let a = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let b = ScalarTensor::new(3.0, DType::F64, Device::Cpu);
        assert_eq!(lt_scalar(&a, &b).expect("lt should succeed").value(), 1.0);
        assert_eq!(lt_scalar(&b, &a).expect("lt should succeed").value(), 0.0);
        assert_eq!(gt_scalar(&b, &a).expect("gt should succeed").value(), 1.0);
        assert_eq!(gt_scalar(&a, &b).expect("gt should succeed").value(), 0.0);
    }

    #[test]
    fn le_ge_scalar_returns_expected_values() {
        let a = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let b = ScalarTensor::new(2.0, DType::F64, Device::Cpu);
        let c = ScalarTensor::new(3.0, DType::F64, Device::Cpu);
        assert_eq!(le_scalar(&a, &b).expect("le should succeed").value(), 1.0);
        assert_eq!(le_scalar(&a, &c).expect("le should succeed").value(), 1.0);
        assert_eq!(le_scalar(&c, &a).expect("le should succeed").value(), 0.0);
        assert_eq!(ge_scalar(&a, &b).expect("ge should succeed").value(), 1.0);
        assert_eq!(ge_scalar(&c, &a).expect("ge should succeed").value(), 1.0);
        assert_eq!(ge_scalar(&a, &c).expect("ge should succeed").value(), 0.0);
    }

    #[test]
    fn lt_gt_le_ge_scalar_respect_ieee_special_values() {
        let pos_inf = ScalarTensor::new(f64::INFINITY, DType::F64, Device::Cpu);
        let neg_inf = ScalarTensor::new(f64::NEG_INFINITY, DType::F64, Device::Cpu);
        let nan = ScalarTensor::new(f64::NAN, DType::F64, Device::Cpu);

        assert_eq!(
            lt_scalar(&nan, &nan)
                .expect("lt(nan,nan) should succeed")
                .value(),
            0.0
        );
        assert_eq!(
            gt_scalar(&nan, &nan)
                .expect("gt(nan,nan) should succeed")
                .value(),
            0.0
        );
        assert_eq!(
            le_scalar(&nan, &nan)
                .expect("le(nan,nan) should succeed")
                .value(),
            0.0
        );
        assert_eq!(
            ge_scalar(&nan, &nan)
                .expect("ge(nan,nan) should succeed")
                .value(),
            0.0
        );
        assert_eq!(
            lt_scalar(&neg_inf, &pos_inf)
                .expect("lt(-inf,+inf) should succeed")
                .value(),
            1.0
        );
        assert_eq!(
            gt_scalar(&pos_inf, &neg_inf)
                .expect("gt(+inf,-inf) should succeed")
                .value(),
            1.0
        );
        assert_eq!(
            le_scalar(&pos_inf, &pos_inf)
                .expect("le(+inf,+inf) should succeed")
                .value(),
            1.0
        );
        assert_eq!(
            ge_scalar(&neg_inf, &neg_inf)
                .expect("ge(-inf,-inf) should succeed")
                .value(),
            1.0
        );
    }

    #[test]
    fn eq_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let lhs = vec![1.0, 2.0, 3.0, 4.0];
        let rhs = vec![1.0, 5.0, 3.0, 0.0];
        let out = eq_tensor_contiguous_f64(&lhs, &rhs, &meta, &meta).expect("eq should succeed");
        assert_eq!(out, vec![1.0, 0.0, 1.0, 0.0]);
    }

    #[test]
    fn ne_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let lhs = vec![1.0, 2.0, 3.0, 4.0];
        let rhs = vec![1.0, 5.0, 3.0, 0.0];
        let out = ne_tensor_contiguous_f64(&lhs, &rhs, &meta, &meta).expect("ne should succeed");
        assert_eq!(out, vec![0.0, 1.0, 0.0, 1.0]);
    }

    #[test]
    fn eq_ne_tensor_contiguous_respect_ieee_special_values() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let lhs = vec![f64::INFINITY, f64::NEG_INFINITY, f64::NAN, 1.0];
        let rhs = vec![f64::INFINITY, f64::NEG_INFINITY, f64::NAN, 2.0];

        let eq_out =
            eq_tensor_contiguous_f64(&lhs, &rhs, &meta, &meta).expect("eq tensor should succeed");
        let ne_out =
            ne_tensor_contiguous_f64(&lhs, &rhs, &meta, &meta).expect("ne tensor should succeed");

        assert_eq!(eq_out, vec![1.0, 1.0, 0.0, 0.0]);
        assert_eq!(ne_out, vec![0.0, 0.0, 1.0, 1.0]);
    }

    #[test]
    fn lt_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let lhs = vec![1.0, 3.0, 2.0];
        let rhs = vec![2.0, 3.0, 1.0];
        let out = lt_tensor_contiguous_f64(&lhs, &rhs, &meta, &meta).expect("lt should succeed");
        assert_eq!(out, vec![1.0, 0.0, 0.0]);
    }

    #[test]
    fn gt_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let lhs = vec![1.0, 3.0, 2.0];
        let rhs = vec![2.0, 3.0, 1.0];
        let out = gt_tensor_contiguous_f64(&lhs, &rhs, &meta, &meta).expect("gt should succeed");
        assert_eq!(out, vec![0.0, 0.0, 1.0]);
    }

    #[test]
    fn le_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let lhs = vec![1.0, 3.0, 2.0];
        let rhs = vec![2.0, 3.0, 1.0];
        let out = le_tensor_contiguous_f64(&lhs, &rhs, &meta, &meta).expect("le should succeed");
        assert_eq!(out, vec![1.0, 1.0, 0.0]);
    }

    #[test]
    fn ge_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let lhs = vec![1.0, 3.0, 2.0];
        let rhs = vec![2.0, 3.0, 1.0];
        let out = ge_tensor_contiguous_f64(&lhs, &rhs, &meta, &meta).expect("ge should succeed");
        assert_eq!(out, vec![0.0, 1.0, 1.0]);
    }

    #[test]
    fn lt_gt_le_ge_tensor_contiguous_respect_ieee_special_values() {
        let meta = TensorMeta::from_shape(vec![5], DType::F64, Device::Cpu);
        let lhs = vec![f64::NAN, f64::INFINITY, f64::NEG_INFINITY, 1.0, 2.0];
        let rhs = vec![f64::NAN, f64::NEG_INFINITY, f64::INFINITY, 1.0, 3.0];

        let lt_out = lt_tensor_contiguous_f64(&lhs, &rhs, &meta, &meta).expect("lt should succeed");
        let gt_out = gt_tensor_contiguous_f64(&lhs, &rhs, &meta, &meta).expect("gt should succeed");
        let le_out = le_tensor_contiguous_f64(&lhs, &rhs, &meta, &meta).expect("le should succeed");
        let ge_out = ge_tensor_contiguous_f64(&lhs, &rhs, &meta, &meta).expect("ge should succeed");

        assert_eq!(lt_out, vec![0.0, 0.0, 1.0, 0.0, 1.0]);
        assert_eq!(gt_out, vec![0.0, 1.0, 0.0, 0.0, 0.0]);
        assert_eq!(le_out, vec![0.0, 0.0, 1.0, 1.0, 1.0]);
        assert_eq!(ge_out, vec![0.0, 1.0, 0.0, 1.0, 0.0]);
    }

    #[test]
    fn sqrt_scalar_returns_expected_value() {
        let input = ScalarTensor::new(9.0, DType::F64, Device::Cpu);
        let out = sqrt_scalar(&input);
        assert_eq!(out.value(), 3.0);
    }

    #[test]
    fn sqrt_scalar_of_zero() {
        let input = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = sqrt_scalar(&input);
        assert_eq!(out.value(), 0.0);
    }

    #[test]
    fn sqrt_scalar_of_one() {
        let input = ScalarTensor::new(1.0, DType::F64, Device::Cpu);
        let out = sqrt_scalar(&input);
        assert_eq!(out.value(), 1.0);
    }

    #[test]
    fn sqrt_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let input = vec![0.0, 1.0, 4.0, 9.0];
        let out =
            sqrt_tensor_contiguous_f64(&input, &meta).expect("contiguous sqrt should succeed");
        assert_eq!(out, vec![0.0, 1.0, 2.0, 3.0]);
    }

    #[test]
    fn reciprocal_scalar_returns_expected_value() {
        let input = ScalarTensor::new(4.0, DType::F64, Device::Cpu);
        let out = reciprocal_scalar(&input);
        assert_eq!(out.value(), 0.25);
    }

    #[test]
    fn reciprocal_scalar_of_one() {
        let input = ScalarTensor::new(1.0, DType::F64, Device::Cpu);
        let out = reciprocal_scalar(&input);
        assert_eq!(out.value(), 1.0);
    }

    #[test]
    fn reciprocal_scalar_negative() {
        let input = ScalarTensor::new(-2.0, DType::F64, Device::Cpu);
        let out = reciprocal_scalar(&input);
        assert_eq!(out.value(), -0.5);
    }

    #[test]
    fn reciprocal_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 4.0];
        let out = reciprocal_tensor_contiguous_f64(&input, &meta)
            .expect("contiguous reciprocal should succeed");
        assert_eq!(out, vec![1.0, 0.5, 0.25]);
    }

    #[test]
    fn pow_scalar_returns_expected_value() {
        let input = ScalarTensor::new(3.0, DType::F64, Device::Cpu);
        let out = pow_scalar(&input, 2.0);
        assert_eq!(out.value(), 9.0);
    }

    #[test]
    fn pow_scalar_zero_exponent() {
        let input = ScalarTensor::new(5.0, DType::F64, Device::Cpu);
        let out = pow_scalar(&input, 0.0);
        assert_eq!(out.value(), 1.0);
    }

    #[test]
    fn pow_scalar_fractional_exponent() {
        let input = ScalarTensor::new(4.0, DType::F64, Device::Cpu);
        let out = pow_scalar(&input, 0.5);
        assert!((out.value() - 2.0).abs() < 1e-10);
    }

    #[test]
    fn pow_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0];
        let out =
            pow_tensor_contiguous_f64(&input, &meta, 2.0).expect("contiguous pow should succeed");
        assert_eq!(out, vec![1.0, 4.0, 9.0]);
    }

    #[test]
    fn pow_tensor_contiguous_negative_exponent() {
        let meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let input = vec![2.0, 4.0];
        let out = pow_tensor_contiguous_f64(&input, &meta, -1.0)
            .expect("pow with negative exponent should succeed");
        assert_eq!(out, vec![0.5, 0.25]);
    }

    #[test]
    fn clamp_scalar_within_range() {
        let input = ScalarTensor::new(3.0, DType::F64, Device::Cpu);
        let out = clamp_scalar(&input, 1.0, 5.0);
        assert_eq!(out.value(), 3.0);
    }

    #[test]
    fn clamp_scalar_below_min() {
        let input = ScalarTensor::new(-1.0, DType::F64, Device::Cpu);
        let out = clamp_scalar(&input, 0.0, 5.0);
        assert_eq!(out.value(), 0.0);
    }

    #[test]
    fn clamp_scalar_above_max() {
        let input = ScalarTensor::new(10.0, DType::F64, Device::Cpu);
        let out = clamp_scalar(&input, 0.0, 5.0);
        assert_eq!(out.value(), 5.0);
    }

    #[test]
    fn clamp_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![5], DType::F64, Device::Cpu);
        let input = vec![-2.0, 0.0, 3.0, 5.0, 8.0];
        let out = clamp_tensor_contiguous_f64(&input, &meta, 0.0, 5.0)
            .expect("contiguous clamp should succeed");
        assert_eq!(out, vec![0.0, 0.0, 3.0, 5.0, 5.0]);
    }

    #[test]
    fn min_scalar_returns_expected_value() {
        let a = ScalarTensor::new(3.0, DType::F64, Device::Cpu);
        let b = ScalarTensor::new(5.0, DType::F64, Device::Cpu);
        let out = min_scalar(&a, &b).expect("min should succeed");
        assert_eq!(out.value(), 3.0);
    }

    #[test]
    fn max_scalar_returns_expected_value() {
        let a = ScalarTensor::new(3.0, DType::F64, Device::Cpu);
        let b = ScalarTensor::new(5.0, DType::F64, Device::Cpu);
        let out = max_scalar(&a, &b).expect("max should succeed");
        assert_eq!(out.value(), 5.0);
    }

    #[test]
    fn min_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let lhs = vec![1.0, 5.0, 3.0];
        let rhs = vec![2.0, 4.0, 3.0];
        let out = min_tensor_contiguous_f64(&lhs, &rhs, &meta, &meta).expect("min should succeed");
        assert_eq!(out, vec![1.0, 4.0, 3.0]);
    }

    #[test]
    fn max_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let lhs = vec![1.0, 5.0, 3.0];
        let rhs = vec![2.0, 4.0, 3.0];
        let out = max_tensor_contiguous_f64(&lhs, &rhs, &meta, &meta).expect("max should succeed");
        assert_eq!(out, vec![2.0, 5.0, 3.0]);
    }

    #[test]
    fn sum_dim_reduces_along_dim0() {
        // shape [2, 3]: [[1, 2, 3], [4, 5, 6]]
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let out = sum_dim_tensor_contiguous_f64(&input, &meta, 0).expect("sum_dim 0");
        // reduce rows: [1+4, 2+5, 3+6] = [5, 7, 9]
        assert_eq!(out, vec![5.0, 7.0, 9.0]);
    }

    #[test]
    fn sum_dim_reduces_along_dim1() {
        // shape [2, 3]: [[1, 2, 3], [4, 5, 6]]
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let out = sum_dim_tensor_contiguous_f64(&input, &meta, 1).expect("sum_dim 1");
        // reduce cols: [1+2+3, 4+5+6] = [6, 15]
        assert_eq!(out, vec![6.0, 15.0]);
    }

    #[test]
    fn sum_dim_3d_reduces_middle_dim() {
        // shape [2, 3, 2]: 12 elements
        let meta = TensorMeta::from_shape(vec![2, 3, 2], DType::F64, Device::Cpu);
        let input = vec![
            1.0, 2.0, 3.0, 4.0, 5.0, 6.0, // first [3,2] slice
            7.0, 8.0, 9.0, 10.0, 11.0, 12.0, // second [3,2] slice
        ];
        let out = sum_dim_tensor_contiguous_f64(&input, &meta, 1).expect("sum_dim 1 on 3d");
        // Output shape [2, 2]: reduce along dim 1 (size 3)
        // [0,0]: 1+3+5=9, [0,1]: 2+4+6=12, [1,0]: 7+9+11=27, [1,1]: 8+10+12=30
        assert_eq!(out, vec![9.0, 12.0, 27.0, 30.0]);
    }

    #[test]
    fn sum_dim_invalid_dim_returns_error() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let err = sum_dim_tensor_contiguous_f64(&input, &meta, 2).unwrap_err();
        assert!(matches!(
            err,
            KernelError::InvalidDimension { dim: 2, ndim: 2 }
        ));
    }

    #[test]
    fn mean_dim_reduces_along_dim0() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let out = mean_dim_tensor_contiguous_f64(&input, &meta, 0).expect("mean_dim 0");
        // [2.5, 3.5, 4.5]
        assert_eq!(out, vec![2.5, 3.5, 4.5]);
    }

    #[test]
    fn mean_dim_reduces_along_dim1() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let out = mean_dim_tensor_contiguous_f64(&input, &meta, 1).expect("mean_dim 1");
        // [6/3, 15/3] = [2.0, 5.0]
        assert_eq!(out, vec![2.0, 5.0]);
    }

    #[test]
    fn sum_dim_1d_reduces_to_scalar() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0];
        let out = sum_dim_tensor_contiguous_f64(&input, &meta, 0).expect("sum_dim on 1d");
        // Reduces [4] along dim 0 -> scalar-like vec of len 1? No, outer=1, inner=1 -> output len 1
        assert_eq!(out, vec![10.0]);
    }

    #[test]
    fn prod_dim_reduces_along_dim0() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let out = prod_dim_tensor_contiguous_f64(&input, &meta, 0).expect("prod_dim 0");
        // [1*4, 2*5, 3*6] = [4, 10, 18]
        assert_eq!(out, vec![4.0, 10.0, 18.0]);
    }

    #[test]
    fn prod_dim_reduces_along_dim1() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let out = prod_dim_tensor_contiguous_f64(&input, &meta, 1).expect("prod_dim 1");
        // [1*2*3, 4*5*6] = [6, 120]
        assert_eq!(out, vec![6.0, 120.0]);
    }

    #[test]
    fn prod_dim_invalid_dim_returns_error() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let err = prod_dim_tensor_contiguous_f64(&input, &meta, 2).unwrap_err();
        assert!(matches!(
            err,
            KernelError::InvalidDimension { dim: 2, ndim: 2 }
        ));
    }

    #[test]
    fn var_dim_reduces_along_dim0() {
        let meta = TensorMeta::from_shape(vec![3, 2], DType::F64, Device::Cpu);
        // col0: [1,3,5] mean=3, var=((1-3)^2+(3-3)^2+(5-3)^2)/2 = (4+0+4)/2 = 4
        // col1: [2,4,6] mean=4, var=((2-4)^2+(4-4)^2+(6-4)^2)/2 = (4+0+4)/2 = 4
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let out = var_dim_tensor_contiguous_f64(&input, &meta, 0).expect("var_dim 0");
        assert!((out[0] - 4.0).abs() < 1e-12);
        assert!((out[1] - 4.0).abs() < 1e-12);
    }

    #[test]
    fn var_dim_reduces_along_dim1() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        // row0: [1,2,3] mean=2, var=((1-2)^2+(2-2)^2+(3-2)^2)/2 = (1+0+1)/2 = 1
        // row1: [4,5,6] mean=5, var=((4-5)^2+(5-5)^2+(6-5)^2)/2 = (1+0+1)/2 = 1
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let out = var_dim_tensor_contiguous_f64(&input, &meta, 1).expect("var_dim 1");
        assert!((out[0] - 1.0).abs() < 1e-12);
        assert!((out[1] - 1.0).abs() < 1e-12);
    }

    #[test]
    fn std_dim_reduces_along_dim0() {
        let meta = TensorMeta::from_shape(vec![3, 2], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let out = std_dim_tensor_contiguous_f64(&input, &meta, 0).expect("std_dim 0");
        assert!((out[0] - 2.0).abs() < 1e-12); // sqrt(4) = 2
        assert!((out[1] - 2.0).abs() < 1e-12);
    }

    #[test]
    fn argmax_dim_reduces_along_dim0() {
        // shape [2, 3]: [[1, 5, 3], [4, 2, 6]]
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 5.0, 3.0, 4.0, 2.0, 6.0];
        let out = argmax_dim_tensor_contiguous_f64(&input, &meta, 0).expect("argmax_dim 0");
        // col0: max(1,4)=4 at idx 1; col1: max(5,2)=5 at idx 0; col2: max(3,6)=6 at idx 1
        assert_eq!(out, vec![1.0, 0.0, 1.0]);
    }

    #[test]
    fn argmax_dim_reduces_along_dim1() {
        // shape [2, 3]: [[1, 5, 3], [4, 2, 6]]
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 5.0, 3.0, 4.0, 2.0, 6.0];
        let out = argmax_dim_tensor_contiguous_f64(&input, &meta, 1).expect("argmax_dim 1");
        // row0: max(1,5,3)=5 at idx 1; row1: max(4,2,6)=6 at idx 2
        assert_eq!(out, vec![1.0, 2.0]);
    }

    #[test]
    fn argmax_dim_invalid_dim_returns_error() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let err = argmax_dim_tensor_contiguous_f64(&input, &meta, 2).unwrap_err();
        assert!(matches!(
            err,
            KernelError::InvalidDimension { dim: 2, ndim: 2 }
        ));
    }

    #[test]
    fn argmin_dim_reduces_along_dim0() {
        // shape [2, 3]: [[1, 5, 3], [4, 2, 6]]
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 5.0, 3.0, 4.0, 2.0, 6.0];
        let out = argmin_dim_tensor_contiguous_f64(&input, &meta, 0).expect("argmin_dim 0");
        // col0: min(1,4)=1 at idx 0; col1: min(5,2)=2 at idx 1; col2: min(3,6)=3 at idx 0
        assert_eq!(out, vec![0.0, 1.0, 0.0]);
    }

    #[test]
    fn argmin_dim_reduces_along_dim1() {
        // shape [2, 3]: [[1, 5, 3], [4, 2, 6]]
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 5.0, 3.0, 4.0, 2.0, 6.0];
        let out = argmin_dim_tensor_contiguous_f64(&input, &meta, 1).expect("argmin_dim 1");
        // row0: min(1,5,3)=1 at idx 0; row1: min(4,2,6)=2 at idx 1
        assert_eq!(out, vec![0.0, 1.0]);
    }

    #[test]
    fn argmin_dim_invalid_dim_returns_error() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let err = argmin_dim_tensor_contiguous_f64(&input, &meta, 2).unwrap_err();
        assert!(matches!(
            err,
            KernelError::InvalidDimension { dim: 2, ndim: 2 }
        ));
    }

    #[test]
    fn max_dim_reduces_along_dim0() {
        // shape [2, 3]: [[1, 5, 3], [4, 2, 6]]
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 5.0, 3.0, 4.0, 2.0, 6.0];
        let (values, indices) = max_dim_tensor_contiguous_f64(&input, &meta, 0).expect("max_dim 0");
        assert_eq!(values, vec![4.0, 5.0, 6.0]);
        assert_eq!(indices, vec![1.0, 0.0, 1.0]);
    }

    #[test]
    fn max_dim_reduces_along_dim1() {
        // shape [2, 3]: [[1, 5, 3], [4, 2, 6]]
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 5.0, 3.0, 4.0, 2.0, 6.0];
        let (values, indices) = max_dim_tensor_contiguous_f64(&input, &meta, 1).expect("max_dim 1");
        assert_eq!(values, vec![5.0, 6.0]);
        assert_eq!(indices, vec![1.0, 2.0]);
    }

    #[test]
    fn max_dim_invalid_dim_returns_error() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let err = max_dim_tensor_contiguous_f64(&input, &meta, 2).unwrap_err();
        assert!(matches!(
            err,
            KernelError::InvalidDimension { dim: 2, ndim: 2 }
        ));
    }

    #[test]
    fn min_dim_reduces_along_dim0() {
        // shape [2, 3]: [[1, 5, 3], [4, 2, 6]]
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 5.0, 3.0, 4.0, 2.0, 6.0];
        let (values, indices) = min_dim_tensor_contiguous_f64(&input, &meta, 0).expect("min_dim 0");
        assert_eq!(values, vec![1.0, 2.0, 3.0]);
        assert_eq!(indices, vec![0.0, 1.0, 0.0]);
    }

    #[test]
    fn min_dim_reduces_along_dim1() {
        // shape [2, 3]: [[1, 5, 3], [4, 2, 6]]
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 5.0, 3.0, 4.0, 2.0, 6.0];
        let (values, indices) = min_dim_tensor_contiguous_f64(&input, &meta, 1).expect("min_dim 1");
        assert_eq!(values, vec![1.0, 2.0]);
        assert_eq!(indices, vec![0.0, 1.0]);
    }

    #[test]
    fn min_dim_invalid_dim_returns_error() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let err = min_dim_tensor_contiguous_f64(&input, &meta, 2).unwrap_err();
        assert!(matches!(
            err,
            KernelError::InvalidDimension { dim: 2, ndim: 2 }
        ));
    }

    #[test]
    fn max_dim_3d_reduces_middle_dim() {
        // shape [2, 3, 2]: 12 elements
        let meta = TensorMeta::from_shape(vec![2, 3, 2], DType::F64, Device::Cpu);
        let input = vec![
            1.0, 2.0, 3.0, 4.0, 5.0, 6.0, // first [3,2] slice
            7.0, 8.0, 9.0, 10.0, 11.0, 12.0, // second [3,2] slice
        ];
        let (values, indices) =
            max_dim_tensor_contiguous_f64(&input, &meta, 1).expect("max_dim 1 on 3d");
        // Output shape [2, 2]: max along dim 1 (size 3)
        // [0,0]: max(1,3,5)=5 at idx 2; [0,1]: max(2,4,6)=6 at idx 2
        // [1,0]: max(7,9,11)=11 at idx 2; [1,1]: max(8,10,12)=12 at idx 2
        assert_eq!(values, vec![5.0, 6.0, 11.0, 12.0]);
        assert_eq!(indices, vec![2.0, 2.0, 2.0, 2.0]);
    }

    #[test]
    fn argmax_dim_1d_reduces_to_scalar() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let input = vec![1.0, 4.0, 2.0, 3.0];
        let out = argmax_dim_tensor_contiguous_f64(&input, &meta, 0).expect("argmax_dim on 1d");
        assert_eq!(out, vec![1.0]);
    }

    #[test]
    fn argmin_dim_1d_reduces_to_scalar() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let input = vec![3.0, 1.0, 4.0, 2.0];
        let out = argmin_dim_tensor_contiguous_f64(&input, &meta, 0).expect("argmin_dim on 1d");
        assert_eq!(out, vec![1.0]);
    }

    #[test]
    fn softmax_dim_sums_to_one() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let out = softmax_dim_tensor_contiguous_f64(&input, &meta, 1).expect("softmax");
        // Each row should sum to 1
        let row0_sum: f64 = out[0..3].iter().sum();
        let row1_sum: f64 = out[3..6].iter().sum();
        assert!((row0_sum - 1.0).abs() < 1e-12);
        assert!((row1_sum - 1.0).abs() < 1e-12);
        // All values should be positive
        assert!(out.iter().all(|&v| v > 0.0));
    }

    #[test]
    fn softmax_dim_preserves_relative_order() {
        let meta = TensorMeta::from_shape(vec![1, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 3.0, 2.0];
        let out = softmax_dim_tensor_contiguous_f64(&input, &meta, 1).expect("softmax");
        // input[1] > input[2] > input[0], so output should preserve order
        assert!(out[1] > out[2]);
        assert!(out[2] > out[0]);
    }

    #[test]
    fn log_softmax_dim_values() {
        let meta = TensorMeta::from_shape(vec![1, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0];
        let out = log_softmax_dim_tensor_contiguous_f64(&input, &meta, 1).expect("log_softmax");
        let sm = softmax_dim_tensor_contiguous_f64(&input, &meta, 1).expect("softmax");
        // log_softmax should equal log(softmax)
        for i in 0..3 {
            assert!((out[i] - sm[i].ln()).abs() < 1e-12);
        }
    }

    #[test]
    fn softmax_dim_invalid_dim() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let err = softmax_dim_tensor_contiguous_f64(&input, &meta, 2).unwrap_err();
        assert!(matches!(
            err,
            KernelError::InvalidDimension { dim: 2, ndim: 2 }
        ));
    }

    #[test]
    fn cat_along_dim0() {
        let m0 = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let d0 = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let m1 = TensorMeta::from_shape(vec![1, 3], DType::F64, Device::Cpu);
        let d1 = vec![7.0, 8.0, 9.0];
        let out = cat_tensor_contiguous_f64(&[(&d0, &m0), (&d1, &m1)], 0).expect("cat dim 0");
        assert_eq!(out, vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]);
    }

    #[test]
    fn cat_along_dim1() {
        let m0 = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let d0 = vec![1.0, 2.0, 3.0, 4.0];
        let m1 = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let d1 = vec![5.0, 6.0, 7.0, 8.0, 9.0, 10.0];
        let out = cat_tensor_contiguous_f64(&[(&d0, &m0), (&d1, &m1)], 1).expect("cat dim 1");
        // row0: [1,2,5,6,7], row1: [3,4,8,9,10]
        assert_eq!(out, vec![1.0, 2.0, 5.0, 6.0, 7.0, 3.0, 4.0, 8.0, 9.0, 10.0]);
    }

    #[test]
    fn stack_along_dim0() {
        let m0 = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let d0 = vec![1.0, 2.0, 3.0];
        let m1 = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let d1 = vec![4.0, 5.0, 6.0];
        let out = stack_tensor_contiguous_f64(&[(&d0, &m0), (&d1, &m1)], 0).expect("stack dim 0");
        // shape [2,3]: [[1,2,3],[4,5,6]]
        assert_eq!(out, vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]);
    }

    #[test]
    fn stack_along_dim1() {
        let m0 = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let d0 = vec![1.0, 2.0, 3.0, 4.0];
        let m1 = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let d1 = vec![5.0, 6.0, 7.0, 8.0];
        let out = stack_tensor_contiguous_f64(&[(&d0, &m0), (&d1, &m1)], 1).expect("stack dim 1");
        // shape [2,2,2]: [[[1,2],[5,6]],[[3,4],[7,8]]]
        assert_eq!(out, vec![1.0, 2.0, 5.0, 6.0, 3.0, 4.0, 7.0, 8.0]);
    }

    #[test]
    fn narrow_dim0_middle_rows() {
        // shape [4, 3], narrow(dim=0, start=1, length=2) -> shape [2, 3]
        let meta = TensorMeta::from_shape(vec![4, 3], DType::F64, Device::Cpu);
        let input: Vec<f64> = (1..=12).map(|i| i as f64).collect();
        let result = narrow_tensor_contiguous_f64(&input, &meta, 0, 1, 2).unwrap();
        assert_eq!(result, vec![4.0, 5.0, 6.0, 7.0, 8.0, 9.0]);
    }

    #[test]
    fn narrow_dim1_selects_columns() {
        // shape [2, 4], narrow(dim=1, start=1, length=2) -> shape [2, 2]
        let meta = TensorMeta::from_shape(vec![2, 4], DType::F64, Device::Cpu);
        let input: Vec<f64> = (1..=8).map(|i| i as f64).collect();
        let result = narrow_tensor_contiguous_f64(&input, &meta, 1, 1, 2).unwrap();
        assert_eq!(result, vec![2.0, 3.0, 6.0, 7.0]);
    }

    #[test]
    fn narrow_out_of_bounds_fails() {
        let meta = TensorMeta::from_shape(vec![3, 2], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let result = narrow_tensor_contiguous_f64(&input, &meta, 0, 2, 2);
        assert!(result.is_err());
    }

    #[test]
    fn narrow_invalid_dim_fails() {
        let meta = TensorMeta::from_shape(vec![3, 2], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let result = narrow_tensor_contiguous_f64(&input, &meta, 5, 0, 1);
        assert!(result.is_err());
    }

    #[test]
    fn narrow_start_plus_length_overflow_fails_closed() {
        let meta = TensorMeta::from_shape(vec![3, 2], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let result = narrow_tensor_contiguous_f64(&input, &meta, 0, usize::MAX, 1);
        assert!(result.is_err());
    }

    #[test]
    fn narrow_3d_tensor() {
        // shape [2, 3, 4], narrow(dim=1, start=1, length=2) -> shape [2, 2, 4]
        let meta = TensorMeta::from_shape(vec![2, 3, 4], DType::F64, Device::Cpu);
        let input: Vec<f64> = (1..=24).map(|i| i as f64).collect();
        let result = narrow_tensor_contiguous_f64(&input, &meta, 1, 1, 2).unwrap();
        assert_eq!(
            result,
            vec![
                5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0,
                23.0, 24.0
            ]
        );
    }

    #[test]
    fn expand_singleton_dim() {
        // shape [1, 3] -> [4, 3]
        let meta = TensorMeta::from_shape(vec![1, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0];
        let result = expand_tensor_contiguous_f64(&input, &meta, &[4, 3]).unwrap();
        assert_eq!(
            result,
            vec![1.0, 2.0, 3.0, 1.0, 2.0, 3.0, 1.0, 2.0, 3.0, 1.0, 2.0, 3.0]
        );
    }

    #[test]
    fn expand_multiple_singleton_dims() {
        // shape [1, 3, 1] -> [2, 3, 4]
        let meta = TensorMeta::from_shape(vec![1, 3, 1], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0];
        let result = expand_tensor_contiguous_f64(&input, &meta, &[2, 3, 4]).unwrap();
        assert_eq!(result.len(), 24);
        // First slice (outer=0): [1,1,1,1, 2,2,2,2, 3,3,3,3]
        assert_eq!(
            &result[0..12],
            &[1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0]
        );
        // Second slice same as first
        assert_eq!(
            &result[12..24],
            &[1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0]
        );
    }

    #[test]
    fn expand_non_singleton_fails() {
        // shape [2, 3] -> [4, 3]: dim 0 is 2, not 1 => error
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let result = expand_tensor_contiguous_f64(&input, &meta, &[4, 3]);
        assert!(result.is_err());
    }

    #[test]
    fn expand_same_shape_is_identity() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let result = expand_tensor_contiguous_f64(&input, &meta, &[2, 3]).unwrap();
        assert_eq!(result, input);
    }

    #[test]
    fn expand_rank_mismatch_fails() {
        let meta = TensorMeta::from_shape(vec![1, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0];
        let result = expand_tensor_contiguous_f64(&input, &meta, &[4, 3, 2]);
        assert!(result.is_err());
    }

    #[test]
    fn reduce_sum_for_broadcast_singleton_leading_dim() {
        let expanded = vec![1.0; 12];
        let reduced =
            super::reduce_sum_for_broadcast(&expanded, &[4, 3], &[1, 3]).expect("valid reduce");
        assert_eq!(reduced, vec![4.0, 4.0, 4.0]);
    }

    #[test]
    fn reduce_sum_for_broadcast_multiple_singletons() {
        let expanded: Vec<f64> = vec![1.0; 24];
        let reduced = super::reduce_sum_for_broadcast(&expanded, &[2, 3, 4], &[1, 3, 1])
            .expect("valid reduce");
        assert_eq!(reduced, vec![8.0, 8.0, 8.0]);
    }

    #[test]
    fn reduce_sum_for_broadcast_rejects_rank_mismatch() {
        let err = super::reduce_sum_for_broadcast(&[1.0, 2.0], &[2], &[1, 2])
            .expect_err("rank mismatch should fail");
        assert!(matches!(err, super::KernelError::ShapeMismatch { .. }));
    }

    #[test]
    fn reduce_sum_for_broadcast_rejects_invalid_broadcast_contract() {
        let err = super::reduce_sum_for_broadcast(&[1.0, 2.0, 3.0, 4.0], &[2, 2], &[2, 3])
            .expect_err("incompatible broadcast should fail");
        assert!(matches!(err, super::KernelError::ShapeMismatch { .. }));
    }

    #[test]
    fn reduce_sum_for_broadcast_rejects_gradient_len_mismatch() {
        let err = super::reduce_sum_for_broadcast(&[1.0, 2.0, 3.0], &[2, 2], &[1, 2])
            .expect_err("gradient shape mismatch should fail");
        assert!(matches!(err, super::KernelError::ShapeMismatch { .. }));
    }

    // ── index_select tests ─────────────────────────────────────────────

    #[test]
    fn index_select_dim0_swaps_rows() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let indices = vec![1.0, 0.0];
        let result = index_select_tensor_contiguous_f64(&input, &meta, 0, &indices).unwrap();
        assert_eq!(result, vec![4.0, 5.0, 6.0, 1.0, 2.0, 3.0]);
    }

    #[test]
    fn index_select_dim1_picks_columns() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let indices = vec![2.0, 0.0];
        let result = index_select_tensor_contiguous_f64(&input, &meta, 1, &indices).unwrap();
        assert_eq!(result, vec![3.0, 1.0, 6.0, 4.0]);
    }

    #[test]
    fn index_select_invalid_dim_returns_error() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let result = index_select_tensor_contiguous_f64(&input, &meta, 5, &[0.0]);
        assert!(result.is_err());
    }

    #[test]
    fn index_select_out_of_bounds_index_returns_error() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let result = index_select_tensor_contiguous_f64(&input, &meta, 0, &[5.0]);
        assert!(result.is_err());
    }

    #[test]
    fn index_select_nan_index_returns_error() {
        let meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0];
        let result = index_select_tensor_contiguous_f64(&input, &meta, 0, &[f64::NAN]);
        assert!(result.is_err());
    }

    #[test]
    fn index_select_huge_index_returns_error() {
        let meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0];
        let result = index_select_tensor_contiguous_f64(&input, &meta, 0, &[1.0e300]);
        assert!(result.is_err());
    }

    #[test]
    fn index_select_single_index() {
        let meta = TensorMeta::from_shape(vec![3, 2], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let indices = vec![1.0];
        let result = index_select_tensor_contiguous_f64(&input, &meta, 0, &indices).unwrap();
        assert_eq!(result, vec![3.0, 4.0]);
    }

    // ── gather tests ───────────────────────────────────────────────────

    #[test]
    fn gather_dim1_picks_values() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let index = vec![0.0, 2.0, 1.0, 0.0];
        let idx_meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let result = gather_tensor_contiguous_f64(&input, &meta, 1, &index, &idx_meta).unwrap();
        assert_eq!(result, vec![1.0, 3.0, 5.0, 4.0]);
    }

    #[test]
    fn gather_dim0_picks_values() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let index = vec![0.0, 1.0, 1.0];
        let idx_meta = TensorMeta::from_shape(vec![1, 3], DType::F64, Device::Cpu);
        let result = gather_tensor_contiguous_f64(&input, &meta, 0, &index, &idx_meta).unwrap();
        assert_eq!(result, vec![1.0, 5.0, 6.0]);
    }

    #[test]
    fn gather_invalid_dim_returns_error() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let idx_meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let result = gather_tensor_contiguous_f64(&input, &meta, 5, &[0.0; 4], &idx_meta);
        assert!(result.is_err());
    }

    #[test]
    fn gather_shape_mismatch_returns_error() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let idx_meta = TensorMeta::from_shape(vec![3, 2], DType::F64, Device::Cpu);
        let result = gather_tensor_contiguous_f64(&input, &meta, 1, &[0.0; 6], &idx_meta);
        assert!(result.is_err());
    }

    #[test]
    fn gather_non_integer_index_returns_error() {
        let meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0];
        let index = vec![0.5, 0.0];
        let idx_meta = TensorMeta::from_shape(vec![1, 2], DType::F64, Device::Cpu);
        let result = gather_tensor_contiguous_f64(&input, &meta, 1, &index, &idx_meta);
        assert!(result.is_err());
    }

    // ── scatter tests ──────────────────────────────────────────────────

    #[test]
    fn scatter_dim1_places_values() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![0.0; 6];
        let index = vec![0.0, 2.0, 1.0, 0.0];
        let idx_meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let src = vec![10.0, 20.0, 30.0, 40.0];
        let result =
            scatter_tensor_contiguous_f64(&input, &meta, 1, &index, &idx_meta, &src).unwrap();
        assert_eq!(result, vec![10.0, 0.0, 20.0, 40.0, 30.0, 0.0]);
    }

    #[test]
    fn scatter_dim0_places_values() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![0.0; 6];
        let index = vec![1.0, 0.0, 1.0];
        let idx_meta = TensorMeta::from_shape(vec![1, 3], DType::F64, Device::Cpu);
        let src = vec![10.0, 20.0, 30.0];
        let result =
            scatter_tensor_contiguous_f64(&input, &meta, 0, &index, &idx_meta, &src).unwrap();
        assert_eq!(result, vec![0.0, 20.0, 0.0, 10.0, 0.0, 30.0]);
    }

    #[test]
    fn scatter_invalid_dim_returns_error() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![0.0; 6];
        let idx_meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let result =
            scatter_tensor_contiguous_f64(&input, &meta, 5, &[0.0; 4], &idx_meta, &[0.0; 4]);
        assert!(result.is_err());
    }

    #[test]
    fn scatter_non_finite_index_returns_error() {
        let meta = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let input = vec![0.0; 4];
        let index = vec![f64::INFINITY, 0.0];
        let idx_meta = TensorMeta::from_shape(vec![1, 2], DType::F64, Device::Cpu);
        let src = vec![1.0, 2.0];
        let result = scatter_tensor_contiguous_f64(&input, &meta, 1, &index, &idx_meta, &src);
        assert!(result.is_err());
    }

    // ── masked_fill tests ──────────────────────────────────────────────

    #[test]
    fn masked_fill_replaces_masked_positions() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let mask = vec![1.0, 0.0, 1.0, 0.0, 1.0, 0.0];
        let result = masked_fill_tensor_contiguous_f64(&input, &meta, &mask, -1.0).unwrap();
        assert_eq!(result, vec![-1.0, 2.0, -1.0, 4.0, -1.0, 6.0]);
    }

    #[test]
    fn masked_fill_all_masked() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0];
        let mask = vec![1.0, 1.0, 1.0];
        let result = masked_fill_tensor_contiguous_f64(&input, &meta, &mask, 0.0).unwrap();
        assert_eq!(result, vec![0.0, 0.0, 0.0]);
    }

    #[test]
    fn masked_fill_none_masked() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0];
        let mask = vec![0.0, 0.0, 0.0];
        let result = masked_fill_tensor_contiguous_f64(&input, &meta, &mask, -1.0).unwrap();
        assert_eq!(result, vec![1.0, 2.0, 3.0]);
    }

    #[test]
    fn masked_fill_shape_mismatch_returns_error() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let mask = vec![1.0, 0.0]; // too short
        let result = masked_fill_tensor_contiguous_f64(&input, &meta, &mask, -1.0);
        assert!(result.is_err());
    }

    // ---- cos ----

    #[test]
    fn cos_scalar_returns_expected_value() {
        let input = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = cos_scalar(&input);
        assert!((out.value() - 1.0).abs() < 1e-10);
    }

    #[test]
    fn cos_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![0.0, std::f64::consts::PI, std::f64::consts::FRAC_PI_2];
        let out = cos_tensor_contiguous_f64(&input, &meta).expect("cos should succeed");
        assert!((out[0] - 1.0).abs() < 1e-10);
        assert!((out[1] - (-1.0)).abs() < 1e-10);
        assert!(out[2].abs() < 1e-10);
    }

    // ---- floor ----

    #[test]
    fn floor_scalar_returns_expected_value() {
        let input = ScalarTensor::new(3.7, DType::F64, Device::Cpu);
        let out = floor_scalar(&input);
        assert_eq!(out.value(), 3.0);
    }

    #[test]
    fn floor_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let input = vec![1.1, 2.9, -0.5, -2.1];
        let out = floor_tensor_contiguous_f64(&input, &meta).expect("floor should succeed");
        assert_eq!(out, vec![1.0, 2.0, -1.0, -3.0]);
    }

    // ---- ceil ----

    #[test]
    fn ceil_scalar_returns_expected_value() {
        let input = ScalarTensor::new(3.2, DType::F64, Device::Cpu);
        let out = ceil_scalar(&input);
        assert_eq!(out.value(), 4.0);
    }

    #[test]
    fn ceil_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let input = vec![1.1, 2.9, -0.5, -2.1];
        let out = ceil_tensor_contiguous_f64(&input, &meta).expect("ceil should succeed");
        assert_eq!(out, vec![2.0, 3.0, 0.0, -2.0]);
    }

    // ---- log2 ----

    #[test]
    fn log2_scalar_returns_expected_value() {
        let input = ScalarTensor::new(8.0, DType::F64, Device::Cpu);
        let out = log2_scalar(&input);
        assert!((out.value() - 3.0).abs() < 1e-10);
    }

    #[test]
    fn log2_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 4.0];
        let out = log2_tensor_contiguous_f64(&input, &meta).expect("log2 should succeed");
        assert!((out[0] - 0.0).abs() < 1e-10);
        assert!((out[1] - 1.0).abs() < 1e-10);
        assert!((out[2] - 2.0).abs() < 1e-10);
    }

    // ---- log10 ----

    #[test]
    fn log10_scalar_returns_expected_value() {
        let input = ScalarTensor::new(1000.0, DType::F64, Device::Cpu);
        let out = log10_scalar(&input);
        assert!((out.value() - 3.0).abs() < 1e-10);
    }

    #[test]
    fn log10_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![1.0, 10.0, 100.0];
        let out = log10_tensor_contiguous_f64(&input, &meta).expect("log10 should succeed");
        assert!((out[0] - 0.0).abs() < 1e-10);
        assert!((out[1] - 1.0).abs() < 1e-10);
        assert!((out[2] - 2.0).abs() < 1e-10);
    }

    // ---- log1p ----

    #[test]
    fn log1p_scalar_returns_expected_value() {
        let input = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = log1p_scalar(&input);
        assert!((out.value() - 0.0).abs() < 1e-10);
    }

    #[test]
    fn log1p_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![0.0, 1.0, std::f64::consts::E - 1.0];
        let out = log1p_tensor_contiguous_f64(&input, &meta).expect("log1p should succeed");
        assert!((out[0] - 0.0).abs() < 1e-10);
        assert!((out[1] - 2.0_f64.ln()).abs() < 1e-10);
        assert!((out[2] - 1.0).abs() < 1e-10);
    }

    // ---- expm1 ----

    #[test]
    fn expm1_scalar_returns_expected_value() {
        let input = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = expm1_scalar(&input);
        assert!((out.value() - 0.0).abs() < 1e-10);
    }

    #[test]
    fn expm1_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let input = vec![0.0, 1.0];
        let out = expm1_tensor_contiguous_f64(&input, &meta).expect("expm1 should succeed");
        assert!((out[0] - 0.0).abs() < 1e-10);
        assert!((out[1] - (std::f64::consts::E - 1.0)).abs() < 1e-10);
    }

    // ---- sign ----

    #[test]
    fn sign_scalar_returns_expected_value() {
        let pos = ScalarTensor::new(5.0, DType::F64, Device::Cpu);
        let neg = ScalarTensor::new(-3.0, DType::F64, Device::Cpu);
        let neg_zero = ScalarTensor::new(-0.0, DType::F64, Device::Cpu);
        assert_eq!(sign_scalar(&pos).value(), 1.0);
        assert_eq!(sign_scalar(&neg).value(), -1.0);
        // Rust signum: +0.0 → 1.0, -0.0 → -1.0 (IEEE 754 sign bit)
        assert_eq!(sign_scalar(&neg_zero).value(), -1.0);
    }

    #[test]
    fn sign_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let input = vec![3.0, -2.0, -0.0, -0.5];
        let out = sign_tensor_contiguous_f64(&input, &meta).expect("sign should succeed");
        // Rust signum: +3.0→1.0, -2.0→-1.0, -0.0→-1.0, -0.5→-1.0
        assert_eq!(out, vec![1.0, -1.0, -1.0, -1.0]);
    }

    // ---- trunc ----

    #[test]
    fn trunc_scalar_returns_expected_value() {
        let input = ScalarTensor::new(3.7, DType::F64, Device::Cpu);
        let out = trunc_scalar(&input);
        assert_eq!(out.value(), 3.0);
        let neg = ScalarTensor::new(-2.9, DType::F64, Device::Cpu);
        assert_eq!(trunc_scalar(&neg).value(), -2.0);
    }

    #[test]
    fn trunc_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let input = vec![1.9, -2.9, 0.1, -0.1];
        let out = trunc_tensor_contiguous_f64(&input, &meta).expect("trunc should succeed");
        assert_eq!(out, vec![1.0, -2.0, 0.0, 0.0]);
    }

    // ---- asin ----

    #[test]
    fn asin_scalar_returns_expected_value() {
        let input = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = asin_scalar(&input);
        assert!((out.value() - 0.0).abs() < 1e-10);
    }

    #[test]
    fn asin_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![0.0, 0.5, 1.0];
        let out = asin_tensor_contiguous_f64(&input, &meta).expect("asin should succeed");
        assert!((out[0] - 0.0).abs() < 1e-10);
        assert!((out[1] - std::f64::consts::FRAC_PI_6).abs() < 1e-10);
        assert!((out[2] - std::f64::consts::FRAC_PI_2).abs() < 1e-10);
    }

    // ---- acos ----

    #[test]
    fn acos_scalar_returns_expected_value() {
        let input = ScalarTensor::new(1.0, DType::F64, Device::Cpu);
        let out = acos_scalar(&input);
        assert!((out.value() - 0.0).abs() < 1e-10);
    }

    #[test]
    fn acos_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![1.0, 0.0, -1.0];
        let out = acos_tensor_contiguous_f64(&input, &meta).expect("acos should succeed");
        assert!((out[0] - 0.0).abs() < 1e-10);
        assert!((out[1] - std::f64::consts::FRAC_PI_2).abs() < 1e-10);
        assert!((out[2] - std::f64::consts::PI).abs() < 1e-10);
    }

    // ---- atan ----

    #[test]
    fn atan_scalar_returns_expected_value() {
        let input = ScalarTensor::new(1.0, DType::F64, Device::Cpu);
        let out = atan_scalar(&input);
        assert!((out.value() - std::f64::consts::FRAC_PI_4).abs() < 1e-10);
    }

    #[test]
    fn atan_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![0.0, 1.0, -1.0];
        let out = atan_tensor_contiguous_f64(&input, &meta).expect("atan should succeed");
        assert!((out[0] - 0.0).abs() < 1e-10);
        assert!((out[1] - std::f64::consts::FRAC_PI_4).abs() < 1e-10);
        assert!((out[2] - (-std::f64::consts::FRAC_PI_4)).abs() < 1e-10);
    }

    // ---- sinh ----

    #[test]
    fn sinh_scalar_returns_expected_value() {
        let input = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = sinh_scalar(&input);
        assert!((out.value() - 0.0).abs() < 1e-10);
    }

    #[test]
    fn sinh_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let input = vec![0.0, 1.0];
        let out = sinh_tensor_contiguous_f64(&input, &meta).expect("sinh should succeed");
        assert!((out[0] - 0.0).abs() < 1e-10);
        assert!((out[1] - 1.0_f64.sinh()).abs() < 1e-10);
    }

    // ---- cosh ----

    #[test]
    fn cosh_scalar_returns_expected_value() {
        let input = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = cosh_scalar(&input);
        assert!((out.value() - 1.0).abs() < 1e-10);
    }

    #[test]
    fn cosh_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let input = vec![0.0, 1.0];
        let out = cosh_tensor_contiguous_f64(&input, &meta).expect("cosh should succeed");
        assert!((out[0] - 1.0).abs() < 1e-10);
        assert!((out[1] - 1.0_f64.cosh()).abs() < 1e-10);
    }

    // ---- gelu ----

    #[test]
    fn gelu_scalar_returns_expected_value() {
        let zero = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = gelu_scalar(&zero);
        assert!((out.value() - 0.0).abs() < 1e-10);
        // GELU(x) for large positive x ≈ x
        let large = ScalarTensor::new(5.0, DType::F64, Device::Cpu);
        let out_large = gelu_scalar(&large);
        assert!((out_large.value() - 5.0).abs() < 0.01);
    }

    #[test]
    fn gelu_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![0.0, -5.0, 5.0];
        let out = gelu_tensor_contiguous_f64(&input, &meta).expect("gelu should succeed");
        assert!((out[0] - 0.0).abs() < 1e-10);
        assert!(out[1].abs() < 0.01); // GELU(-5) ≈ 0
        assert!((out[2] - 5.0).abs() < 0.01); // GELU(5) ≈ 5
    }

    // ---- silu ----

    #[test]
    fn silu_scalar_returns_expected_value() {
        let zero = ScalarTensor::new(0.0, DType::F64, Device::Cpu);
        let out = silu_scalar(&zero);
        assert!((out.value() - 0.0).abs() < 1e-10);
        // SiLU(x) = x * sigmoid(x), for large x → x
        let large = ScalarTensor::new(10.0, DType::F64, Device::Cpu);
        let out_large = silu_scalar(&large);
        assert!((out_large.value() - 10.0).abs() < 0.001);
    }

    #[test]
    fn silu_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![0.0, -10.0, 10.0];
        let out = silu_tensor_contiguous_f64(&input, &meta).expect("silu should succeed");
        assert!((out[0] - 0.0).abs() < 1e-10);
        assert!(out[1].abs() < 0.001); // SiLU(-10) ≈ 0
        assert!((out[2] - 10.0).abs() < 0.001); // SiLU(10) ≈ 10
    }

    // ---- leaky_relu ----

    #[test]
    fn leaky_relu_scalar_returns_expected_value() {
        let pos = ScalarTensor::new(3.0, DType::F64, Device::Cpu);
        let neg = ScalarTensor::new(-2.0, DType::F64, Device::Cpu);
        assert_eq!(leaky_relu_scalar(&pos).value(), 3.0);
        assert!((leaky_relu_scalar(&neg).value() - (-0.02)).abs() < 1e-10);
    }

    #[test]
    fn leaky_relu_tensor_contiguous_returns_expected_values() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let input = vec![1.0, -1.0, 0.0, -5.0];
        let out =
            leaky_relu_tensor_contiguous_f64(&input, &meta).expect("leaky_relu should succeed");
        assert_eq!(out[0], 1.0);
        assert!((out[1] - (-0.01)).abs() < 1e-10);
        assert_eq!(out[2], 0.0);
        assert!((out[3] - (-0.05)).abs() < 1e-10);
    }

    // ---- cumsum ----

    #[test]
    fn cumsum_1d() {
        let meta = TensorMeta::from_shape(vec![5], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0];
        let out =
            super::cumsum_tensor_contiguous_f64(&input, &meta, 0).expect("cumsum should succeed");
        assert_eq!(out, vec![1.0, 3.0, 6.0, 10.0, 15.0]);
    }

    #[test]
    fn cumsum_2d_dim0() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let out =
            super::cumsum_tensor_contiguous_f64(&input, &meta, 0).expect("cumsum should succeed");
        // dim0: accumulate rows: [1,2,3] -> [1,2,3], [1+4,2+5,3+6] = [5,7,9]
        assert_eq!(out, vec![1.0, 2.0, 3.0, 5.0, 7.0, 9.0]);
    }

    #[test]
    fn cumsum_2d_dim1() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let out =
            super::cumsum_tensor_contiguous_f64(&input, &meta, 1).expect("cumsum should succeed");
        // dim1: accumulate within rows: [1,1+2,1+2+3] [4,4+5,4+5+6] = [1,3,6,4,9,15]
        assert_eq!(out, vec![1.0, 3.0, 6.0, 4.0, 9.0, 15.0]);
    }

    #[test]
    fn cumsum_invalid_dim() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0];
        let result = super::cumsum_tensor_contiguous_f64(&input, &meta, 1);
        assert!(result.is_err());
    }

    #[test]
    fn cumsum_backward_is_reverse_cumsum() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let grad_output = vec![1.0, 1.0, 1.0, 1.0];
        let grad_input = super::cumsum_backward_tensor_contiguous_f64(&grad_output, &meta, 0)
            .expect("cumsum backward should succeed");
        // Reverse cumsum of [1,1,1,1] = [4,3,2,1]
        assert_eq!(grad_input, vec![4.0, 3.0, 2.0, 1.0]);
    }

    // ---- cumprod ----

    #[test]
    fn cumprod_1d() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let input = vec![2.0, 3.0, 4.0, 5.0];
        let out =
            super::cumprod_tensor_contiguous_f64(&input, &meta, 0).expect("cumprod should succeed");
        assert_eq!(out, vec![2.0, 6.0, 24.0, 120.0]);
    }

    #[test]
    fn cumprod_2d_dim0() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let out =
            super::cumprod_tensor_contiguous_f64(&input, &meta, 0).expect("cumprod should succeed");
        // dim0: [1,2,3] -> [1,2,3], [1*4,2*5,3*6] = [4,10,18]
        assert_eq!(out, vec![1.0, 2.0, 3.0, 4.0, 10.0, 18.0]);
    }

    #[test]
    fn cumprod_2d_dim1() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let out =
            super::cumprod_tensor_contiguous_f64(&input, &meta, 1).expect("cumprod should succeed");
        // dim1: [1,1*2,1*2*3] [4,4*5,4*5*6] = [1,2,6,4,20,120]
        assert_eq!(out, vec![1.0, 2.0, 6.0, 4.0, 20.0, 120.0]);
    }

    #[test]
    fn cumprod_invalid_dim() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0];
        let result = super::cumprod_tensor_contiguous_f64(&input, &meta, 1);
        assert!(result.is_err());
    }

    // ---- where ----

    #[test]
    fn where_selects_by_condition() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let cond = vec![1.0, 0.0, 1.0, 0.0];
        let x = vec![10.0, 20.0, 30.0, 40.0];
        let y = vec![-1.0, -2.0, -3.0, -4.0];
        let out =
            super::where_tensor_contiguous_f64(&cond, &x, &y, &meta).expect("where should succeed");
        assert_eq!(out, vec![10.0, -2.0, 30.0, -4.0]);
    }

    #[test]
    fn where_all_true() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let cond = vec![1.0, 1.0, 1.0];
        let x = vec![10.0, 20.0, 30.0];
        let y = vec![-1.0, -2.0, -3.0];
        let out =
            super::where_tensor_contiguous_f64(&cond, &x, &y, &meta).expect("where should succeed");
        assert_eq!(out, vec![10.0, 20.0, 30.0]);
    }

    #[test]
    fn where_all_false() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let cond = vec![0.0, 0.0, 0.0];
        let x = vec![10.0, 20.0, 30.0];
        let y = vec![-1.0, -2.0, -3.0];
        let out =
            super::where_tensor_contiguous_f64(&cond, &x, &y, &meta).expect("where should succeed");
        assert_eq!(out, vec![-1.0, -2.0, -3.0]);
    }

    // ---- sort ----

    #[test]
    fn sort_1d_ascending() {
        let meta = TensorMeta::from_shape(vec![5], DType::F64, Device::Cpu);
        let input = vec![3.0, 1.0, 4.0, 1.0, 5.0];
        let (vals, idxs) =
            super::sort_tensor_contiguous_f64(&input, &meta, 0, false).expect("sort should work");
        assert_eq!(vals, vec![1.0, 1.0, 3.0, 4.0, 5.0]);
        assert_eq!(idxs, vec![1, 3, 0, 2, 4]);
    }

    #[test]
    fn sort_1d_descending() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let input = vec![2.0, 5.0, 1.0, 3.0];
        let (vals, idxs) =
            super::sort_tensor_contiguous_f64(&input, &meta, 0, true).expect("sort should work");
        assert_eq!(vals, vec![5.0, 3.0, 2.0, 1.0]);
        assert_eq!(idxs, vec![1, 3, 0, 2]);
    }

    #[test]
    fn sort_2d_along_dim1() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let input = vec![3.0, 1.0, 2.0, 6.0, 4.0, 5.0];
        let (vals, idxs) =
            super::sort_tensor_contiguous_f64(&input, &meta, 1, false).expect("sort should work");
        assert_eq!(vals, vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]);
        assert_eq!(idxs, vec![1, 2, 0, 1, 2, 0]);
    }

    #[test]
    fn sort_invalid_dim() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0];
        let result = super::sort_tensor_contiguous_f64(&input, &meta, 1, false);
        assert!(result.is_err());
    }

    // ---- topk ----

    #[test]
    fn topk_largest_sorted() {
        let meta = TensorMeta::from_shape(vec![5], DType::F64, Device::Cpu);
        let input = vec![3.0, 1.0, 4.0, 1.0, 5.0];
        let (vals, idxs) =
            super::topk_tensor_contiguous_f64(&input, &meta, 3, 0, true, true).expect("topk");
        assert_eq!(vals, vec![5.0, 4.0, 3.0]);
        assert_eq!(idxs, vec![4, 2, 0]);
    }

    #[test]
    fn topk_smallest_sorted() {
        let meta = TensorMeta::from_shape(vec![5], DType::F64, Device::Cpu);
        let input = vec![3.0, 1.0, 4.0, 1.0, 5.0];
        let (vals, idxs) =
            super::topk_tensor_contiguous_f64(&input, &meta, 2, 0, false, true).expect("topk");
        assert_eq!(vals, vec![1.0, 1.0]);
        assert_eq!(idxs, vec![1, 3]);
    }

    #[test]
    fn topk_2d_along_dim1() {
        let meta = TensorMeta::from_shape(vec![2, 4], DType::F64, Device::Cpu);
        let input = vec![4.0, 2.0, 3.0, 1.0, 8.0, 6.0, 7.0, 5.0];
        let (vals, idxs) =
            super::topk_tensor_contiguous_f64(&input, &meta, 2, 1, true, true).expect("topk");
        // First row top-2: 4.0 (idx 0), 3.0 (idx 2)
        // Second row top-2: 8.0 (idx 0), 7.0 (idx 2)
        assert_eq!(vals, vec![4.0, 3.0, 8.0, 7.0]);
        assert_eq!(idxs, vec![0, 2, 0, 2]);
    }

    #[test]
    fn topk_k_exceeds_dim_returns_error() {
        let meta = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let input = vec![1.0, 2.0, 3.0];
        let result = super::topk_tensor_contiguous_f64(&input, &meta, 5, 0, true, true);
        assert!(result.is_err());
    }

    // -------------------------------------------------------------------
    // LU Decomposition tests
    // -------------------------------------------------------------------

    fn mat_mul_nn(a: &[f64], b: &[f64], n: usize) -> Vec<f64> {
        let mut c = vec![0.0; n * n];
        for i in 0..n {
            for j in 0..n {
                let mut acc = 0.0;
                for k in 0..n {
                    acc += a[i * n + k] * b[k * n + j];
                }
                c[i * n + j] = acc;
            }
        }
        c
    }

    fn assert_mat_approx_eq(a: &[f64], b: &[f64], tol: f64, msg: &str) {
        assert_eq!(a.len(), b.len(), "{msg}: length mismatch");
        for (i, (&av, &bv)) in a.iter().zip(b.iter()).enumerate() {
            assert!(
                (av - bv).abs() < tol,
                "{msg}: element {i} differs: {av} vs {bv} (tol={tol})"
            );
        }
    }

    #[test]
    fn lu_factor_identity_3x3() {
        let meta = TensorMeta::from_shape(vec![3, 3], DType::F64, Device::Cpu);
        #[rustfmt::skip]
        let a = vec![
            1.0, 0.0, 0.0,
            0.0, 1.0, 0.0,
            0.0, 0.0, 1.0,
        ];
        let result = super::lu_factor_contiguous_f64(&a, &meta).expect("lu_factor should succeed");
        assert_eq!(result.n, 3);

        let unpacked = super::lu_unpack(&result);
        // For identity: P=I, L=I, U=I
        assert_mat_approx_eq(&unpacked.l, &a, 1e-12, "L should be identity");
        assert_mat_approx_eq(&unpacked.u, &a, 1e-12, "U should be identity");

        // P @ L @ U == A
        let pl = mat_mul_nn(&unpacked.p, &unpacked.l, 3);
        let plu = mat_mul_nn(&pl, &unpacked.u, 3);
        assert_mat_approx_eq(&plu, &a, 1e-12, "P @ L @ U should equal A");
    }

    #[test]
    fn lu_factor_known_3x3() {
        let meta = TensorMeta::from_shape(vec![3, 3], DType::F64, Device::Cpu);
        #[rustfmt::skip]
        let a = vec![
            2.0, 1.0, 1.0,
            4.0, 3.0, 3.0,
            8.0, 7.0, 9.0,
        ];
        let result = super::lu_factor_contiguous_f64(&a, &meta).expect("lu_factor should succeed");
        let unpacked = super::lu_unpack(&result);

        // Verify P @ L @ U == A
        let pl = mat_mul_nn(&unpacked.p, &unpacked.l, 3);
        let plu = mat_mul_nn(&pl, &unpacked.u, 3);
        assert_mat_approx_eq(&plu, &a, 1e-10, "P @ L @ U should equal A");

        // Verify L is lower triangular with unit diagonal
        for i in 0..3 {
            assert!(
                (unpacked.l[i * 3 + i] - 1.0).abs() < 1e-12,
                "L diagonal should be 1.0"
            );
            for j in (i + 1)..3 {
                assert!(
                    unpacked.l[i * 3 + j].abs() < 1e-12,
                    "L should be zero above diagonal"
                );
            }
        }

        // Verify U is upper triangular
        for i in 0..3 {
            for j in 0..i {
                assert!(
                    unpacked.u[i * 3 + j].abs() < 1e-12,
                    "U should be zero below diagonal"
                );
            }
        }
    }

    #[test]
    fn lu_factor_already_upper_triangular() {
        let meta = TensorMeta::from_shape(vec![3, 3], DType::F64, Device::Cpu);
        #[rustfmt::skip]
        let a = vec![
            3.0, 2.0, 1.0,
            0.0, 5.0, 4.0,
            0.0, 0.0, 6.0,
        ];
        let result = super::lu_factor_contiguous_f64(&a, &meta).expect("lu_factor");
        let unpacked = super::lu_unpack(&result);

        let pl = mat_mul_nn(&unpacked.p, &unpacked.l, 3);
        let plu = mat_mul_nn(&pl, &unpacked.u, 3);
        assert_mat_approx_eq(&plu, &a, 1e-10, "P @ L @ U should equal A");
    }

    #[test]
    fn lu_factor_1x1() {
        let meta = TensorMeta::from_shape(vec![1, 1], DType::F64, Device::Cpu);
        let a = vec![5.0];
        let result = super::lu_factor_contiguous_f64(&a, &meta).expect("lu_factor");
        let unpacked = super::lu_unpack(&result);

        assert_mat_approx_eq(&unpacked.p, &[1.0], 1e-12, "P for 1x1");
        assert_mat_approx_eq(&unpacked.l, &[1.0], 1e-12, "L for 1x1");
        assert_mat_approx_eq(&unpacked.u, &[5.0], 1e-12, "U for 1x1");
    }

    #[test]
    fn lu_factor_requires_square() {
        let meta = TensorMeta::from_shape(vec![2, 3], DType::F64, Device::Cpu);
        let a = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
        let result = super::lu_factor_contiguous_f64(&a, &meta);
        assert!(result.is_err(), "non-square matrix should error");
    }

    #[test]
    fn lu_factor_requires_2d() {
        let meta = TensorMeta::from_shape(vec![8], DType::F64, Device::Cpu);
        let a = vec![1.0; 8];
        let result = super::lu_factor_contiguous_f64(&a, &meta);
        assert!(result.is_err(), "1D tensor should error");
    }

    #[test]
    fn lu_factor_zeros_on_diagonal_needs_pivoting() {
        let meta = TensorMeta::from_shape(vec![3, 3], DType::F64, Device::Cpu);
        #[rustfmt::skip]
        let a = vec![
            0.0, 1.0, 2.0,
            3.0, 4.0, 5.0,
            6.0, 7.0, 8.0,
        ];
        let result = super::lu_factor_contiguous_f64(&a, &meta).expect("pivoting should handle this");
        let unpacked = super::lu_unpack(&result);

        let pl = mat_mul_nn(&unpacked.p, &unpacked.l, 3);
        let plu = mat_mul_nn(&pl, &unpacked.u, 3);
        assert_mat_approx_eq(&plu, &a, 1e-10, "P @ L @ U should equal A even with zero diagonal");
    }

    #[test]
    fn lu_solve_simple_system() {
        // Solve A * x = b where A = [[2, 1], [5, 3]], b = [4, 7]
        // Expected: x = [5, -6]
        let meta_a = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let a = vec![2.0, 1.0, 5.0, 3.0];
        let factor = super::lu_factor_contiguous_f64(&a, &meta_a).expect("lu_factor");

        let meta_b = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let b = vec![4.0, 7.0];
        let x = super::lu_solve_contiguous_f64(&factor, &b, &meta_b).expect("lu_solve");

        assert_mat_approx_eq(&x, &[5.0, -6.0], 1e-10, "solution should be [5, -6]");
    }

    #[test]
    fn lu_solve_3x3_system() {
        // A = [[1, 2, 3], [4, 5, 6], [7, 8, 10]]
        // b = [1, 2, 3]
        let meta_a = TensorMeta::from_shape(vec![3, 3], DType::F64, Device::Cpu);
        #[rustfmt::skip]
        let a = vec![
            1.0, 2.0, 3.0,
            4.0, 5.0, 6.0,
            7.0, 8.0, 10.0,
        ];
        let factor = super::lu_factor_contiguous_f64(&a, &meta_a).expect("lu_factor");

        let meta_b = TensorMeta::from_shape(vec![3], DType::F64, Device::Cpu);
        let b = vec![1.0, 2.0, 3.0];
        let x = super::lu_solve_contiguous_f64(&factor, &b, &meta_b).expect("lu_solve");

        // Verify A * x ≈ b
        let mut ax = vec![0.0; 3];
        for i in 0..3 {
            for j in 0..3 {
                ax[i] += a[i * 3 + j] * x[j];
            }
        }
        assert_mat_approx_eq(&ax, &b, 1e-10, "A * x should equal b");
    }

    #[test]
    fn lu_solve_multiple_rhs() {
        let meta_a = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let a = vec![2.0, 1.0, 5.0, 3.0];
        let factor = super::lu_factor_contiguous_f64(&a, &meta_a).expect("lu_factor");

        // B is [2, 2]: two right-hand sides
        let meta_b = TensorMeta::from_shape(vec![2, 2], DType::F64, Device::Cpu);
        let b = vec![4.0, 3.0, 7.0, 8.0]; // columns: [4,7] and [3,8]
        let x = super::lu_solve_contiguous_f64(&factor, &b, &meta_b).expect("lu_solve multi-rhs");

        // Verify A * X ≈ B (column by column)
        for rhs in 0..2 {
            for i in 0..2 {
                let mut val = 0.0;
                for j in 0..2 {
                    val += a[i * 2 + j] * x[j * 2 + rhs];
                }
                assert!(
                    (val - b[i * 2 + rhs]).abs() < 1e-10,
                    "A*X[{i},{rhs}] = {val}, expected {}",
                    b[i * 2 + rhs]
                );
            }
        }
    }

    #[test]
    fn lu_solve_dimension_mismatch_errors() {
        let meta_a = TensorMeta::from_shape(vec![3, 3], DType::F64, Device::Cpu);
        // Use a simple diagonal to avoid singularity
        let a_diag = vec![1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0];
        let factor = super::lu_factor_contiguous_f64(&a_diag, &meta_a).expect("lu_factor");

        let meta_b = TensorMeta::from_shape(vec![2], DType::F64, Device::Cpu);
        let b = vec![1.0, 2.0];
        let result = super::lu_solve_contiguous_f64(&factor, &b, &meta_b);
        assert!(result.is_err(), "mismatched dimensions should error");
    }

    #[test]
    fn lu_factor_empty_matrix() {
        let meta = TensorMeta::from_shape(vec![0, 0], DType::F64, Device::Cpu);
        let a: Vec<f64> = vec![];
        let result = super::lu_factor_contiguous_f64(&a, &meta).expect("empty should succeed");
        assert_eq!(result.n, 0);
        assert!(result.lu.is_empty());
        assert!(result.pivots.is_empty());
    }

    // ---- QR Decomposition tests (bd-2drq.4) ----

    /// Multiply an (m x p) matrix by a (p x n) matrix, returning (m x n).
    fn mat_mul(a: &[f64], b: &[f64], m: usize, p: usize, n: usize) -> Vec<f64> {
        let mut c = vec![0.0; m * n];
        for i in 0..m {
            for j in 0..n {
                let mut acc = 0.0;
                for k in 0..p {
                    acc += a[i * p + k] * b[k * n + j];
                }
                c[i * n + j] = acc;
            }
        }
        c
    }

    #[test]
    fn qr_identity_3x3_complete() {
        let meta = TensorMeta::from_shape(vec![3, 3], DType::F64, Device::Cpu);
        #[rustfmt::skip]
        let a = vec![
            1.0, 0.0, 0.0,
            0.0, 1.0, 0.0,
            0.0, 0.0, 1.0,
        ];
        let result = super::qr_contiguous_f64(&a, &meta, false).expect("qr should succeed");
        // Complete mode: Q is 3x3, R is 3x3
        assert_eq!(result.q.len(), 9);
        assert_eq!(result.r.len(), 9);

        // Q should be orthogonal (Q^T @ Q == I)
        let qtq = mat_mul_nn(&transpose_mat(&result.q, 3, 3), &result.q, 3);
        assert_mat_approx_eq(&qtq, &a, 1e-12, "Q^T @ Q should be identity");

        // Q @ R == A
        let qr = mat_mul(&result.q, &result.r, 3, 3, 3);
        assert_mat_approx_eq(&qr, &a, 1e-12, "Q @ R should equal A");
    }

    fn transpose_mat(a: &[f64], m: usize, n: usize) -> Vec<f64> {
        let mut t = vec![0.0; m * n];
        for i in 0..m {
            for j in 0..n {
                t[j * m + i] = a[i * n + j];
            }
        }
        t
    }

    #[test]
    fn qr_known_3x3_reduced() {
        let meta = TensorMeta::from_shape(vec![3, 3], DType::F64, Device::Cpu);
        #[rustfmt::skip]
        let a = vec![
            12.0, -51.0,   4.0,
             6.0, 167.0, -68.0,
            -4.0,  24.0, -41.0,
        ];
        let result = super::qr_contiguous_f64(&a, &meta, true).expect("qr should succeed");
        // Reduced: k = min(3,3) = 3, so Q is 3x3, R is 3x3
        assert_eq!(result.q.len(), 9);
        assert_eq!(result.r.len(), 9);

        // Q @ R == A
        let qr = mat_mul(&result.q, &result.r, 3, 3, 3);
        assert_mat_approx_eq(&qr, &a, 1e-10, "Q @ R should equal A");

        // Q^T @ Q should be identity
        let qtq = mat_mul(&transpose_mat(&result.q, 3, 3), &result.q, 3, 3, 3);
        #[rustfmt::skip]
        let eye3 = vec![
            1.0, 0.0, 0.0,
            0.0, 1.0, 0.0,
            0.0, 0.0, 1.0,
        ];
        assert_mat_approx_eq(&qtq, &eye3, 1e-12, "Q^T @ Q should be identity");

        // R should be upper triangular
        for i in 0..3 {
            for j in 0..i {
                assert!(
                    result.r[i * 3 + j].abs() < 1e-12,
                    "R[{i},{j}] = {} should be zero",
                    result.r[i * 3 + j]
                );
            }
        }
    }

    #[test]
    fn qr_tall_matrix_4x2_reduced() {
        let meta = TensorMeta::from_shape(vec![4, 2], DType::F64, Device::Cpu);
        #[rustfmt::skip]
        let a = vec![
            1.0, 2.0,
            3.0, 4.0,
            5.0, 6.0,
            7.0, 8.0,
        ];
        let result = super::qr_contiguous_f64(&a, &meta, true).expect("qr should succeed");
        let m = 4;
        let n = 2;
        let k = 2; // min(4,2)
        // Reduced: Q is 4x2, R is 2x2
        assert_eq!(result.q.len(), m * k);
        assert_eq!(result.r.len(), k * n);

        // Q @ R == A
        let qr = mat_mul(&result.q, &result.r, m, k, n);
        assert_mat_approx_eq(&qr, &a, 1e-10, "Q @ R should equal A");

        // Q^T @ Q should be k x k identity
        let qtq = mat_mul(&transpose_mat(&result.q, m, k), &result.q, k, m, k);
        let mut eye_k = vec![0.0; k * k];
        for i in 0..k {
            eye_k[i * k + i] = 1.0;
        }
        assert_mat_approx_eq(&qtq, &eye_k, 1e-12, "Q^T @ Q should be identity");
    }

    #[test]
    fn qr_tall_matrix_4x2_complete() {
        let meta = TensorMeta::from_shape(vec![4, 2], DType::F64, Device::Cpu);
        #[rustfmt::skip]
        let a = vec![
            1.0, 2.0,
            3.0, 4.0,
            5.0, 6.0,
            7.0, 8.0,
        ];
        let result = super::qr_contiguous_f64(&a, &meta, false).expect("qr should succeed");
        let m = 4;
        let n = 2;
        // Complete: Q is 4x4, R is 4x2
        assert_eq!(result.q.len(), m * m);
        assert_eq!(result.r.len(), m * n);

        // Q @ R == A
        let qr = mat_mul(&result.q, &result.r, m, m, n);
        assert_mat_approx_eq(&qr, &a, 1e-10, "Q @ R should equal A");

        // Q^T @ Q should be m x m identity
        let qtq = mat_mul(&transpose_mat(&result.q, m, m), &result.q, m, m, m);
        let mut eye_m = vec![0.0; m * m];
        for i in 0..m {
            eye_m[i * m + i] = 1.0;
        }
        assert_mat_approx_eq(&qtq, &eye_m, 1e-12, "Q^T @ Q should be identity");
    }

    #[test]
    fn qr_wide_matrix_2x4_reduced() {
        let meta = TensorMeta::from_shape(vec![2, 4], DType::F64, Device::Cpu);
        #[rustfmt::skip]
        let a = vec![
            1.0, 2.0, 3.0, 4.0,
            5.0, 6.0, 7.0, 8.0,
        ];
        let result = super::qr_contiguous_f64(&a, &meta, true).expect("qr should succeed");
        let m = 2;
        let n = 4;
        let k = 2; // min(2,4)
        // Reduced: Q is 2x2, R is 2x4
        assert_eq!(result.q.len(), m * k);
        assert_eq!(result.r.len(), k * n);

        // Q @ R == A
        let qr = mat_mul(&result.q, &result.r, m, k, n);
        assert_mat_approx_eq(&qr, &a, 1e-10, "Q @ R should equal A");

        // Q^T @ Q should be k x k identity
        let qtq = mat_mul(&transpose_mat(&result.q, m, k), &result.q, k, m, k);
        let mut eye_k = vec![0.0; k * k];
        for i in 0..k {
            eye_k[i * k + i] = 1.0;
        }
        assert_mat_approx_eq(&qtq, &eye_k, 1e-12, "Q^T @ Q should be identity");
    }

    #[test]
    fn qr_1x1() {
        let meta = TensorMeta::from_shape(vec![1, 1], DType::F64, Device::Cpu);
        let a = vec![5.0];
        let result = super::qr_contiguous_f64(&a, &meta, true).expect("qr should succeed");
        assert_eq!(result.q.len(), 1);
        assert_eq!(result.r.len(), 1);
        // For scalar: Q = ±1, R = ±5, Q*R = 5
        let qr_val = result.q[0] * result.r[0];
        assert!((qr_val - 5.0).abs() < 1e-12, "Q*R should equal 5.0");
        // Q should be orthogonal: |Q| = 1
        assert!(
            (result.q[0].abs() - 1.0).abs() < 1e-12,
            "Q should be ±1"
        );
    }

    #[test]
    fn qr_empty_matrix() {
        let meta = TensorMeta::from_shape(vec![0, 3], DType::F64, Device::Cpu);
        let a: Vec<f64> = vec![];
        let result = super::qr_contiguous_f64(&a, &meta, true).expect("empty should succeed");
        assert!(result.q.is_empty());
        assert!(result.r.is_empty());
    }

    #[test]
    fn qr_rejects_1d_tensor() {
        let meta = TensorMeta::from_shape(vec![4], DType::F64, Device::Cpu);
        let a = vec![1.0, 2.0, 3.0, 4.0];
        assert!(
            super::qr_contiguous_f64(&a, &meta, true).is_err(),
            "1D tensor should be rejected"
        );
    }

    #[test]
    fn qr_r_is_upper_triangular() {
        let meta = TensorMeta::from_shape(vec![4, 3], DType::F64, Device::Cpu);
        #[rustfmt::skip]
        let a = vec![
            1.0, 2.0, 3.0,
            4.0, 5.0, 6.0,
            7.0, 8.0, 9.0,
            10.0, 11.0, 12.0,
        ];
        let result = super::qr_contiguous_f64(&a, &meta, true).expect("qr should succeed");
        let k = 3; // min(4,3)
        let n = 3;
        for i in 0..k {
            for j in 0..i.min(n) {
                assert!(
                    result.r[i * n + j].abs() < 1e-12,
                    "R[{i},{j}] = {} should be zero below diagonal",
                    result.r[i * n + j]
                );
            }
        }
    }
}
